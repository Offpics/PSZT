{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pszt import net\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_bagofwords.npy')\n",
    "y_train = np.load('y_bagofwords.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ pierwsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: relu\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 628.23, accuracy: 69.3%\n",
      "Epoch: 20, loss: 717.06, accuracy: 56.4%\n",
      "Epoch: 30, loss: 630.83, accuracy: 68.3%\n",
      "Epoch: 40, loss: 719.03, accuracy: 56.5%\n",
      "Epoch: 50, loss: 641.00, accuracy: 66.2%\n",
      "Epoch: 60, loss: 588.65, accuracy: 70.4%\n",
      "Epoch: 70, loss: 692.06, accuracy: 63.2%\n",
      "Epoch: 80, loss: 756.29, accuracy: 61.1%\n",
      "Epoch: 90, loss: 635.14, accuracy: 69.3%\n",
      "Epoch: 100, loss: 700.54, accuracy: 58.2%\n",
      "Epoch: 110, loss: 571.17, accuracy: 71.5%\n",
      "Epoch: 120, loss: 886.61, accuracy: 59.4%\n",
      "Epoch: 130, loss: 588.85, accuracy: 72.7%\n",
      "Epoch: 140, loss: 714.75, accuracy: 57.4%\n",
      "Epoch: 150, loss: 683.58, accuracy: 61.5%\n",
      "Epoch: 160, loss: 595.69, accuracy: 71.7%\n",
      "Epoch: 170, loss: 668.21, accuracy: 71.1%\n",
      "Epoch: 180, loss: 628.14, accuracy: 71.7%\n",
      "Epoch: 190, loss: 652.52, accuracy: 71.4%\n",
      "Epoch: 200, loss: 654.28, accuracy: 71.7%\n",
      "Epoch: 210, loss: 646.31, accuracy: 72.3%\n",
      "Epoch: 220, loss: 679.38, accuracy: 71.5%\n",
      "Epoch: 230, loss: 660.72, accuracy: 72.5%\n",
      "Epoch: 240, loss: 719.66, accuracy: 71.1%\n",
      "Epoch: 250, loss: 607.63, accuracy: 72.3%\n",
      "Epoch: 260, loss: 621.02, accuracy: 70.3%\n",
      "Epoch: 270, loss: 601.04, accuracy: 73.7%\n",
      "Epoch: 280, loss: 710.31, accuracy: 71.5%\n",
      "Epoch: 290, loss: 666.44, accuracy: 72.5%\n",
      "Epoch: 300, loss: 627.81, accuracy: 73.6%\n",
      "Mean accuracy on T0: 67.51\n",
      "Mean losses on T0: 663.35\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 697.61, accuracy: 61.6%\n",
      "Epoch: 20, loss: 637.03, accuracy: 66.6%\n",
      "Epoch: 30, loss: 712.35, accuracy: 56.0%\n",
      "Epoch: 40, loss: 930.12, accuracy: 56.0%\n",
      "Epoch: 50, loss: 707.99, accuracy: 56.0%\n",
      "Epoch: 60, loss: 719.85, accuracy: 56.0%\n",
      "Epoch: 70, loss: 700.26, accuracy: 56.0%\n",
      "Epoch: 80, loss: 886.36, accuracy: 33.1%\n",
      "Epoch: 90, loss: 706.94, accuracy: 56.0%\n",
      "Epoch: 100, loss: 659.44, accuracy: 67.1%\n",
      "Epoch: 110, loss: 610.24, accuracy: 68.7%\n",
      "Epoch: 120, loss: 612.04, accuracy: 67.4%\n",
      "Epoch: 130, loss: 621.96, accuracy: 68.9%\n",
      "Epoch: 140, loss: 678.53, accuracy: 66.6%\n",
      "Epoch: 150, loss: 709.71, accuracy: 66.2%\n",
      "Epoch: 160, loss: 654.77, accuracy: 68.7%\n",
      "Epoch: 170, loss: 652.13, accuracy: 69.4%\n",
      "Epoch: 180, loss: 650.54, accuracy: 69.6%\n",
      "Epoch: 190, loss: 672.64, accuracy: 69.9%\n",
      "Epoch: 200, loss: 653.35, accuracy: 69.5%\n",
      "Epoch: 210, loss: 716.66, accuracy: 70.2%\n",
      "Epoch: 220, loss: 592.71, accuracy: 69.9%\n",
      "Epoch: 230, loss: 631.23, accuracy: 64.5%\n",
      "Epoch: 240, loss: 608.40, accuracy: 67.9%\n",
      "Epoch: 250, loss: 674.95, accuracy: 70.3%\n",
      "Epoch: 260, loss: 673.07, accuracy: 68.9%\n",
      "Epoch: 270, loss: 819.73, accuracy: 64.1%\n",
      "Epoch: 280, loss: 755.16, accuracy: 70.4%\n",
      "Epoch: 290, loss: 828.60, accuracy: 65.3%\n",
      "Epoch: 300, loss: 693.08, accuracy: 70.6%\n",
      "Mean accuracy on T1: 63.41\n",
      "Mean losses on T1: 693.16\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 708.32, accuracy: 53.6%\n",
      "Epoch: 20, loss: 689.88, accuracy: 59.7%\n",
      "Epoch: 30, loss: 699.33, accuracy: 57.3%\n",
      "Epoch: 40, loss: 633.33, accuracy: 67.4%\n",
      "Epoch: 50, loss: 696.79, accuracy: 57.3%\n",
      "Epoch: 60, loss: 912.75, accuracy: 31.0%\n",
      "Epoch: 70, loss: 711.27, accuracy: 57.3%\n",
      "Epoch: 80, loss: 710.41, accuracy: 57.3%\n",
      "Epoch: 90, loss: 703.40, accuracy: 57.3%\n",
      "Epoch: 100, loss: 647.04, accuracy: 61.4%\n",
      "Epoch: 110, loss: 997.57, accuracy: 31.0%\n",
      "Epoch: 120, loss: 710.62, accuracy: 57.3%\n",
      "Epoch: 130, loss: 710.60, accuracy: 57.3%\n",
      "Epoch: 140, loss: 710.59, accuracy: 57.3%\n",
      "Epoch: 150, loss: 710.58, accuracy: 57.3%\n",
      "Epoch: 160, loss: 709.26, accuracy: 57.3%\n",
      "Epoch: 170, loss: 658.65, accuracy: 66.2%\n",
      "Epoch: 180, loss: 710.75, accuracy: 57.3%\n",
      "Epoch: 190, loss: 710.72, accuracy: 57.3%\n",
      "Epoch: 200, loss: 710.69, accuracy: 57.3%\n",
      "Epoch: 210, loss: 710.68, accuracy: 57.3%\n",
      "Epoch: 220, loss: 710.69, accuracy: 57.3%\n",
      "Epoch: 230, loss: 710.68, accuracy: 57.3%\n",
      "Epoch: 240, loss: 710.70, accuracy: 57.3%\n",
      "Epoch: 250, loss: 710.71, accuracy: 57.3%\n",
      "Epoch: 260, loss: 710.68, accuracy: 57.3%\n",
      "Epoch: 270, loss: 662.28, accuracy: 65.8%\n",
      "Epoch: 280, loss: 614.52, accuracy: 69.0%\n",
      "Epoch: 290, loss: 700.13, accuracy: 58.9%\n",
      "Epoch: 300, loss: 618.62, accuracy: 68.6%\n",
      "Mean accuracy on T2: 57.99\n",
      "Mean losses on T2: 712.76\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 592.64, accuracy: 69.1%\n",
      "Epoch: 20, loss: 698.21, accuracy: 57.7%\n",
      "Epoch: 30, loss: 710.29, accuracy: 57.7%\n",
      "Epoch: 40, loss: 699.25, accuracy: 57.7%\n",
      "Epoch: 50, loss: 613.47, accuracy: 66.8%\n",
      "Epoch: 60, loss: 698.30, accuracy: 57.7%\n",
      "Epoch: 70, loss: 815.76, accuracy: 57.7%\n",
      "Epoch: 80, loss: 875.85, accuracy: 58.0%\n",
      "Epoch: 90, loss: 655.56, accuracy: 64.7%\n",
      "Epoch: 100, loss: 727.04, accuracy: 67.0%\n",
      "Epoch: 110, loss: 761.00, accuracy: 63.6%\n",
      "Epoch: 120, loss: 594.15, accuracy: 70.4%\n",
      "Epoch: 130, loss: 662.33, accuracy: 65.7%\n",
      "Epoch: 140, loss: 683.14, accuracy: 64.4%\n",
      "Epoch: 150, loss: 583.95, accuracy: 70.7%\n",
      "Epoch: 160, loss: 591.40, accuracy: 70.0%\n",
      "Epoch: 170, loss: 707.00, accuracy: 67.8%\n",
      "Epoch: 180, loss: 757.79, accuracy: 66.0%\n",
      "Epoch: 190, loss: 563.63, accuracy: 72.0%\n",
      "Epoch: 200, loss: 584.72, accuracy: 71.2%\n",
      "Epoch: 210, loss: 680.52, accuracy: 61.6%\n",
      "Epoch: 220, loss: 602.96, accuracy: 71.5%\n",
      "Epoch: 230, loss: 660.84, accuracy: 72.8%\n",
      "Epoch: 240, loss: 694.59, accuracy: 70.7%\n",
      "Epoch: 250, loss: 573.55, accuracy: 73.7%\n",
      "Epoch: 260, loss: 601.82, accuracy: 72.0%\n",
      "Epoch: 270, loss: 917.21, accuracy: 60.2%\n",
      "Epoch: 280, loss: 590.76, accuracy: 72.8%\n",
      "Epoch: 290, loss: 608.97, accuracy: 71.5%\n",
      "Epoch: 300, loss: 806.87, accuracy: 63.6%\n",
      "Mean accuracy on T3: 66.68\n",
      "Mean losses on T3: 655.39\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 700.50, accuracy: 54.7%\n",
      "Epoch: 20, loss: 656.26, accuracy: 62.6%\n",
      "Epoch: 30, loss: 638.93, accuracy: 66.6%\n",
      "Epoch: 40, loss: 632.53, accuracy: 63.3%\n",
      "Epoch: 50, loss: 599.20, accuracy: 70.4%\n",
      "Epoch: 60, loss: 579.83, accuracy: 72.9%\n",
      "Epoch: 70, loss: 847.30, accuracy: 57.6%\n",
      "Epoch: 80, loss: 593.18, accuracy: 70.8%\n",
      "Epoch: 90, loss: 552.36, accuracy: 72.0%\n",
      "Epoch: 100, loss: 554.90, accuracy: 72.6%\n",
      "Epoch: 110, loss: 826.02, accuracy: 58.2%\n",
      "Epoch: 120, loss: 568.73, accuracy: 70.9%\n",
      "Epoch: 130, loss: 592.45, accuracy: 69.6%\n",
      "Epoch: 140, loss: 642.14, accuracy: 67.4%\n",
      "Epoch: 150, loss: 533.88, accuracy: 72.5%\n",
      "Epoch: 160, loss: 588.82, accuracy: 66.7%\n",
      "Epoch: 170, loss: 662.26, accuracy: 65.7%\n",
      "Epoch: 180, loss: 516.39, accuracy: 73.7%\n",
      "Epoch: 190, loss: 524.96, accuracy: 72.5%\n",
      "Epoch: 200, loss: 577.72, accuracy: 71.6%\n",
      "Epoch: 210, loss: 615.28, accuracy: 71.8%\n",
      "Epoch: 220, loss: 647.15, accuracy: 68.3%\n",
      "Epoch: 230, loss: 609.92, accuracy: 69.7%\n",
      "Epoch: 240, loss: 562.04, accuracy: 72.8%\n",
      "Epoch: 250, loss: 537.73, accuracy: 75.3%\n",
      "Epoch: 260, loss: 552.26, accuracy: 73.6%\n",
      "Epoch: 270, loss: 664.83, accuracy: 67.4%\n",
      "Epoch: 280, loss: 608.98, accuracy: 70.9%\n",
      "Epoch: 290, loss: 605.69, accuracy: 70.8%\n",
      "Epoch: 300, loss: 522.70, accuracy: 74.7%\n",
      "Mean accuracy on T4: 69.52\n",
      "Mean losses on T4: 619.24\n",
      "\n",
      "Accuracy list on Ti sets: [67.5107314936487, 63.4095488392466, 57.99167761717039, 66.67586508979413, 69.51842105263158]\n",
      "Losses list on Ti sets: [663.3516069920175, 693.1580237697169, 712.7627272823986, 655.3856418617821, 619.2449134428844]\n",
      "Mean accuracy on all T sets: 65.02\n",
      "Mean losses on all T sets: 668.78\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ druga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: sigmoid\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 720.62, accuracy: 58.1%\n",
      "Epoch: 20, loss: 697.95, accuracy: 58.5%\n",
      "Epoch: 30, loss: 680.35, accuracy: 59.9%\n",
      "Epoch: 40, loss: 663.03, accuracy: 61.0%\n",
      "Epoch: 50, loss: 647.93, accuracy: 63.3%\n",
      "Epoch: 60, loss: 634.60, accuracy: 64.1%\n",
      "Epoch: 70, loss: 621.49, accuracy: 65.0%\n",
      "Epoch: 80, loss: 609.78, accuracy: 67.3%\n",
      "Epoch: 90, loss: 598.67, accuracy: 68.7%\n",
      "Epoch: 100, loss: 589.15, accuracy: 69.9%\n",
      "Epoch: 110, loss: 580.84, accuracy: 70.4%\n",
      "Epoch: 120, loss: 758.53, accuracy: 61.1%\n",
      "Epoch: 130, loss: 584.01, accuracy: 68.1%\n",
      "Epoch: 140, loss: 610.11, accuracy: 70.0%\n",
      "Epoch: 150, loss: 658.97, accuracy: 67.5%\n",
      "Epoch: 160, loss: 651.71, accuracy: 68.1%\n",
      "Epoch: 170, loss: 649.29, accuracy: 68.2%\n",
      "Epoch: 180, loss: 637.28, accuracy: 69.1%\n",
      "Epoch: 190, loss: 633.37, accuracy: 69.1%\n",
      "Epoch: 200, loss: 629.80, accuracy: 69.1%\n",
      "Epoch: 210, loss: 626.47, accuracy: 69.0%\n",
      "Epoch: 220, loss: 622.72, accuracy: 69.8%\n",
      "Epoch: 230, loss: 618.89, accuracy: 70.3%\n",
      "Epoch: 240, loss: 615.39, accuracy: 69.8%\n",
      "Epoch: 250, loss: 612.33, accuracy: 70.3%\n",
      "Epoch: 260, loss: 609.58, accuracy: 70.4%\n",
      "Epoch: 270, loss: 607.06, accuracy: 70.6%\n",
      "Epoch: 280, loss: 604.87, accuracy: 70.2%\n",
      "Epoch: 290, loss: 603.11, accuracy: 70.4%\n",
      "Epoch: 300, loss: 601.79, accuracy: 70.7%\n",
      "Mean accuracy on T0: 67.06\n",
      "Mean losses on T0: 644.97\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 697.15, accuracy: 56.6%\n",
      "Epoch: 20, loss: 679.43, accuracy: 56.1%\n",
      "Epoch: 30, loss: 663.86, accuracy: 59.0%\n",
      "Epoch: 40, loss: 646.65, accuracy: 62.8%\n",
      "Epoch: 50, loss: 630.63, accuracy: 65.4%\n",
      "Epoch: 60, loss: 616.37, accuracy: 66.1%\n",
      "Epoch: 70, loss: 602.33, accuracy: 67.0%\n",
      "Epoch: 80, loss: 587.38, accuracy: 68.5%\n",
      "Epoch: 90, loss: 574.20, accuracy: 70.0%\n",
      "Epoch: 100, loss: 564.53, accuracy: 71.5%\n",
      "Epoch: 110, loss: 556.14, accuracy: 72.1%\n",
      "Epoch: 120, loss: 547.72, accuracy: 72.9%\n",
      "Epoch: 130, loss: 551.44, accuracy: 71.7%\n",
      "Epoch: 140, loss: 547.07, accuracy: 72.4%\n",
      "Epoch: 150, loss: 573.42, accuracy: 69.9%\n",
      "Epoch: 160, loss: 650.83, accuracy: 67.0%\n",
      "Epoch: 170, loss: 622.36, accuracy: 67.9%\n",
      "Epoch: 180, loss: 606.18, accuracy: 68.3%\n",
      "Epoch: 190, loss: 602.51, accuracy: 69.0%\n",
      "Epoch: 200, loss: 599.45, accuracy: 69.3%\n",
      "Epoch: 210, loss: 595.94, accuracy: 69.3%\n",
      "Epoch: 220, loss: 592.11, accuracy: 69.4%\n",
      "Epoch: 230, loss: 588.30, accuracy: 69.5%\n",
      "Epoch: 240, loss: 584.87, accuracy: 70.2%\n",
      "Epoch: 250, loss: 581.96, accuracy: 70.2%\n",
      "Epoch: 260, loss: 579.51, accuracy: 70.3%\n",
      "Epoch: 270, loss: 577.53, accuracy: 70.4%\n",
      "Epoch: 280, loss: 576.15, accuracy: 70.8%\n",
      "Epoch: 290, loss: 575.33, accuracy: 71.1%\n",
      "Epoch: 300, loss: 574.97, accuracy: 71.0%\n",
      "Mean accuracy on T1: 67.71\n",
      "Mean losses on T1: 618.25\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 701.09, accuracy: 56.4%\n",
      "Epoch: 20, loss: 678.29, accuracy: 56.5%\n",
      "Epoch: 30, loss: 659.02, accuracy: 61.1%\n",
      "Epoch: 40, loss: 638.10, accuracy: 63.6%\n",
      "Epoch: 50, loss: 619.06, accuracy: 66.4%\n",
      "Epoch: 60, loss: 603.45, accuracy: 67.7%\n",
      "Epoch: 70, loss: 588.23, accuracy: 68.5%\n",
      "Epoch: 80, loss: 574.75, accuracy: 69.3%\n",
      "Epoch: 90, loss: 565.33, accuracy: 70.3%\n",
      "Epoch: 100, loss: 557.25, accuracy: 70.8%\n",
      "Epoch: 110, loss: 549.33, accuracy: 71.2%\n",
      "Epoch: 120, loss: 616.86, accuracy: 66.8%\n",
      "Epoch: 130, loss: 549.73, accuracy: 70.8%\n",
      "Epoch: 140, loss: 611.30, accuracy: 66.5%\n",
      "Epoch: 150, loss: 630.79, accuracy: 66.5%\n",
      "Epoch: 160, loss: 633.13, accuracy: 66.5%\n",
      "Epoch: 170, loss: 629.89, accuracy: 66.6%\n",
      "Epoch: 180, loss: 627.46, accuracy: 66.6%\n",
      "Epoch: 190, loss: 627.62, accuracy: 66.6%\n",
      "Epoch: 200, loss: 630.15, accuracy: 66.8%\n",
      "Epoch: 210, loss: 630.30, accuracy: 67.3%\n",
      "Epoch: 220, loss: 629.53, accuracy: 67.7%\n",
      "Epoch: 230, loss: 629.59, accuracy: 67.5%\n",
      "Epoch: 240, loss: 630.12, accuracy: 67.7%\n",
      "Epoch: 250, loss: 630.87, accuracy: 67.9%\n",
      "Epoch: 260, loss: 631.76, accuracy: 68.3%\n",
      "Epoch: 270, loss: 632.77, accuracy: 68.5%\n",
      "Epoch: 280, loss: 633.81, accuracy: 68.3%\n",
      "Epoch: 290, loss: 634.36, accuracy: 68.5%\n",
      "Epoch: 300, loss: 633.07, accuracy: 68.3%\n",
      "Mean accuracy on T2: 68.04\n",
      "Mean losses on T2: 623.21\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 696.47, accuracy: 58.3%\n",
      "Epoch: 20, loss: 671.88, accuracy: 58.7%\n",
      "Epoch: 30, loss: 652.10, accuracy: 61.9%\n",
      "Epoch: 40, loss: 632.39, accuracy: 64.5%\n",
      "Epoch: 50, loss: 615.14, accuracy: 66.2%\n",
      "Epoch: 60, loss: 600.49, accuracy: 68.1%\n",
      "Epoch: 70, loss: 586.56, accuracy: 68.9%\n",
      "Epoch: 80, loss: 572.65, accuracy: 69.8%\n",
      "Epoch: 90, loss: 560.94, accuracy: 71.2%\n",
      "Epoch: 100, loss: 551.33, accuracy: 72.3%\n",
      "Epoch: 110, loss: 543.12, accuracy: 73.5%\n",
      "Epoch: 120, loss: 535.60, accuracy: 73.5%\n",
      "Epoch: 130, loss: 1084.05, accuracy: 41.3%\n",
      "Epoch: 140, loss: 545.19, accuracy: 71.1%\n",
      "Epoch: 150, loss: 528.72, accuracy: 74.1%\n",
      "Epoch: 160, loss: 562.69, accuracy: 72.9%\n",
      "Epoch: 170, loss: 605.80, accuracy: 69.9%\n",
      "Epoch: 180, loss: 574.45, accuracy: 72.0%\n",
      "Epoch: 190, loss: 565.45, accuracy: 72.8%\n",
      "Epoch: 200, loss: 564.46, accuracy: 73.1%\n",
      "Epoch: 210, loss: 561.97, accuracy: 73.1%\n",
      "Epoch: 220, loss: 559.54, accuracy: 73.2%\n",
      "Epoch: 230, loss: 557.17, accuracy: 73.1%\n",
      "Epoch: 240, loss: 554.38, accuracy: 72.8%\n",
      "Epoch: 250, loss: 551.33, accuracy: 72.8%\n",
      "Epoch: 260, loss: 548.28, accuracy: 73.5%\n",
      "Epoch: 270, loss: 545.16, accuracy: 73.6%\n",
      "Epoch: 280, loss: 542.02, accuracy: 73.7%\n",
      "Epoch: 290, loss: 538.68, accuracy: 74.5%\n",
      "Epoch: 300, loss: 534.99, accuracy: 74.6%\n",
      "Mean accuracy on T3: 69.27\n",
      "Mean losses on T3: 606.12\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 728.43, accuracy: 54.7%\n",
      "Epoch: 20, loss: 706.20, accuracy: 53.8%\n",
      "Epoch: 30, loss: 687.04, accuracy: 58.8%\n",
      "Epoch: 40, loss: 666.36, accuracy: 63.2%\n",
      "Epoch: 50, loss: 647.73, accuracy: 64.6%\n",
      "Epoch: 60, loss: 632.40, accuracy: 66.7%\n",
      "Epoch: 70, loss: 618.20, accuracy: 67.4%\n",
      "Epoch: 80, loss: 606.07, accuracy: 68.9%\n",
      "Epoch: 90, loss: 598.51, accuracy: 69.6%\n",
      "Epoch: 100, loss: 593.10, accuracy: 70.0%\n",
      "Epoch: 110, loss: 588.24, accuracy: 70.5%\n",
      "Epoch: 120, loss: 1462.03, accuracy: 54.9%\n",
      "Epoch: 130, loss: 589.20, accuracy: 70.9%\n",
      "Epoch: 140, loss: 637.53, accuracy: 68.2%\n",
      "Epoch: 150, loss: 669.27, accuracy: 65.7%\n",
      "Epoch: 160, loss: 674.81, accuracy: 65.7%\n",
      "Epoch: 170, loss: 663.70, accuracy: 66.1%\n",
      "Epoch: 180, loss: 656.31, accuracy: 66.4%\n",
      "Epoch: 190, loss: 653.42, accuracy: 66.8%\n",
      "Epoch: 200, loss: 651.47, accuracy: 67.0%\n",
      "Epoch: 210, loss: 649.25, accuracy: 67.1%\n",
      "Epoch: 220, loss: 646.25, accuracy: 67.1%\n",
      "Epoch: 230, loss: 642.43, accuracy: 67.2%\n",
      "Epoch: 240, loss: 638.26, accuracy: 67.9%\n",
      "Epoch: 250, loss: 634.46, accuracy: 67.9%\n",
      "Epoch: 260, loss: 631.53, accuracy: 67.6%\n",
      "Epoch: 270, loss: 628.85, accuracy: 67.5%\n",
      "Epoch: 280, loss: 626.38, accuracy: 67.2%\n",
      "Epoch: 290, loss: 624.33, accuracy: 67.6%\n",
      "Epoch: 300, loss: 622.69, accuracy: 67.9%\n",
      "Mean accuracy on T4: 65.74\n",
      "Mean losses on T4: 657.96\n",
      "\n",
      "Accuracy list on Ti sets: [67.05562855891371, 67.71222076215506, 68.03547963206307, 69.27069645203679, 65.73728070175437]\n",
      "Losses list on Ti sets: [644.9742788092682, 618.2545726709122, 623.2082603064631, 606.1205278048942, 657.9554045978063]\n",
      "Mean accuracy on all T sets: 67.56\n",
      "Mean losses on all T sets: 630.10\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ trzecia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: relu\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 1135.01, accuracy: 30.2%\n",
      "Epoch: 20, loss: 642.33, accuracy: 62.2%\n",
      "Epoch: 30, loss: 698.64, accuracy: 58.3%\n",
      "Epoch: 40, loss: 659.35, accuracy: 60.7%\n",
      "Epoch: 50, loss: 638.60, accuracy: 62.4%\n",
      "Epoch: 60, loss: 823.25, accuracy: 41.4%\n",
      "Epoch: 70, loss: 667.20, accuracy: 63.2%\n",
      "Epoch: 80, loss: 667.32, accuracy: 63.2%\n",
      "Epoch: 90, loss: 678.86, accuracy: 65.2%\n",
      "Epoch: 100, loss: 651.17, accuracy: 65.8%\n",
      "Epoch: 110, loss: 652.39, accuracy: 67.1%\n",
      "Epoch: 120, loss: 700.09, accuracy: 64.3%\n",
      "Epoch: 130, loss: 606.81, accuracy: 65.8%\n",
      "Epoch: 140, loss: 680.51, accuracy: 65.7%\n",
      "Epoch: 150, loss: 709.53, accuracy: 65.4%\n",
      "Epoch: 160, loss: 693.10, accuracy: 66.1%\n",
      "Epoch: 170, loss: 703.31, accuracy: 65.3%\n",
      "Epoch: 180, loss: 671.53, accuracy: 68.1%\n",
      "Epoch: 190, loss: 662.24, accuracy: 68.9%\n",
      "Epoch: 200, loss: 686.44, accuracy: 68.1%\n",
      "Epoch: 210, loss: 753.58, accuracy: 59.1%\n",
      "Epoch: 220, loss: 601.03, accuracy: 71.6%\n",
      "Epoch: 230, loss: 797.63, accuracy: 65.0%\n",
      "Epoch: 240, loss: 718.61, accuracy: 66.8%\n",
      "Epoch: 250, loss: 1207.53, accuracy: 50.3%\n",
      "Epoch: 260, loss: 706.25, accuracy: 51.0%\n",
      "Epoch: 270, loss: 584.49, accuracy: 66.5%\n",
      "Epoch: 280, loss: 644.59, accuracy: 59.0%\n",
      "Epoch: 290, loss: 650.01, accuracy: 59.5%\n",
      "Epoch: 300, loss: 712.14, accuracy: 65.6%\n",
      "Mean accuracy on T0: 62.72\n",
      "Mean losses on T0: 679.27\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 959.95, accuracy: 59.1%\n",
      "Epoch: 20, loss: 697.98, accuracy: 58.7%\n",
      "Epoch: 30, loss: 1159.87, accuracy: 30.4%\n",
      "Epoch: 40, loss: 669.00, accuracy: 58.7%\n",
      "Epoch: 50, loss: 645.51, accuracy: 64.9%\n",
      "Epoch: 60, loss: 606.79, accuracy: 68.7%\n",
      "Epoch: 70, loss: 689.76, accuracy: 59.0%\n",
      "Epoch: 80, loss: 605.54, accuracy: 69.0%\n",
      "Epoch: 90, loss: 606.62, accuracy: 70.0%\n",
      "Epoch: 100, loss: 578.00, accuracy: 70.3%\n",
      "Epoch: 110, loss: 640.71, accuracy: 69.8%\n",
      "Epoch: 120, loss: 556.12, accuracy: 71.7%\n",
      "Epoch: 130, loss: 578.73, accuracy: 70.7%\n",
      "Epoch: 140, loss: 577.28, accuracy: 71.7%\n",
      "Epoch: 150, loss: 619.00, accuracy: 71.5%\n",
      "Epoch: 160, loss: 649.57, accuracy: 66.6%\n",
      "Epoch: 170, loss: 554.50, accuracy: 72.9%\n",
      "Epoch: 180, loss: 607.93, accuracy: 71.7%\n",
      "Epoch: 190, loss: 642.27, accuracy: 70.4%\n",
      "Epoch: 200, loss: 675.54, accuracy: 70.3%\n",
      "Epoch: 210, loss: 664.91, accuracy: 71.2%\n",
      "Epoch: 220, loss: 626.95, accuracy: 71.7%\n",
      "Epoch: 230, loss: 659.59, accuracy: 71.6%\n",
      "Epoch: 240, loss: 632.88, accuracy: 71.0%\n",
      "Epoch: 250, loss: 684.59, accuracy: 68.6%\n",
      "Epoch: 260, loss: 569.71, accuracy: 71.2%\n",
      "Epoch: 270, loss: 602.72, accuracy: 72.4%\n",
      "Epoch: 280, loss: 572.30, accuracy: 67.8%\n",
      "Epoch: 290, loss: 598.86, accuracy: 73.5%\n",
      "Epoch: 300, loss: 764.27, accuracy: 68.9%\n",
      "Mean accuracy on T1: 67.08\n",
      "Mean losses on T1: 652.84\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 831.12, accuracy: 59.3%\n",
      "Epoch: 20, loss: 670.29, accuracy: 58.0%\n",
      "Epoch: 30, loss: 695.13, accuracy: 58.0%\n",
      "Epoch: 40, loss: 707.33, accuracy: 58.0%\n",
      "Epoch: 50, loss: 1037.27, accuracy: 31.0%\n",
      "Epoch: 60, loss: 635.59, accuracy: 66.5%\n",
      "Epoch: 70, loss: 652.55, accuracy: 68.2%\n",
      "Epoch: 80, loss: 699.88, accuracy: 58.0%\n",
      "Epoch: 90, loss: 651.46, accuracy: 58.0%\n",
      "Epoch: 100, loss: 752.05, accuracy: 58.0%\n",
      "Epoch: 110, loss: 652.83, accuracy: 64.4%\n",
      "Epoch: 120, loss: 618.15, accuracy: 66.4%\n",
      "Epoch: 130, loss: 668.04, accuracy: 58.0%\n",
      "Epoch: 140, loss: 700.75, accuracy: 58.0%\n",
      "Epoch: 150, loss: 716.17, accuracy: 52.7%\n",
      "Epoch: 160, loss: 620.97, accuracy: 66.5%\n",
      "Epoch: 170, loss: 693.27, accuracy: 58.0%\n",
      "Epoch: 180, loss: 635.46, accuracy: 58.0%\n",
      "Epoch: 190, loss: 644.24, accuracy: 58.0%\n",
      "Epoch: 200, loss: 617.48, accuracy: 58.0%\n",
      "Epoch: 210, loss: 702.61, accuracy: 58.0%\n",
      "Epoch: 220, loss: 702.38, accuracy: 58.0%\n",
      "Epoch: 230, loss: 702.34, accuracy: 58.0%\n",
      "Epoch: 240, loss: 701.85, accuracy: 58.0%\n",
      "Epoch: 250, loss: 745.11, accuracy: 60.3%\n",
      "Epoch: 260, loss: 702.46, accuracy: 58.0%\n",
      "Epoch: 270, loss: 702.45, accuracy: 58.0%\n",
      "Epoch: 280, loss: 702.44, accuracy: 58.0%\n",
      "Epoch: 290, loss: 702.39, accuracy: 58.0%\n",
      "Epoch: 300, loss: 701.65, accuracy: 58.0%\n",
      "Mean accuracy on T2: 57.55\n",
      "Mean losses on T2: 710.30\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 629.32, accuracy: 68.2%\n",
      "Epoch: 20, loss: 711.86, accuracy: 53.9%\n",
      "Epoch: 30, loss: 821.72, accuracy: 53.9%\n",
      "Epoch: 40, loss: 897.48, accuracy: 53.9%\n",
      "Epoch: 50, loss: 559.45, accuracy: 72.5%\n",
      "Epoch: 60, loss: 680.74, accuracy: 53.9%\n",
      "Epoch: 70, loss: 627.90, accuracy: 69.0%\n",
      "Epoch: 80, loss: 632.42, accuracy: 66.8%\n",
      "Epoch: 90, loss: 598.91, accuracy: 68.7%\n",
      "Epoch: 100, loss: 613.19, accuracy: 67.5%\n",
      "Epoch: 110, loss: 648.81, accuracy: 66.9%\n",
      "Epoch: 120, loss: 624.05, accuracy: 68.7%\n",
      "Epoch: 130, loss: 584.35, accuracy: 70.2%\n",
      "Epoch: 140, loss: 606.27, accuracy: 69.8%\n",
      "Epoch: 150, loss: 630.79, accuracy: 69.0%\n",
      "Epoch: 160, loss: 552.35, accuracy: 71.5%\n",
      "Epoch: 170, loss: 557.10, accuracy: 72.3%\n",
      "Epoch: 180, loss: 587.52, accuracy: 71.1%\n",
      "Epoch: 190, loss: 594.07, accuracy: 66.0%\n",
      "Epoch: 200, loss: 550.91, accuracy: 72.1%\n",
      "Epoch: 210, loss: 530.03, accuracy: 71.2%\n",
      "Epoch: 220, loss: 738.34, accuracy: 31.8%\n",
      "Epoch: 230, loss: 695.30, accuracy: 68.7%\n",
      "Epoch: 240, loss: 588.82, accuracy: 72.3%\n",
      "Epoch: 250, loss: 540.68, accuracy: 72.5%\n",
      "Epoch: 260, loss: 767.19, accuracy: 65.6%\n",
      "Epoch: 270, loss: 555.99, accuracy: 69.8%\n",
      "Epoch: 280, loss: 761.04, accuracy: 55.3%\n",
      "Epoch: 290, loss: 634.66, accuracy: 58.2%\n",
      "Epoch: 300, loss: 825.09, accuracy: 38.9%\n",
      "Mean accuracy on T3: 63.59\n",
      "Mean losses on T3: 671.15\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 605.99, accuracy: 68.7%\n",
      "Epoch: 20, loss: 590.76, accuracy: 72.2%\n",
      "Epoch: 30, loss: 777.28, accuracy: 55.3%\n",
      "Epoch: 40, loss: 625.85, accuracy: 67.9%\n",
      "Epoch: 50, loss: 670.72, accuracy: 55.3%\n",
      "Epoch: 60, loss: 581.96, accuracy: 71.3%\n",
      "Epoch: 70, loss: 586.11, accuracy: 71.8%\n",
      "Epoch: 80, loss: 740.26, accuracy: 55.3%\n",
      "Epoch: 90, loss: 655.27, accuracy: 65.8%\n",
      "Epoch: 100, loss: 675.91, accuracy: 62.1%\n",
      "Epoch: 110, loss: 628.74, accuracy: 68.8%\n",
      "Epoch: 120, loss: 698.97, accuracy: 67.2%\n",
      "Epoch: 130, loss: 550.34, accuracy: 73.2%\n",
      "Epoch: 140, loss: 544.41, accuracy: 72.8%\n",
      "Epoch: 150, loss: 568.58, accuracy: 73.7%\n",
      "Epoch: 160, loss: 600.81, accuracy: 72.6%\n",
      "Epoch: 170, loss: 611.08, accuracy: 71.2%\n",
      "Epoch: 180, loss: 550.41, accuracy: 70.8%\n",
      "Epoch: 190, loss: 640.20, accuracy: 68.4%\n",
      "Epoch: 200, loss: 584.12, accuracy: 74.6%\n",
      "Epoch: 210, loss: 633.13, accuracy: 70.1%\n",
      "Epoch: 220, loss: 606.19, accuracy: 72.9%\n",
      "Epoch: 230, loss: 638.33, accuracy: 68.9%\n",
      "Epoch: 240, loss: 609.50, accuracy: 73.6%\n",
      "Epoch: 250, loss: 753.70, accuracy: 65.1%\n",
      "Epoch: 260, loss: 718.99, accuracy: 38.3%\n",
      "Epoch: 270, loss: 566.24, accuracy: 67.5%\n",
      "Epoch: 280, loss: 653.44, accuracy: 58.2%\n",
      "Epoch: 290, loss: 632.12, accuracy: 66.1%\n",
      "Epoch: 300, loss: 670.28, accuracy: 73.8%\n",
      "Mean accuracy on T4: 65.81\n",
      "Mean losses on T4: 641.17\n",
      "\n",
      "Accuracy list on Ti sets: [62.71835304424004, 67.08147174770039, 57.54577310556285, 63.59088918090233, 65.81008771929824]\n",
      "Losses list on Ti sets: [679.2681839785919, 652.840276090353, 710.2984807207007, 671.1483768856376, 641.1731892991625]\n",
      "Mean accuracy on all T sets: 63.35\n",
      "Mean losses on all T sets: 670.95\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ czwarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: sigmoid\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 690.32, accuracy: 56.5%\n",
      "Epoch: 20, loss: 663.61, accuracy: 59.5%\n",
      "Epoch: 30, loss: 653.48, accuracy: 60.7%\n",
      "Epoch: 40, loss: 644.22, accuracy: 61.8%\n",
      "Epoch: 50, loss: 634.15, accuracy: 64.0%\n",
      "Epoch: 60, loss: 622.11, accuracy: 66.0%\n",
      "Epoch: 70, loss: 606.77, accuracy: 67.4%\n",
      "Epoch: 80, loss: 593.33, accuracy: 69.1%\n",
      "Epoch: 90, loss: 726.35, accuracy: 58.5%\n",
      "Epoch: 100, loss: 630.46, accuracy: 63.6%\n",
      "Epoch: 110, loss: 631.52, accuracy: 64.5%\n",
      "Epoch: 120, loss: 619.09, accuracy: 65.4%\n",
      "Epoch: 130, loss: 612.77, accuracy: 67.0%\n",
      "Epoch: 140, loss: 606.96, accuracy: 67.8%\n",
      "Epoch: 150, loss: 603.32, accuracy: 68.6%\n",
      "Epoch: 160, loss: 600.86, accuracy: 68.9%\n",
      "Epoch: 170, loss: 599.36, accuracy: 68.9%\n",
      "Epoch: 180, loss: 598.61, accuracy: 69.8%\n",
      "Epoch: 190, loss: 598.46, accuracy: 69.8%\n",
      "Epoch: 200, loss: 598.81, accuracy: 69.9%\n",
      "Epoch: 210, loss: 599.62, accuracy: 70.3%\n",
      "Epoch: 220, loss: 600.83, accuracy: 70.7%\n",
      "Epoch: 230, loss: 602.40, accuracy: 70.8%\n",
      "Epoch: 240, loss: 604.30, accuracy: 71.0%\n",
      "Epoch: 250, loss: 606.48, accuracy: 71.0%\n",
      "Epoch: 260, loss: 608.85, accuracy: 71.0%\n",
      "Epoch: 270, loss: 611.20, accuracy: 70.8%\n",
      "Epoch: 280, loss: 613.22, accuracy: 70.8%\n",
      "Epoch: 290, loss: 614.54, accuracy: 70.8%\n",
      "Epoch: 300, loss: 615.04, accuracy: 70.8%\n",
      "Mean accuracy on T0: 67.60\n",
      "Mean losses on T0: 652.97\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 732.69, accuracy: 54.3%\n",
      "Epoch: 20, loss: 707.01, accuracy: 55.2%\n",
      "Epoch: 30, loss: 696.10, accuracy: 56.4%\n",
      "Epoch: 40, loss: 686.87, accuracy: 58.2%\n",
      "Epoch: 50, loss: 676.92, accuracy: 60.6%\n",
      "Epoch: 60, loss: 664.40, accuracy: 62.9%\n",
      "Epoch: 70, loss: 651.52, accuracy: 63.3%\n",
      "Epoch: 80, loss: 767.40, accuracy: 54.9%\n",
      "Epoch: 90, loss: 680.45, accuracy: 61.4%\n",
      "Epoch: 100, loss: 680.79, accuracy: 62.2%\n",
      "Epoch: 110, loss: 669.28, accuracy: 63.1%\n",
      "Epoch: 120, loss: 662.73, accuracy: 63.9%\n",
      "Epoch: 130, loss: 656.76, accuracy: 65.0%\n",
      "Epoch: 140, loss: 653.07, accuracy: 65.4%\n",
      "Epoch: 150, loss: 650.54, accuracy: 66.0%\n",
      "Epoch: 160, loss: 649.27, accuracy: 66.0%\n",
      "Epoch: 170, loss: 648.99, accuracy: 66.0%\n",
      "Epoch: 180, loss: 649.68, accuracy: 66.5%\n",
      "Epoch: 190, loss: 651.27, accuracy: 67.1%\n",
      "Epoch: 200, loss: 653.39, accuracy: 67.1%\n",
      "Epoch: 210, loss: 654.10, accuracy: 67.3%\n",
      "Epoch: 220, loss: 652.27, accuracy: 67.1%\n",
      "Epoch: 230, loss: 652.92, accuracy: 67.0%\n",
      "Epoch: 240, loss: 653.83, accuracy: 66.9%\n",
      "Epoch: 250, loss: 654.59, accuracy: 67.0%\n",
      "Epoch: 260, loss: 654.72, accuracy: 67.0%\n",
      "Epoch: 270, loss: 653.83, accuracy: 66.9%\n",
      "Epoch: 280, loss: 651.78, accuracy: 66.8%\n",
      "Epoch: 290, loss: 648.81, accuracy: 67.9%\n",
      "Epoch: 300, loss: 645.37, accuracy: 67.8%\n",
      "Mean accuracy on T1: 64.34\n",
      "Mean losses on T1: 709.59\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 712.55, accuracy: 55.6%\n",
      "Epoch: 20, loss: 690.55, accuracy: 57.7%\n",
      "Epoch: 30, loss: 681.00, accuracy: 59.3%\n",
      "Epoch: 40, loss: 672.10, accuracy: 60.7%\n",
      "Epoch: 50, loss: 662.69, accuracy: 61.2%\n",
      "Epoch: 60, loss: 652.23, accuracy: 61.6%\n",
      "Epoch: 70, loss: 656.64, accuracy: 62.5%\n",
      "Epoch: 80, loss: 718.16, accuracy: 56.6%\n",
      "Epoch: 90, loss: 670.61, accuracy: 60.8%\n",
      "Epoch: 100, loss: 668.04, accuracy: 61.5%\n",
      "Epoch: 110, loss: 657.07, accuracy: 62.4%\n",
      "Epoch: 120, loss: 648.44, accuracy: 63.2%\n",
      "Epoch: 130, loss: 641.42, accuracy: 64.3%\n",
      "Epoch: 140, loss: 636.70, accuracy: 64.7%\n",
      "Epoch: 150, loss: 633.39, accuracy: 65.0%\n",
      "Epoch: 160, loss: 631.19, accuracy: 65.6%\n",
      "Epoch: 170, loss: 629.76, accuracy: 65.8%\n",
      "Epoch: 180, loss: 628.89, accuracy: 66.0%\n",
      "Epoch: 190, loss: 628.47, accuracy: 66.4%\n",
      "Epoch: 200, loss: 628.42, accuracy: 66.2%\n",
      "Epoch: 210, loss: 628.72, accuracy: 66.5%\n",
      "Epoch: 220, loss: 629.36, accuracy: 66.9%\n",
      "Epoch: 230, loss: 630.35, accuracy: 67.4%\n",
      "Epoch: 240, loss: 631.80, accuracy: 67.3%\n",
      "Epoch: 250, loss: 633.81, accuracy: 67.3%\n",
      "Epoch: 260, loss: 636.25, accuracy: 67.5%\n",
      "Epoch: 270, loss: 638.75, accuracy: 67.8%\n",
      "Epoch: 280, loss: 640.73, accuracy: 67.8%\n",
      "Epoch: 290, loss: 639.06, accuracy: 67.9%\n",
      "Epoch: 300, loss: 632.65, accuracy: 68.5%\n",
      "Mean accuracy on T2: 65.96\n",
      "Mean losses on T2: 675.63\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 685.61, accuracy: 59.1%\n",
      "Epoch: 20, loss: 673.21, accuracy: 59.3%\n",
      "Epoch: 30, loss: 668.09, accuracy: 60.8%\n",
      "Epoch: 40, loss: 661.85, accuracy: 60.8%\n",
      "Epoch: 50, loss: 654.02, accuracy: 61.9%\n",
      "Epoch: 60, loss: 643.26, accuracy: 62.7%\n",
      "Epoch: 70, loss: 627.48, accuracy: 64.8%\n",
      "Epoch: 80, loss: 718.04, accuracy: 59.7%\n",
      "Epoch: 90, loss: 642.27, accuracy: 65.6%\n",
      "Epoch: 100, loss: 643.14, accuracy: 66.1%\n",
      "Epoch: 110, loss: 629.92, accuracy: 66.5%\n",
      "Epoch: 120, loss: 624.07, accuracy: 67.8%\n",
      "Epoch: 130, loss: 618.10, accuracy: 68.7%\n",
      "Epoch: 140, loss: 614.06, accuracy: 69.0%\n",
      "Epoch: 150, loss: 611.11, accuracy: 69.8%\n",
      "Epoch: 160, loss: 608.89, accuracy: 69.8%\n",
      "Epoch: 170, loss: 607.20, accuracy: 69.9%\n",
      "Epoch: 180, loss: 605.93, accuracy: 69.9%\n",
      "Epoch: 190, loss: 605.13, accuracy: 69.9%\n",
      "Epoch: 200, loss: 605.11, accuracy: 69.8%\n",
      "Epoch: 210, loss: 606.15, accuracy: 69.8%\n",
      "Epoch: 220, loss: 607.04, accuracy: 69.6%\n",
      "Epoch: 230, loss: 605.90, accuracy: 69.5%\n",
      "Epoch: 240, loss: 601.51, accuracy: 69.3%\n",
      "Epoch: 250, loss: 599.36, accuracy: 69.1%\n",
      "Epoch: 260, loss: 598.59, accuracy: 69.6%\n",
      "Epoch: 270, loss: 598.12, accuracy: 69.6%\n",
      "Epoch: 280, loss: 597.76, accuracy: 69.5%\n",
      "Epoch: 290, loss: 597.50, accuracy: 69.8%\n",
      "Epoch: 300, loss: 597.24, accuracy: 69.5%\n",
      "Mean accuracy on T3: 66.16\n",
      "Mean losses on T3: 679.81\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 689.29, accuracy: 58.6%\n",
      "Epoch: 20, loss: 671.30, accuracy: 60.4%\n",
      "Epoch: 30, loss: 665.16, accuracy: 61.3%\n",
      "Epoch: 40, loss: 658.65, accuracy: 62.5%\n",
      "Epoch: 50, loss: 650.81, accuracy: 63.6%\n",
      "Epoch: 60, loss: 640.05, accuracy: 64.6%\n",
      "Epoch: 70, loss: 622.86, accuracy: 65.5%\n",
      "Epoch: 80, loss: 669.43, accuracy: 59.9%\n",
      "Epoch: 90, loss: 638.50, accuracy: 65.3%\n",
      "Epoch: 100, loss: 629.54, accuracy: 66.1%\n",
      "Epoch: 110, loss: 614.61, accuracy: 68.0%\n",
      "Epoch: 120, loss: 603.65, accuracy: 68.8%\n",
      "Epoch: 130, loss: 594.16, accuracy: 69.7%\n",
      "Epoch: 140, loss: 587.43, accuracy: 70.4%\n",
      "Epoch: 150, loss: 582.16, accuracy: 70.3%\n",
      "Epoch: 160, loss: 578.31, accuracy: 70.0%\n",
      "Epoch: 170, loss: 575.36, accuracy: 70.7%\n",
      "Epoch: 180, loss: 573.18, accuracy: 70.5%\n",
      "Epoch: 190, loss: 571.61, accuracy: 70.8%\n",
      "Epoch: 200, loss: 570.56, accuracy: 70.9%\n",
      "Epoch: 210, loss: 569.94, accuracy: 71.1%\n",
      "Epoch: 220, loss: 569.68, accuracy: 71.1%\n",
      "Epoch: 230, loss: 569.69, accuracy: 70.9%\n",
      "Epoch: 240, loss: 569.90, accuracy: 71.1%\n",
      "Epoch: 250, loss: 570.25, accuracy: 71.2%\n",
      "Epoch: 260, loss: 570.64, accuracy: 71.3%\n",
      "Epoch: 270, loss: 570.97, accuracy: 71.4%\n",
      "Epoch: 280, loss: 571.04, accuracy: 71.3%\n",
      "Epoch: 290, loss: 570.53, accuracy: 71.4%\n",
      "Epoch: 300, loss: 569.18, accuracy: 71.4%\n",
      "Mean accuracy on T4: 67.39\n",
      "Mean losses on T4: 649.04\n",
      "\n",
      "Accuracy list on Ti sets: [67.60183968462549, 64.33990363556724, 65.96364432763907, 66.15681121331582, 67.38947368421051]\n",
      "Losses list on Ti sets: [652.9712824416575, 709.5870759152396, 675.6256214979201, 679.8083519499722, 649.0354016114874]\n",
      "Mean accuracy on all T sets: 66.29\n",
      "Mean losses on all T sets: 673.41\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ piÄ…ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: relu\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 986.92, accuracy: 32.5%\n",
      "Epoch: 20, loss: 589.43, accuracy: 71.1%\n",
      "Epoch: 30, loss: 617.99, accuracy: 66.5%\n",
      "Epoch: 40, loss: 634.90, accuracy: 68.3%\n",
      "Epoch: 50, loss: 662.21, accuracy: 62.9%\n",
      "Epoch: 60, loss: 722.80, accuracy: 57.0%\n",
      "Epoch: 70, loss: 682.21, accuracy: 61.5%\n",
      "Epoch: 80, loss: 656.30, accuracy: 69.5%\n",
      "Epoch: 90, loss: 577.57, accuracy: 71.5%\n",
      "Epoch: 100, loss: 738.60, accuracy: 64.3%\n",
      "Epoch: 110, loss: 596.67, accuracy: 71.1%\n",
      "Epoch: 120, loss: 567.63, accuracy: 71.7%\n",
      "Epoch: 130, loss: 613.83, accuracy: 71.4%\n",
      "Epoch: 140, loss: 586.12, accuracy: 73.3%\n",
      "Epoch: 150, loss: 750.53, accuracy: 53.1%\n",
      "Epoch: 160, loss: 593.90, accuracy: 72.5%\n",
      "Epoch: 170, loss: 581.74, accuracy: 72.5%\n",
      "Epoch: 180, loss: 642.85, accuracy: 71.6%\n",
      "Epoch: 190, loss: 586.79, accuracy: 72.0%\n",
      "Epoch: 200, loss: 540.47, accuracy: 73.9%\n",
      "Epoch: 210, loss: 573.46, accuracy: 73.5%\n",
      "Epoch: 220, loss: 638.90, accuracy: 71.9%\n",
      "Epoch: 230, loss: 561.22, accuracy: 73.1%\n",
      "Epoch: 240, loss: 647.42, accuracy: 71.7%\n",
      "Epoch: 250, loss: 571.20, accuracy: 70.3%\n",
      "Epoch: 260, loss: 573.75, accuracy: 69.3%\n",
      "Epoch: 270, loss: 609.69, accuracy: 69.8%\n",
      "Epoch: 280, loss: 645.60, accuracy: 68.3%\n",
      "Epoch: 290, loss: 565.25, accuracy: 70.6%\n",
      "Epoch: 300, loss: 707.38, accuracy: 70.6%\n",
      "Mean accuracy on T0: 67.06\n",
      "Mean losses on T0: 649.69\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 1126.19, accuracy: 29.6%\n",
      "Epoch: 20, loss: 615.95, accuracy: 70.0%\n",
      "Epoch: 30, loss: 642.16, accuracy: 66.9%\n",
      "Epoch: 40, loss: 832.16, accuracy: 54.9%\n",
      "Epoch: 50, loss: 630.44, accuracy: 69.8%\n",
      "Epoch: 60, loss: 617.75, accuracy: 71.1%\n",
      "Epoch: 70, loss: 585.90, accuracy: 70.6%\n",
      "Epoch: 80, loss: 863.36, accuracy: 64.0%\n",
      "Epoch: 90, loss: 890.94, accuracy: 59.3%\n",
      "Epoch: 100, loss: 650.10, accuracy: 67.5%\n",
      "Epoch: 110, loss: 620.28, accuracy: 69.5%\n",
      "Epoch: 120, loss: 664.12, accuracy: 67.0%\n",
      "Epoch: 130, loss: 630.90, accuracy: 69.8%\n",
      "Epoch: 140, loss: 644.63, accuracy: 69.5%\n",
      "Epoch: 150, loss: 672.27, accuracy: 71.5%\n",
      "Epoch: 160, loss: 697.46, accuracy: 69.1%\n",
      "Epoch: 170, loss: 533.51, accuracy: 72.1%\n",
      "Epoch: 180, loss: 610.32, accuracy: 66.5%\n",
      "Epoch: 190, loss: 548.78, accuracy: 71.6%\n",
      "Epoch: 200, loss: 663.43, accuracy: 65.8%\n",
      "Epoch: 210, loss: 671.05, accuracy: 68.1%\n",
      "Epoch: 220, loss: 602.75, accuracy: 71.2%\n",
      "Epoch: 230, loss: 597.55, accuracy: 69.5%\n",
      "Epoch: 240, loss: 561.27, accuracy: 71.9%\n",
      "Epoch: 250, loss: 660.34, accuracy: 69.8%\n",
      "Epoch: 260, loss: 558.74, accuracy: 71.6%\n",
      "Epoch: 270, loss: 698.66, accuracy: 72.9%\n",
      "Epoch: 280, loss: 556.76, accuracy: 74.5%\n",
      "Epoch: 290, loss: 678.28, accuracy: 66.2%\n",
      "Epoch: 300, loss: 568.05, accuracy: 72.7%\n",
      "Mean accuracy on T1: 67.89\n",
      "Mean losses on T1: 653.17\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 1079.72, accuracy: 32.1%\n",
      "Epoch: 20, loss: 718.54, accuracy: 54.8%\n",
      "Epoch: 30, loss: 662.30, accuracy: 65.7%\n",
      "Epoch: 40, loss: 676.20, accuracy: 65.8%\n",
      "Epoch: 50, loss: 655.24, accuracy: 66.8%\n",
      "Epoch: 60, loss: 659.70, accuracy: 66.8%\n",
      "Epoch: 70, loss: 652.32, accuracy: 67.3%\n",
      "Epoch: 80, loss: 630.18, accuracy: 68.1%\n",
      "Epoch: 90, loss: 629.99, accuracy: 69.5%\n",
      "Epoch: 100, loss: 679.43, accuracy: 67.4%\n",
      "Epoch: 110, loss: 617.61, accuracy: 69.9%\n",
      "Epoch: 120, loss: 568.12, accuracy: 68.3%\n",
      "Epoch: 130, loss: 598.84, accuracy: 69.0%\n",
      "Epoch: 140, loss: 578.88, accuracy: 69.9%\n",
      "Epoch: 150, loss: 682.18, accuracy: 70.0%\n",
      "Epoch: 160, loss: 680.77, accuracy: 69.3%\n",
      "Epoch: 170, loss: 688.79, accuracy: 68.5%\n",
      "Epoch: 180, loss: 661.86, accuracy: 63.7%\n",
      "Epoch: 190, loss: 955.96, accuracy: 57.7%\n",
      "Epoch: 200, loss: 608.85, accuracy: 70.4%\n",
      "Epoch: 210, loss: 719.74, accuracy: 66.6%\n",
      "Epoch: 220, loss: 769.67, accuracy: 67.1%\n",
      "Epoch: 230, loss: 739.54, accuracy: 65.0%\n",
      "Epoch: 240, loss: 593.55, accuracy: 69.5%\n",
      "Epoch: 250, loss: 790.25, accuracy: 63.3%\n",
      "Epoch: 260, loss: 667.44, accuracy: 68.7%\n",
      "Epoch: 270, loss: 652.22, accuracy: 69.8%\n",
      "Epoch: 280, loss: 695.28, accuracy: 71.4%\n",
      "Epoch: 290, loss: 834.86, accuracy: 71.5%\n",
      "Epoch: 300, loss: 713.90, accuracy: 64.5%\n",
      "Mean accuracy on T2: 65.38\n",
      "Mean losses on T2: 699.89\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 832.85, accuracy: 39.3%\n",
      "Epoch: 20, loss: 645.57, accuracy: 68.9%\n",
      "Epoch: 30, loss: 744.99, accuracy: 62.0%\n",
      "Epoch: 40, loss: 693.82, accuracy: 58.2%\n",
      "Epoch: 50, loss: 593.15, accuracy: 70.6%\n",
      "Epoch: 60, loss: 581.51, accuracy: 69.9%\n",
      "Epoch: 70, loss: 555.83, accuracy: 70.6%\n",
      "Epoch: 80, loss: 683.24, accuracy: 59.3%\n",
      "Epoch: 90, loss: 555.76, accuracy: 72.4%\n",
      "Epoch: 100, loss: 701.30, accuracy: 61.5%\n",
      "Epoch: 110, loss: 612.66, accuracy: 67.3%\n",
      "Epoch: 120, loss: 627.92, accuracy: 66.6%\n",
      "Epoch: 130, loss: 575.04, accuracy: 68.9%\n",
      "Epoch: 140, loss: 621.08, accuracy: 67.4%\n",
      "Epoch: 150, loss: 566.83, accuracy: 69.9%\n",
      "Epoch: 160, loss: 605.05, accuracy: 68.5%\n",
      "Epoch: 170, loss: 576.10, accuracy: 69.3%\n",
      "Epoch: 180, loss: 726.14, accuracy: 62.4%\n",
      "Epoch: 190, loss: 578.40, accuracy: 69.5%\n",
      "Epoch: 200, loss: 509.81, accuracy: 73.9%\n",
      "Epoch: 210, loss: 869.22, accuracy: 67.7%\n",
      "Epoch: 220, loss: 549.09, accuracy: 72.7%\n",
      "Epoch: 230, loss: 544.69, accuracy: 72.9%\n",
      "Epoch: 240, loss: 531.88, accuracy: 73.2%\n",
      "Epoch: 250, loss: 535.68, accuracy: 72.1%\n",
      "Epoch: 260, loss: 546.15, accuracy: 71.5%\n",
      "Epoch: 270, loss: 558.05, accuracy: 72.8%\n",
      "Epoch: 280, loss: 704.44, accuracy: 63.3%\n",
      "Epoch: 290, loss: 814.51, accuracy: 65.7%\n",
      "Epoch: 300, loss: 646.93, accuracy: 64.0%\n",
      "Mean accuracy on T3: 67.91\n",
      "Mean losses on T3: 637.85\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 1020.15, accuracy: 34.9%\n",
      "Epoch: 20, loss: 665.77, accuracy: 59.9%\n",
      "Epoch: 30, loss: 617.22, accuracy: 67.6%\n",
      "Epoch: 40, loss: 626.28, accuracy: 68.8%\n",
      "Epoch: 50, loss: 609.10, accuracy: 69.5%\n",
      "Epoch: 60, loss: 598.56, accuracy: 69.7%\n",
      "Epoch: 70, loss: 537.78, accuracy: 71.1%\n",
      "Epoch: 80, loss: 713.45, accuracy: 64.7%\n",
      "Epoch: 90, loss: 533.10, accuracy: 71.3%\n",
      "Epoch: 100, loss: 585.23, accuracy: 68.3%\n",
      "Epoch: 110, loss: 558.77, accuracy: 71.2%\n",
      "Epoch: 120, loss: 552.98, accuracy: 70.8%\n",
      "Epoch: 130, loss: 651.19, accuracy: 66.7%\n",
      "Epoch: 140, loss: 556.89, accuracy: 71.1%\n",
      "Epoch: 150, loss: 749.75, accuracy: 58.3%\n",
      "Epoch: 160, loss: 652.26, accuracy: 70.8%\n",
      "Epoch: 170, loss: 630.18, accuracy: 70.9%\n",
      "Epoch: 180, loss: 813.74, accuracy: 61.3%\n",
      "Epoch: 190, loss: 535.17, accuracy: 70.9%\n",
      "Epoch: 200, loss: 489.89, accuracy: 74.3%\n",
      "Epoch: 210, loss: 721.42, accuracy: 62.4%\n",
      "Epoch: 220, loss: 718.76, accuracy: 65.8%\n",
      "Epoch: 230, loss: 554.63, accuracy: 73.9%\n",
      "Epoch: 240, loss: 557.97, accuracy: 73.3%\n",
      "Epoch: 250, loss: 591.73, accuracy: 68.9%\n",
      "Epoch: 260, loss: 765.30, accuracy: 68.6%\n",
      "Epoch: 270, loss: 639.33, accuracy: 69.7%\n",
      "Epoch: 280, loss: 600.27, accuracy: 74.5%\n",
      "Epoch: 290, loss: 989.20, accuracy: 58.2%\n",
      "Epoch: 300, loss: 650.74, accuracy: 65.4%\n",
      "Mean accuracy on T4: 68.20\n",
      "Mean losses on T4: 627.26\n",
      "\n",
      "Accuracy list on Ti sets: [67.06263688129654, 67.88655278142797, 65.3766973280771, 67.91458607095927, 68.19605263157895]\n",
      "Losses list on Ti sets: [649.6852651006229, 653.1673659012359, 699.8942856284602, 637.8502715191005, 627.2594673238398]\n",
      "Mean accuracy on all T sets: 67.29\n",
      "Mean losses on all T sets: 653.57\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ szÃ³sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: sigmoid\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 955.74, accuracy: 57.6%\n",
      "Epoch: 20, loss: 761.07, accuracy: 57.6%\n",
      "Epoch: 30, loss: 713.44, accuracy: 57.2%\n",
      "Epoch: 40, loss: 698.12, accuracy: 57.6%\n",
      "Epoch: 50, loss: 690.25, accuracy: 57.7%\n",
      "Epoch: 60, loss: 684.88, accuracy: 57.7%\n",
      "Epoch: 70, loss: 680.96, accuracy: 58.1%\n",
      "Epoch: 80, loss: 678.76, accuracy: 59.3%\n",
      "Epoch: 90, loss: 678.53, accuracy: 59.3%\n",
      "Epoch: 100, loss: 677.10, accuracy: 60.2%\n",
      "Epoch: 110, loss: 671.95, accuracy: 61.2%\n",
      "Epoch: 120, loss: 664.78, accuracy: 62.2%\n",
      "Epoch: 130, loss: 658.72, accuracy: 63.9%\n",
      "Epoch: 140, loss: 653.38, accuracy: 65.4%\n",
      "Epoch: 150, loss: 647.96, accuracy: 66.6%\n",
      "Epoch: 160, loss: 643.35, accuracy: 67.3%\n",
      "Epoch: 170, loss: 639.78, accuracy: 67.9%\n",
      "Epoch: 180, loss: 637.23, accuracy: 68.2%\n",
      "Epoch: 190, loss: 635.50, accuracy: 69.0%\n",
      "Epoch: 200, loss: 634.41, accuracy: 68.9%\n",
      "Epoch: 210, loss: 633.83, accuracy: 69.4%\n",
      "Epoch: 220, loss: 633.68, accuracy: 69.5%\n",
      "Epoch: 230, loss: 633.92, accuracy: 69.6%\n",
      "Epoch: 240, loss: 634.54, accuracy: 69.8%\n",
      "Epoch: 250, loss: 635.51, accuracy: 69.6%\n",
      "Epoch: 260, loss: 636.72, accuracy: 69.9%\n",
      "Epoch: 270, loss: 637.92, accuracy: 70.0%\n",
      "Epoch: 280, loss: 638.84, accuracy: 69.9%\n",
      "Epoch: 290, loss: 639.32, accuracy: 70.2%\n",
      "Epoch: 300, loss: 639.43, accuracy: 70.8%\n",
      "Mean accuracy on T0: 64.44\n",
      "Mean losses on T0: 750.03\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 1004.73, accuracy: 55.2%\n",
      "Epoch: 20, loss: 796.37, accuracy: 55.2%\n",
      "Epoch: 30, loss: 740.27, accuracy: 54.9%\n",
      "Epoch: 40, loss: 721.49, accuracy: 54.9%\n",
      "Epoch: 50, loss: 711.29, accuracy: 54.5%\n",
      "Epoch: 60, loss: 703.83, accuracy: 55.7%\n",
      "Epoch: 70, loss: 697.60, accuracy: 56.0%\n",
      "Epoch: 80, loss: 692.10, accuracy: 56.1%\n",
      "Epoch: 90, loss: 687.23, accuracy: 56.6%\n",
      "Epoch: 100, loss: 680.89, accuracy: 58.5%\n",
      "Epoch: 110, loss: 669.92, accuracy: 61.5%\n",
      "Epoch: 120, loss: 656.35, accuracy: 63.5%\n",
      "Epoch: 130, loss: 643.58, accuracy: 64.9%\n",
      "Epoch: 140, loss: 632.31, accuracy: 66.6%\n",
      "Epoch: 150, loss: 623.23, accuracy: 67.5%\n",
      "Epoch: 160, loss: 616.40, accuracy: 67.7%\n",
      "Epoch: 170, loss: 611.42, accuracy: 68.5%\n",
      "Epoch: 180, loss: 607.84, accuracy: 68.9%\n",
      "Epoch: 190, loss: 605.31, accuracy: 68.7%\n",
      "Epoch: 200, loss: 603.59, accuracy: 68.9%\n",
      "Epoch: 210, loss: 602.57, accuracy: 68.9%\n",
      "Epoch: 220, loss: 602.23, accuracy: 69.1%\n",
      "Epoch: 230, loss: 602.56, accuracy: 69.3%\n",
      "Epoch: 240, loss: 603.49, accuracy: 69.1%\n",
      "Epoch: 250, loss: 604.79, accuracy: 69.5%\n",
      "Epoch: 260, loss: 606.15, accuracy: 69.3%\n",
      "Epoch: 270, loss: 607.42, accuracy: 69.5%\n",
      "Epoch: 280, loss: 608.49, accuracy: 69.5%\n",
      "Epoch: 290, loss: 609.26, accuracy: 69.5%\n",
      "Epoch: 300, loss: 609.71, accuracy: 69.8%\n",
      "Mean accuracy on T1: 64.31\n",
      "Mean losses on T1: 735.25\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 968.65, accuracy: 57.0%\n",
      "Epoch: 20, loss: 769.71, accuracy: 57.0%\n",
      "Epoch: 30, loss: 716.93, accuracy: 57.3%\n",
      "Epoch: 40, loss: 699.00, accuracy: 57.4%\n",
      "Epoch: 50, loss: 689.24, accuracy: 57.4%\n",
      "Epoch: 60, loss: 682.31, accuracy: 57.8%\n",
      "Epoch: 70, loss: 676.97, accuracy: 58.6%\n",
      "Epoch: 80, loss: 673.40, accuracy: 57.8%\n",
      "Epoch: 90, loss: 672.79, accuracy: 57.7%\n",
      "Epoch: 100, loss: 672.16, accuracy: 59.1%\n",
      "Epoch: 110, loss: 665.19, accuracy: 62.4%\n",
      "Epoch: 120, loss: 653.77, accuracy: 64.3%\n",
      "Epoch: 130, loss: 642.74, accuracy: 64.9%\n",
      "Epoch: 140, loss: 633.57, accuracy: 65.7%\n",
      "Epoch: 150, loss: 626.43, accuracy: 66.1%\n",
      "Epoch: 160, loss: 620.85, accuracy: 67.0%\n",
      "Epoch: 170, loss: 616.38, accuracy: 67.5%\n",
      "Epoch: 180, loss: 612.86, accuracy: 67.9%\n",
      "Epoch: 190, loss: 610.21, accuracy: 68.5%\n",
      "Epoch: 200, loss: 608.30, accuracy: 68.7%\n",
      "Epoch: 210, loss: 607.03, accuracy: 68.5%\n",
      "Epoch: 220, loss: 606.32, accuracy: 68.2%\n",
      "Epoch: 230, loss: 606.11, accuracy: 68.3%\n",
      "Epoch: 240, loss: 606.32, accuracy: 68.3%\n",
      "Epoch: 250, loss: 606.87, accuracy: 68.5%\n",
      "Epoch: 260, loss: 607.69, accuracy: 68.3%\n",
      "Epoch: 270, loss: 608.69, accuracy: 68.5%\n",
      "Epoch: 280, loss: 609.80, accuracy: 68.7%\n",
      "Epoch: 290, loss: 610.95, accuracy: 69.0%\n",
      "Epoch: 300, loss: 612.09, accuracy: 69.3%\n",
      "Mean accuracy on T2: 65.22\n",
      "Mean losses on T2: 724.14\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 967.24, accuracy: 57.7%\n",
      "Epoch: 20, loss: 760.00, accuracy: 57.7%\n",
      "Epoch: 30, loss: 708.70, accuracy: 57.6%\n",
      "Epoch: 40, loss: 691.35, accuracy: 56.9%\n",
      "Epoch: 50, loss: 681.81, accuracy: 57.2%\n",
      "Epoch: 60, loss: 674.83, accuracy: 57.6%\n",
      "Epoch: 70, loss: 669.00, accuracy: 58.5%\n",
      "Epoch: 80, loss: 663.89, accuracy: 58.6%\n",
      "Epoch: 90, loss: 659.61, accuracy: 59.3%\n",
      "Epoch: 100, loss: 655.23, accuracy: 61.5%\n",
      "Epoch: 110, loss: 647.20, accuracy: 63.2%\n",
      "Epoch: 120, loss: 635.26, accuracy: 65.6%\n",
      "Epoch: 130, loss: 623.65, accuracy: 66.8%\n",
      "Epoch: 140, loss: 614.30, accuracy: 67.0%\n",
      "Epoch: 150, loss: 607.54, accuracy: 68.1%\n",
      "Epoch: 160, loss: 602.71, accuracy: 68.1%\n",
      "Epoch: 170, loss: 599.21, accuracy: 68.7%\n",
      "Epoch: 180, loss: 596.65, accuracy: 69.1%\n",
      "Epoch: 190, loss: 594.79, accuracy: 69.3%\n",
      "Epoch: 200, loss: 593.51, accuracy: 69.3%\n",
      "Epoch: 210, loss: 592.76, accuracy: 69.4%\n",
      "Epoch: 220, loss: 592.65, accuracy: 69.3%\n",
      "Epoch: 230, loss: 593.45, accuracy: 69.1%\n",
      "Epoch: 240, loss: 595.37, accuracy: 69.5%\n",
      "Epoch: 250, loss: 597.66, accuracy: 69.8%\n",
      "Epoch: 260, loss: 599.89, accuracy: 69.8%\n",
      "Epoch: 270, loss: 602.11, accuracy: 69.9%\n",
      "Epoch: 280, loss: 604.01, accuracy: 69.9%\n",
      "Epoch: 290, loss: 603.30, accuracy: 69.9%\n",
      "Epoch: 300, loss: 600.15, accuracy: 70.3%\n",
      "Mean accuracy on T3: 65.22\n",
      "Mean losses on T3: 727.02\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 974.99, accuracy: 56.7%\n",
      "Epoch: 20, loss: 772.43, accuracy: 56.7%\n",
      "Epoch: 30, loss: 720.21, accuracy: 56.4%\n",
      "Epoch: 40, loss: 702.61, accuracy: 56.3%\n",
      "Epoch: 50, loss: 692.99, accuracy: 56.2%\n",
      "Epoch: 60, loss: 686.07, accuracy: 55.9%\n",
      "Epoch: 70, loss: 680.57, accuracy: 56.4%\n",
      "Epoch: 80, loss: 676.41, accuracy: 56.8%\n",
      "Epoch: 90, loss: 676.40, accuracy: 57.8%\n",
      "Epoch: 100, loss: 684.36, accuracy: 59.5%\n",
      "Epoch: 110, loss: 668.97, accuracy: 60.9%\n",
      "Epoch: 120, loss: 651.91, accuracy: 63.3%\n",
      "Epoch: 130, loss: 638.87, accuracy: 64.7%\n",
      "Epoch: 140, loss: 630.27, accuracy: 65.8%\n",
      "Epoch: 150, loss: 624.94, accuracy: 66.4%\n",
      "Epoch: 160, loss: 621.67, accuracy: 66.8%\n",
      "Epoch: 170, loss: 619.63, accuracy: 67.1%\n",
      "Epoch: 180, loss: 618.35, accuracy: 67.4%\n",
      "Epoch: 190, loss: 617.54, accuracy: 67.6%\n",
      "Epoch: 200, loss: 616.98, accuracy: 67.6%\n",
      "Epoch: 210, loss: 616.60, accuracy: 67.6%\n",
      "Epoch: 220, loss: 616.36, accuracy: 67.9%\n",
      "Epoch: 230, loss: 616.27, accuracy: 68.0%\n",
      "Epoch: 240, loss: 616.34, accuracy: 68.6%\n",
      "Epoch: 250, loss: 616.57, accuracy: 68.9%\n",
      "Epoch: 260, loss: 616.93, accuracy: 68.9%\n",
      "Epoch: 270, loss: 617.38, accuracy: 68.7%\n",
      "Epoch: 280, loss: 617.90, accuracy: 68.6%\n",
      "Epoch: 290, loss: 618.46, accuracy: 68.3%\n",
      "Epoch: 300, loss: 619.07, accuracy: 68.6%\n",
      "Mean accuracy on T4: 64.79\n",
      "Mean losses on T4: 731.66\n",
      "\n",
      "Accuracy list on Ti sets: [64.437144108629, 64.30792816469557, 65.21901007446343, 65.2185720543145, 64.79429824561403]\n",
      "Losses list on Ti sets: [750.0288965773483, 735.2548493068928, 724.1354919485572, 727.0246373628714, 731.6595909508367]\n",
      "Mean accuracy on all T sets: 64.80\n",
      "Mean losses on all T sets: 733.62\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ siÃ³dma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 1000, activation: sigmoid\n",
      "2. Layer - input_dim: 1000, output_dim: 100, activation: sigmoid\n",
      "3. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=1000, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=1000, output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 725.89, accuracy: 56.9%\n",
      "Epoch: 20, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 30, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 40, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 50, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 60, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 70, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 80, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 90, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 100, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 110, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 120, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 130, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 140, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 150, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 160, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 170, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 180, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 190, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 200, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 210, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 220, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 230, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 240, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 250, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 260, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 270, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 280, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 290, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 300, loss: 726.06, accuracy: 56.9%\n",
      "Mean accuracy on T0: 56.81\n",
      "Mean losses on T0: 751.17\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 690.61, accuracy: 58.1%\n",
      "Epoch: 20, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 30, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 40, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 50, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 60, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 70, loss: 690.54, accuracy: 58.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-77c65f5451b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_fold_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/version-control/pszt-projekt/pszt/net.py\u001b[0m in \u001b[0;36mk_fold_validation\u001b[0;34m(self, x, y_true, k, epochs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# Calculate accuracy of the trained network on test set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0maccuracies_curr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/version-control/pszt-projekt/pszt/net.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, y_true, forward)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# Perform forward step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# Get y_pred values from memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/version-control/pszt-projekt/pszt/net.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Calculate input with weights and perform activation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
