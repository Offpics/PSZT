{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pszt import net\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = net.MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (3804, 8257), y_train.shape: (3804, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load('x_train_3k.npy')\n",
    "y_train = np.load('y_train_3k.npy')\n",
    "print(f'x_train.shape: {x_train.shape}, y_train.shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 1000, activation: relu\n"
     ]
    }
   ],
   "source": [
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=1000, activation=\"relu\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 1000, activation: relu\n",
      "2. Layer - input_dim: 1000, output_dim: 20, activation: relu\n"
     ]
    }
   ],
   "source": [
    "mlp.add_layer(input_dim=1000, output_dim=20, activation=\"relu\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 1000, activation: relu\n",
      "2. Layer - input_dim: 1000, output_dim: 20, activation: relu\n",
      "3. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.init_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00746027, 0.00286397, 0.00247484, ..., 0.00157153, 0.00199117,\n",
       "        0.00181742],\n",
       "       [0.0062533 , 0.00692192, 0.00165462, ..., 0.00939389, 0.00790554,\n",
       "        0.00873649],\n",
       "       [0.00063535, 0.00792251, 0.0065162 , ..., 0.00590421, 0.00094134,\n",
       "        0.008092  ],\n",
       "       ...,\n",
       "       [0.00265788, 0.00328612, 0.00092609, ..., 0.00509776, 0.0022551 ,\n",
       "        0.0044943 ],\n",
       "       [0.00386499, 0.00739242, 0.0050387 , ..., 0.00258935, 0.00839439,\n",
       "        0.00478732],\n",
       "       [0.00560693, 0.00367259, 0.00966889, ..., 0.00268706, 0.00577382,\n",
       "        0.00304974]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.param_values['w2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLP' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f65efd7e6b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLP' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "mlp.forward(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(mlp.memory['a1'], mlp.param_values['w2']) + mlp.param_values['b2']\n",
    "a = 1/(1+np.exp(-z))\n",
    "print(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4185.45, accuracy: 32.0%\n",
      "loss: 9497.28, accuracy: 56.8%\n",
      "loss: 5708.75, accuracy: 32.0%\n",
      "loss: 3817.88, accuracy: 56.8%\n",
      "loss: 3576.77, accuracy: 56.8%\n",
      "loss: 3547.72, accuracy: 56.8%\n",
      "loss: 3540.61, accuracy: 56.8%\n",
      "loss: 3538.77, accuracy: 56.8%\n",
      "loss: 3538.19, accuracy: 56.8%\n",
      "loss: 3538.00, accuracy: 56.8%\n",
      "loss: 3537.94, accuracy: 56.8%\n",
      "loss: 3537.92, accuracy: 56.8%\n",
      "loss: 3537.92, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n",
      "loss: 3537.91, accuracy: 56.8%\n"
     ]
    }
   ],
   "source": [
    "mlp.train(x_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.memory['a2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    mlp.train(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
