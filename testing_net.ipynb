{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pszt import net\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = net.MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (3804, 8257), y_train.shape: (3804, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load('x_train_3k.npy')\n",
    "y_train = np.load('y_train_3k.npy')\n",
    "print(f'x_train.shape: {x_train.shape}, y_train.shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 4, activation: sigmoid\n"
     ]
    }
   ],
   "source": [
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=4, activation=\"sigmoid\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 4, activation: sigmoid\n",
      "2. Layer - input_dim: 4, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp.add_layer(input_dim=4, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.init_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.param_values['w2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'input_dim': 4, 'output_dim': 3, 'activation': 'softmax'}\n",
      "1\n",
      "{'input_dim': 8257, 'output_dim': 4, 'activation': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "for i, layer in reversed(list(enumerate(mlp.layers, start=1))):\n",
    "    print(i)\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2216.71, accuracy: 76.8%\n",
      "loss: 2393.07, accuracy: 75.7%\n",
      "loss: 2201.88, accuracy: 76.9%\n",
      "loss: 2381.30, accuracy: 75.7%\n",
      "loss: 2187.17, accuracy: 77.1%\n",
      "loss: 2369.69, accuracy: 75.8%\n",
      "loss: 2172.59, accuracy: 77.2%\n",
      "loss: 2358.23, accuracy: 75.9%\n",
      "loss: 2158.14, accuracy: 77.4%\n",
      "loss: 2346.93, accuracy: 76.1%\n",
      "loss: 2143.81, accuracy: 77.5%\n",
      "loss: 2335.76, accuracy: 76.2%\n",
      "loss: 2129.61, accuracy: 77.6%\n",
      "loss: 2324.72, accuracy: 76.3%\n",
      "loss: 2115.53, accuracy: 77.7%\n",
      "loss: 2313.78, accuracy: 76.4%\n",
      "loss: 2101.56, accuracy: 77.9%\n",
      "loss: 2302.90, accuracy: 76.5%\n",
      "loss: 2087.68, accuracy: 78.0%\n",
      "loss: 2292.05, accuracy: 76.6%\n",
      "loss: 2073.88, accuracy: 78.1%\n",
      "loss: 2281.19, accuracy: 76.7%\n",
      "loss: 2060.14, accuracy: 78.3%\n",
      "loss: 2270.25, accuracy: 76.9%\n",
      "loss: 2046.44, accuracy: 78.4%\n",
      "loss: 2259.20, accuracy: 77.0%\n",
      "loss: 2032.76, accuracy: 78.6%\n",
      "loss: 2248.00, accuracy: 77.2%\n",
      "loss: 2019.09, accuracy: 78.8%\n",
      "loss: 2236.62, accuracy: 77.3%\n",
      "loss: 2005.42, accuracy: 78.8%\n",
      "loss: 2225.08, accuracy: 77.4%\n",
      "loss: 1991.74, accuracy: 79.0%\n",
      "loss: 2213.39, accuracy: 77.6%\n",
      "loss: 1978.07, accuracy: 79.3%\n",
      "loss: 2201.61, accuracy: 77.6%\n",
      "loss: 1964.43, accuracy: 79.5%\n",
      "loss: 2189.84, accuracy: 77.7%\n",
      "loss: 1950.86, accuracy: 79.8%\n",
      "loss: 2178.17, accuracy: 77.7%\n",
      "loss: 1937.38, accuracy: 80.0%\n",
      "loss: 2166.72, accuracy: 77.8%\n",
      "loss: 1924.04, accuracy: 80.2%\n",
      "loss: 2155.63, accuracy: 77.9%\n",
      "loss: 1910.88, accuracy: 80.4%\n",
      "loss: 2145.02, accuracy: 78.1%\n",
      "loss: 1897.95, accuracy: 80.5%\n",
      "loss: 2135.05, accuracy: 78.3%\n",
      "loss: 1885.28, accuracy: 80.6%\n",
      "loss: 2125.88, accuracy: 78.4%\n",
      "loss: 1872.92, accuracy: 80.7%\n",
      "loss: 2117.76, accuracy: 78.7%\n",
      "loss: 1860.92, accuracy: 80.8%\n",
      "loss: 2111.02, accuracy: 78.8%\n",
      "loss: 1849.37, accuracy: 80.9%\n",
      "loss: 2106.30, accuracy: 79.1%\n",
      "loss: 1838.50, accuracy: 81.1%\n",
      "loss: 2105.06, accuracy: 79.2%\n",
      "loss: 1829.66, accuracy: 81.0%\n",
      "loss: 2112.26, accuracy: 79.6%\n",
      "loss: 1834.84, accuracy: 80.9%\n",
      "loss: 2157.44, accuracy: 80.1%\n",
      "loss: 2007.77, accuracy: 80.4%\n",
      "loss: 2333.03, accuracy: 78.2%\n",
      "loss: 3204.63, accuracy: 78.5%\n",
      "loss: 1656.71, accuracy: 83.1%\n",
      "loss: 1559.85, accuracy: 86.0%\n",
      "loss: 1533.49, accuracy: 85.2%\n",
      "loss: 1563.57, accuracy: 86.9%\n",
      "loss: 1889.80, accuracy: 82.5%\n",
      "loss: 1808.09, accuracy: 84.1%\n",
      "loss: 2613.14, accuracy: 79.4%\n",
      "loss: 1563.21, accuracy: 85.1%\n",
      "loss: 1495.47, accuracy: 86.2%\n",
      "loss: 1445.68, accuracy: 86.9%\n",
      "loss: 1411.39, accuracy: 87.3%\n",
      "loss: 1391.36, accuracy: 87.9%\n",
      "loss: 1382.77, accuracy: 87.4%\n",
      "loss: 1396.73, accuracy: 88.2%\n",
      "loss: 1521.58, accuracy: 84.3%\n",
      "loss: 1886.94, accuracy: 82.3%\n",
      "loss: 3398.22, accuracy: 70.0%\n",
      "loss: 1901.71, accuracy: 78.7%\n",
      "loss: 2195.11, accuracy: 78.8%\n",
      "loss: 2041.84, accuracy: 77.4%\n",
      "loss: 2818.53, accuracy: 74.2%\n",
      "loss: 1877.53, accuracy: 79.7%\n",
      "loss: 2046.45, accuracy: 81.0%\n",
      "loss: 1670.59, accuracy: 82.5%\n",
      "loss: 1825.95, accuracy: 82.9%\n",
      "loss: 1611.05, accuracy: 83.6%\n",
      "loss: 1798.62, accuracy: 82.7%\n",
      "loss: 1601.10, accuracy: 84.3%\n",
      "loss: 1931.68, accuracy: 80.8%\n",
      "loss: 1965.16, accuracy: 79.3%\n",
      "loss: 3457.31, accuracy: 68.5%\n",
      "loss: 1845.29, accuracy: 79.3%\n",
      "loss: 1877.17, accuracy: 82.0%\n",
      "loss: 1599.54, accuracy: 83.4%\n",
      "loss: 1721.25, accuracy: 84.1%\n",
      "loss: 1578.70, accuracy: 84.2%\n",
      "loss: 1778.56, accuracy: 83.8%\n",
      "loss: 1605.16, accuracy: 83.9%\n",
      "loss: 1745.86, accuracy: 84.5%\n",
      "loss: 1761.89, accuracy: 83.2%\n",
      "loss: 1657.69, accuracy: 85.4%\n",
      "loss: 2231.38, accuracy: 81.5%\n",
      "loss: 1368.14, accuracy: 87.7%\n",
      "loss: 1291.07, accuracy: 88.2%\n",
      "loss: 1260.54, accuracy: 89.8%\n",
      "loss: 1272.04, accuracy: 87.6%\n",
      "loss: 1384.16, accuracy: 89.1%\n",
      "loss: 2069.81, accuracy: 79.7%\n",
      "loss: 1592.62, accuracy: 85.3%\n",
      "loss: 2178.15, accuracy: 77.2%\n",
      "loss: 1800.59, accuracy: 81.4%\n",
      "loss: 2593.60, accuracy: 74.7%\n",
      "loss: 1839.65, accuracy: 81.0%\n",
      "loss: 2579.90, accuracy: 74.1%\n",
      "loss: 1720.96, accuracy: 82.5%\n",
      "loss: 2058.22, accuracy: 79.1%\n",
      "loss: 1639.41, accuracy: 83.8%\n",
      "loss: 2069.66, accuracy: 78.9%\n",
      "loss: 1634.45, accuracy: 84.1%\n",
      "loss: 2052.09, accuracy: 79.0%\n",
      "loss: 1590.84, accuracy: 84.6%\n",
      "loss: 1896.23, accuracy: 80.4%\n",
      "loss: 1552.96, accuracy: 85.1%\n",
      "loss: 1869.46, accuracy: 80.6%\n",
      "loss: 1543.17, accuracy: 85.3%\n",
      "loss: 1868.81, accuracy: 80.5%\n",
      "loss: 1527.66, accuracy: 85.4%\n",
      "loss: 1826.28, accuracy: 80.9%\n",
      "loss: 1507.10, accuracy: 85.9%\n",
      "loss: 1793.02, accuracy: 81.2%\n",
      "loss: 1494.24, accuracy: 85.9%\n",
      "loss: 1784.13, accuracy: 81.3%\n",
      "loss: 1486.29, accuracy: 86.0%\n",
      "loss: 1777.87, accuracy: 81.3%\n",
      "loss: 1478.01, accuracy: 86.0%\n",
      "loss: 1768.48, accuracy: 81.5%\n",
      "loss: 1470.69, accuracy: 86.1%\n",
      "loss: 1764.82, accuracy: 81.6%\n",
      "loss: 1466.38, accuracy: 86.2%\n",
      "loss: 1769.07, accuracy: 81.6%\n",
      "loss: 1464.22, accuracy: 86.1%\n",
      "loss: 1776.61, accuracy: 81.7%\n",
      "loss: 1462.22, accuracy: 86.0%\n",
      "loss: 1782.83, accuracy: 81.8%\n",
      "loss: 1458.67, accuracy: 86.0%\n",
      "loss: 1784.01, accuracy: 81.9%\n",
      "loss: 1452.06, accuracy: 86.1%\n",
      "loss: 1776.34, accuracy: 82.0%\n",
      "loss: 1441.00, accuracy: 86.3%\n",
      "loss: 1756.87, accuracy: 82.0%\n",
      "loss: 1424.53, accuracy: 86.7%\n",
      "loss: 1724.43, accuracy: 82.1%\n",
      "loss: 1399.84, accuracy: 87.1%\n",
      "loss: 1675.44, accuracy: 82.5%\n",
      "loss: 1359.13, accuracy: 88.1%\n",
      "loss: 1619.33, accuracy: 83.1%\n",
      "loss: 1337.21, accuracy: 88.6%\n",
      "loss: 1771.18, accuracy: 83.4%\n",
      "loss: 1468.96, accuracy: 88.0%\n",
      "loss: 2159.23, accuracy: 82.4%\n",
      "loss: 1806.13, accuracy: 84.4%\n",
      "loss: 1998.02, accuracy: 80.9%\n",
      "loss: 2464.77, accuracy: 79.0%\n",
      "loss: 2061.98, accuracy: 81.0%\n",
      "loss: 1745.72, accuracy: 84.5%\n",
      "loss: 1298.60, accuracy: 86.5%\n",
      "loss: 1259.34, accuracy: 89.1%\n",
      "loss: 1242.70, accuracy: 88.1%\n",
      "loss: 1290.04, accuracy: 89.0%\n",
      "loss: 1817.92, accuracy: 84.2%\n",
      "loss: 1163.96, accuracy: 90.6%\n",
      "loss: 1223.84, accuracy: 87.8%\n",
      "loss: 1174.34, accuracy: 90.4%\n",
      "loss: 1568.42, accuracy: 84.4%\n",
      "loss: 1169.32, accuracy: 90.6%\n",
      "loss: 1381.88, accuracy: 85.4%\n",
      "loss: 1257.73, accuracy: 89.5%\n",
      "loss: 1717.16, accuracy: 82.1%\n",
      "loss: 1367.76, accuracy: 87.3%\n",
      "loss: 1853.39, accuracy: 80.7%\n",
      "loss: 1608.32, accuracy: 83.7%\n",
      "loss: 2633.55, accuracy: 74.5%\n",
      "loss: 1563.68, accuracy: 84.1%\n",
      "loss: 1956.22, accuracy: 80.6%\n",
      "loss: 1435.33, accuracy: 85.6%\n",
      "loss: 1881.08, accuracy: 81.1%\n",
      "loss: 1399.90, accuracy: 86.5%\n",
      "loss: 1818.77, accuracy: 81.4%\n",
      "loss: 1318.54, accuracy: 87.6%\n",
      "loss: 1553.33, accuracy: 83.9%\n",
      "loss: 1250.16, accuracy: 88.9%\n",
      "loss: 1469.42, accuracy: 84.5%\n",
      "loss: 1217.84, accuracy: 89.5%\n",
      "loss: 1432.36, accuracy: 84.7%\n",
      "loss: 1183.21, accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "mlp.train(x=x_train, y_true=y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.param_values['w1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = mlp.memory['a2']\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = 0\n",
    "for i in range(len(a2)):\n",
    "    equal = np.equal(np.argmax(y_train[i]), np.argmax(a2[i]))\n",
    "    correct_pred += equal.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(correct_pred/len(y_train)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.equal(np.argmax(y_train), np.argmax(a2)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp.accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#ax.plot(mlp.accuracies)\n",
    "\n",
    "#ax.set(xlabel='epochs', ylabel='accuracy [%]')\n",
    "#ax.grid()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pszt import refactor_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3804\n"
     ]
    }
   ],
   "source": [
    "# File path to the csv file.\n",
    "#x_train, y_train = refactor_csv.vectorize_dataset('apple-twitter.csv',\n",
    "#                                     'x_train_3k',\n",
    "#                                     'y_train_3k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
