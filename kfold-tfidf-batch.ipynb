{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pszt import net\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_tfidf.npy')\n",
    "y_train = np.load('y_tfidf.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ pierwsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: relu\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 720.02, accuracy: 56.4%\n",
      "Epoch: 20, loss: 719.94, accuracy: 56.4%\n",
      "Epoch: 30, loss: 719.21, accuracy: 56.4%\n",
      "Epoch: 40, loss: 713.89, accuracy: 56.4%\n",
      "Epoch: 50, loss: 683.18, accuracy: 56.9%\n",
      "Epoch: 60, loss: 719.74, accuracy: 56.4%\n",
      "Epoch: 70, loss: 720.05, accuracy: 56.4%\n",
      "Epoch: 80, loss: 720.01, accuracy: 56.4%\n",
      "Epoch: 90, loss: 719.78, accuracy: 56.4%\n",
      "Epoch: 100, loss: 718.14, accuracy: 56.4%\n",
      "Epoch: 110, loss: 707.56, accuracy: 56.4%\n",
      "Epoch: 120, loss: 733.32, accuracy: 56.4%\n",
      "Epoch: 130, loss: 720.04, accuracy: 56.4%\n",
      "Epoch: 140, loss: 720.04, accuracy: 56.4%\n",
      "Epoch: 150, loss: 719.99, accuracy: 56.4%\n",
      "Epoch: 160, loss: 719.61, accuracy: 56.4%\n",
      "Epoch: 170, loss: 716.94, accuracy: 56.4%\n",
      "Epoch: 180, loss: 699.81, accuracy: 56.4%\n",
      "Epoch: 190, loss: 719.80, accuracy: 56.4%\n",
      "Epoch: 200, loss: 720.05, accuracy: 56.4%\n",
      "Epoch: 210, loss: 720.05, accuracy: 56.4%\n",
      "Epoch: 220, loss: 720.05, accuracy: 56.4%\n",
      "Epoch: 230, loss: 719.28, accuracy: 56.4%\n",
      "Epoch: 240, loss: 714.59, accuracy: 56.4%\n",
      "Epoch: 250, loss: 747.04, accuracy: 56.4%\n",
      "Epoch: 260, loss: 787.10, accuracy: 56.4%\n",
      "Epoch: 270, loss: 812.72, accuracy: 56.4%\n",
      "Epoch: 280, loss: 669.02, accuracy: 56.4%\n",
      "Epoch: 290, loss: 719.88, accuracy: 56.4%\n",
      "Epoch: 300, loss: 720.05, accuracy: 56.4%\n",
      "Mean accuracy on T0: 55.85\n",
      "Mean losses on T0: 725.34\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 709.89, accuracy: 56.0%\n",
      "Epoch: 20, loss: 709.82, accuracy: 56.0%\n",
      "Epoch: 30, loss: 709.32, accuracy: 56.0%\n",
      "Epoch: 40, loss: 705.61, accuracy: 56.0%\n",
      "Epoch: 50, loss: 685.18, accuracy: 59.7%\n",
      "Epoch: 60, loss: 709.92, accuracy: 56.0%\n",
      "Epoch: 70, loss: 709.88, accuracy: 56.0%\n",
      "Epoch: 80, loss: 709.80, accuracy: 56.0%\n",
      "Epoch: 90, loss: 709.24, accuracy: 56.0%\n",
      "Epoch: 100, loss: 705.33, accuracy: 56.0%\n",
      "Epoch: 110, loss: 683.49, accuracy: 56.0%\n",
      "Epoch: 120, loss: 710.00, accuracy: 56.0%\n",
      "Epoch: 130, loss: 709.87, accuracy: 56.0%\n",
      "Epoch: 140, loss: 709.83, accuracy: 56.0%\n",
      "Epoch: 150, loss: 709.75, accuracy: 56.0%\n",
      "Epoch: 160, loss: 709.50, accuracy: 56.0%\n",
      "Epoch: 170, loss: 708.56, accuracy: 56.0%\n",
      "Epoch: 180, loss: 703.60, accuracy: 56.0%\n",
      "Epoch: 190, loss: 857.82, accuracy: 56.0%\n",
      "Epoch: 200, loss: 709.68, accuracy: 56.0%\n",
      "Epoch: 210, loss: 708.57, accuracy: 56.0%\n",
      "Epoch: 220, loss: 701.04, accuracy: 56.0%\n",
      "Epoch: 230, loss: 672.52, accuracy: 56.9%\n",
      "Epoch: 240, loss: 709.92, accuracy: 56.0%\n",
      "Epoch: 250, loss: 709.90, accuracy: 56.0%\n",
      "Epoch: 260, loss: 709.90, accuracy: 56.0%\n",
      "Epoch: 270, loss: 709.90, accuracy: 56.0%\n",
      "Epoch: 280, loss: 709.90, accuracy: 56.0%\n",
      "Epoch: 290, loss: 709.90, accuracy: 56.0%\n",
      "Epoch: 300, loss: 709.90, accuracy: 56.0%\n",
      "Mean accuracy on T1: 55.79\n",
      "Mean losses on T1: 714.36\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 710.53, accuracy: 57.3%\n",
      "Epoch: 20, loss: 710.46, accuracy: 57.3%\n",
      "Epoch: 30, loss: 709.85, accuracy: 57.3%\n",
      "Epoch: 40, loss: 705.40, accuracy: 57.3%\n",
      "Epoch: 50, loss: 679.29, accuracy: 57.3%\n",
      "Epoch: 60, loss: 710.44, accuracy: 57.3%\n",
      "Epoch: 70, loss: 710.24, accuracy: 57.3%\n",
      "Epoch: 80, loss: 708.21, accuracy: 57.3%\n",
      "Epoch: 90, loss: 695.02, accuracy: 57.3%\n",
      "Epoch: 100, loss: 921.72, accuracy: 31.0%\n",
      "Epoch: 110, loss: 708.06, accuracy: 57.3%\n",
      "Epoch: 120, loss: 693.28, accuracy: 57.3%\n",
      "Epoch: 130, loss: 708.25, accuracy: 57.3%\n",
      "Epoch: 140, loss: 689.45, accuracy: 71.4%\n",
      "Epoch: 150, loss: 705.19, accuracy: 57.4%\n",
      "Epoch: 160, loss: 1036.12, accuracy: 31.0%\n",
      "Epoch: 170, loss: 702.48, accuracy: 57.3%\n",
      "Epoch: 180, loss: 707.07, accuracy: 57.3%\n",
      "Epoch: 190, loss: 748.53, accuracy: 57.3%\n",
      "Epoch: 200, loss: 696.10, accuracy: 57.3%\n",
      "Epoch: 210, loss: 714.31, accuracy: 57.3%\n",
      "Epoch: 220, loss: 716.35, accuracy: 57.3%\n",
      "Epoch: 230, loss: 702.97, accuracy: 57.3%\n",
      "Epoch: 240, loss: 686.53, accuracy: 57.3%\n",
      "Epoch: 250, loss: 715.70, accuracy: 57.3%\n",
      "Epoch: 260, loss: 695.34, accuracy: 57.3%\n",
      "Epoch: 270, loss: 701.56, accuracy: 57.3%\n",
      "Epoch: 280, loss: 862.73, accuracy: 57.8%\n",
      "Epoch: 290, loss: 661.84, accuracy: 57.8%\n",
      "Epoch: 300, loss: 709.94, accuracy: 57.3%\n",
      "Mean accuracy on T2: 55.72\n",
      "Mean losses on T2: 722.38\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 699.25, accuracy: 57.7%\n",
      "Epoch: 20, loss: 699.16, accuracy: 57.7%\n",
      "Epoch: 30, loss: 698.51, accuracy: 57.7%\n",
      "Epoch: 40, loss: 694.00, accuracy: 57.7%\n",
      "Epoch: 50, loss: 675.88, accuracy: 57.7%\n",
      "Epoch: 60, loss: 691.95, accuracy: 57.7%\n",
      "Epoch: 70, loss: 786.47, accuracy: 31.8%\n",
      "Epoch: 80, loss: 699.11, accuracy: 57.7%\n",
      "Epoch: 90, loss: 697.20, accuracy: 57.7%\n",
      "Epoch: 100, loss: 684.93, accuracy: 57.7%\n",
      "Epoch: 110, loss: 700.32, accuracy: 57.7%\n",
      "Epoch: 120, loss: 698.67, accuracy: 57.7%\n",
      "Epoch: 130, loss: 692.99, accuracy: 57.7%\n",
      "Epoch: 140, loss: 659.81, accuracy: 58.3%\n",
      "Epoch: 150, loss: 699.65, accuracy: 57.7%\n",
      "Epoch: 160, loss: 674.05, accuracy: 57.7%\n",
      "Epoch: 170, loss: 699.22, accuracy: 57.7%\n",
      "Epoch: 180, loss: 698.37, accuracy: 57.7%\n",
      "Epoch: 190, loss: 693.32, accuracy: 57.7%\n",
      "Epoch: 200, loss: 940.75, accuracy: 31.8%\n",
      "Epoch: 210, loss: 697.88, accuracy: 57.7%\n",
      "Epoch: 220, loss: 977.22, accuracy: 31.8%\n",
      "Epoch: 230, loss: 699.09, accuracy: 57.7%\n",
      "Epoch: 240, loss: 697.95, accuracy: 57.7%\n",
      "Epoch: 250, loss: 691.82, accuracy: 57.7%\n",
      "Epoch: 260, loss: 1181.13, accuracy: 31.8%\n",
      "Epoch: 270, loss: 654.48, accuracy: 57.7%\n",
      "Epoch: 280, loss: 697.08, accuracy: 57.7%\n",
      "Epoch: 290, loss: 700.43, accuracy: 57.7%\n",
      "Epoch: 300, loss: 696.82, accuracy: 57.7%\n",
      "Mean accuracy on T3: 56.65\n",
      "Mean losses on T3: 708.33\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 20, loss: 698.97, accuracy: 56.8%\n",
      "Epoch: 30, loss: 698.20, accuracy: 56.8%\n",
      "Epoch: 40, loss: 692.94, accuracy: 56.8%\n",
      "Epoch: 50, loss: 664.10, accuracy: 57.5%\n",
      "Epoch: 60, loss: 699.03, accuracy: 56.8%\n",
      "Epoch: 70, loss: 698.27, accuracy: 56.8%\n",
      "Epoch: 80, loss: 693.46, accuracy: 56.8%\n",
      "Epoch: 90, loss: 675.93, accuracy: 57.6%\n",
      "Epoch: 100, loss: 699.11, accuracy: 56.8%\n",
      "Epoch: 110, loss: 698.62, accuracy: 56.8%\n",
      "Epoch: 120, loss: 695.80, accuracy: 56.8%\n",
      "Epoch: 130, loss: 677.90, accuracy: 57.4%\n",
      "Epoch: 140, loss: 695.66, accuracy: 56.8%\n",
      "Epoch: 150, loss: 672.15, accuracy: 56.8%\n",
      "Epoch: 160, loss: 698.95, accuracy: 56.8%\n",
      "Epoch: 170, loss: 698.18, accuracy: 56.8%\n",
      "Epoch: 180, loss: 692.72, accuracy: 56.8%\n",
      "Epoch: 190, loss: 663.93, accuracy: 56.8%\n",
      "Epoch: 200, loss: 699.21, accuracy: 56.8%\n",
      "Epoch: 210, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 220, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 230, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 240, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 250, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 260, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 270, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 280, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 290, loss: 699.09, accuracy: 56.8%\n",
      "Epoch: 300, loss: 699.09, accuracy: 56.8%\n",
      "Mean accuracy on T4: 56.43\n",
      "Mean losses on T4: 700.16\n",
      "\n",
      "Accuracy list on Ti sets: [55.85194918966274, 55.79281646955761, 55.717038983793245, 56.654840122645666, 56.429824561403514]\n",
      "Losses list on Ti sets: [725.3383243244011, 714.3622427122011, 722.3838164984637, 708.3313089516037, 700.1612100266481]\n",
      "Mean accuracy on all T sets: 56.09\n",
      "Mean losses on all T sets: 714.12\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ druga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: sigmoid\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 881.65, accuracy: 28.9%\n",
      "Epoch: 20, loss: 832.84, accuracy: 28.9%\n",
      "Epoch: 30, loss: 777.53, accuracy: 28.9%\n",
      "Epoch: 40, loss: 751.67, accuracy: 58.1%\n",
      "Epoch: 50, loss: 738.02, accuracy: 58.1%\n",
      "Epoch: 60, loss: 730.31, accuracy: 58.1%\n",
      "Epoch: 70, loss: 725.70, accuracy: 58.1%\n",
      "Epoch: 80, loss: 722.77, accuracy: 58.1%\n",
      "Epoch: 90, loss: 720.75, accuracy: 58.1%\n",
      "Epoch: 100, loss: 719.23, accuracy: 58.1%\n",
      "Epoch: 110, loss: 718.00, accuracy: 58.1%\n",
      "Epoch: 120, loss: 716.95, accuracy: 58.1%\n",
      "Epoch: 130, loss: 716.18, accuracy: 58.1%\n",
      "Epoch: 140, loss: 716.07, accuracy: 58.1%\n",
      "Epoch: 150, loss: 717.93, accuracy: 58.1%\n",
      "Epoch: 160, loss: 724.60, accuracy: 58.3%\n",
      "Epoch: 170, loss: 735.49, accuracy: 70.6%\n",
      "Epoch: 180, loss: 739.67, accuracy: 61.0%\n",
      "Epoch: 190, loss: 737.28, accuracy: 60.4%\n",
      "Epoch: 200, loss: 734.86, accuracy: 59.9%\n",
      "Epoch: 210, loss: 734.33, accuracy: 54.3%\n",
      "Epoch: 220, loss: 735.08, accuracy: 50.2%\n",
      "Epoch: 230, loss: 735.94, accuracy: 46.1%\n",
      "Epoch: 240, loss: 736.34, accuracy: 43.6%\n",
      "Epoch: 250, loss: 736.23, accuracy: 43.8%\n",
      "Epoch: 260, loss: 735.68, accuracy: 46.4%\n",
      "Epoch: 270, loss: 734.69, accuracy: 46.6%\n",
      "Epoch: 280, loss: 733.27, accuracy: 47.8%\n",
      "Epoch: 290, loss: 731.45, accuracy: 48.8%\n",
      "Epoch: 300, loss: 729.25, accuracy: 49.0%\n",
      "Mean accuracy on T0: 54.91\n",
      "Mean losses on T0: 763.72\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 729.20, accuracy: 33.6%\n",
      "Epoch: 20, loss: 786.82, accuracy: 33.6%\n",
      "Epoch: 30, loss: 737.05, accuracy: 33.6%\n",
      "Epoch: 40, loss: 715.97, accuracy: 56.6%\n",
      "Epoch: 50, loss: 705.98, accuracy: 56.6%\n",
      "Epoch: 60, loss: 700.93, accuracy: 56.6%\n",
      "Epoch: 70, loss: 698.21, accuracy: 56.6%\n",
      "Epoch: 80, loss: 696.54, accuracy: 56.6%\n",
      "Epoch: 90, loss: 695.33, accuracy: 56.6%\n",
      "Epoch: 100, loss: 694.25, accuracy: 56.6%\n",
      "Epoch: 110, loss: 693.14, accuracy: 56.6%\n",
      "Epoch: 120, loss: 691.96, accuracy: 56.6%\n",
      "Epoch: 130, loss: 690.81, accuracy: 56.6%\n",
      "Epoch: 140, loss: 690.27, accuracy: 56.6%\n",
      "Epoch: 150, loss: 692.25, accuracy: 56.6%\n",
      "Epoch: 160, loss: 698.98, accuracy: 61.4%\n",
      "Epoch: 170, loss: 704.20, accuracy: 71.7%\n",
      "Epoch: 180, loss: 702.75, accuracy: 69.0%\n",
      "Epoch: 190, loss: 699.76, accuracy: 68.3%\n",
      "Epoch: 200, loss: 698.14, accuracy: 64.8%\n",
      "Epoch: 210, loss: 697.93, accuracy: 61.0%\n",
      "Epoch: 220, loss: 698.22, accuracy: 56.1%\n",
      "Epoch: 230, loss: 698.29, accuracy: 53.9%\n",
      "Epoch: 240, loss: 697.99, accuracy: 52.2%\n",
      "Epoch: 250, loss: 697.40, accuracy: 51.4%\n",
      "Epoch: 260, loss: 696.52, accuracy: 51.4%\n",
      "Epoch: 270, loss: 695.35, accuracy: 51.8%\n",
      "Epoch: 280, loss: 693.90, accuracy: 52.3%\n",
      "Epoch: 290, loss: 692.17, accuracy: 53.1%\n",
      "Epoch: 300, loss: 690.19, accuracy: 53.6%\n",
      "Mean accuracy on T1: 55.80\n",
      "Mean losses on T1: 742.96\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 735.52, accuracy: 33.6%\n",
      "Epoch: 20, loss: 787.91, accuracy: 33.6%\n",
      "Epoch: 30, loss: 738.98, accuracy: 33.6%\n",
      "Epoch: 40, loss: 718.31, accuracy: 56.4%\n",
      "Epoch: 50, loss: 708.55, accuracy: 56.4%\n",
      "Epoch: 60, loss: 703.67, accuracy: 56.4%\n",
      "Epoch: 70, loss: 701.04, accuracy: 56.4%\n",
      "Epoch: 80, loss: 699.43, accuracy: 56.4%\n",
      "Epoch: 90, loss: 698.25, accuracy: 56.4%\n",
      "Epoch: 100, loss: 697.18, accuracy: 56.4%\n",
      "Epoch: 110, loss: 696.04, accuracy: 56.4%\n",
      "Epoch: 120, loss: 694.79, accuracy: 56.4%\n",
      "Epoch: 130, loss: 693.50, accuracy: 56.4%\n",
      "Epoch: 140, loss: 692.65, accuracy: 56.4%\n",
      "Epoch: 150, loss: 693.94, accuracy: 56.4%\n",
      "Epoch: 160, loss: 699.70, accuracy: 60.2%\n",
      "Epoch: 170, loss: 704.61, accuracy: 71.6%\n",
      "Epoch: 180, loss: 702.98, accuracy: 72.8%\n",
      "Epoch: 190, loss: 699.44, accuracy: 72.0%\n",
      "Epoch: 200, loss: 696.98, accuracy: 70.8%\n",
      "Epoch: 210, loss: 695.77, accuracy: 67.5%\n",
      "Epoch: 220, loss: 694.97, accuracy: 63.7%\n",
      "Epoch: 230, loss: 693.86, accuracy: 56.4%\n",
      "Epoch: 240, loss: 692.28, accuracy: 54.1%\n",
      "Epoch: 250, loss: 690.27, accuracy: 53.2%\n",
      "Epoch: 260, loss: 687.88, accuracy: 53.1%\n",
      "Epoch: 270, loss: 685.13, accuracy: 53.1%\n",
      "Epoch: 280, loss: 682.03, accuracy: 53.9%\n",
      "Epoch: 290, loss: 678.60, accuracy: 58.5%\n",
      "Epoch: 300, loss: 674.89, accuracy: 59.4%\n",
      "Mean accuracy on T2: 56.41\n",
      "Mean losses on T2: 744.59\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 771.18, accuracy: 31.1%\n",
      "Epoch: 20, loss: 802.73, accuracy: 31.1%\n",
      "Epoch: 30, loss: 748.66, accuracy: 31.1%\n",
      "Epoch: 40, loss: 724.22, accuracy: 58.3%\n",
      "Epoch: 50, loss: 711.74, accuracy: 58.3%\n",
      "Epoch: 60, loss: 704.91, accuracy: 58.3%\n",
      "Epoch: 70, loss: 700.92, accuracy: 58.3%\n",
      "Epoch: 80, loss: 698.41, accuracy: 58.3%\n",
      "Epoch: 90, loss: 696.65, accuracy: 58.3%\n",
      "Epoch: 100, loss: 695.27, accuracy: 58.3%\n",
      "Epoch: 110, loss: 694.07, accuracy: 58.3%\n",
      "Epoch: 120, loss: 692.97, accuracy: 58.3%\n",
      "Epoch: 130, loss: 692.04, accuracy: 58.3%\n",
      "Epoch: 140, loss: 691.70, accuracy: 58.3%\n",
      "Epoch: 150, loss: 693.29, accuracy: 58.3%\n",
      "Epoch: 160, loss: 699.84, accuracy: 58.9%\n",
      "Epoch: 170, loss: 710.23, accuracy: 73.2%\n",
      "Epoch: 180, loss: 713.36, accuracy: 66.2%\n",
      "Epoch: 190, loss: 710.43, accuracy: 66.0%\n",
      "Epoch: 200, loss: 707.80, accuracy: 63.3%\n",
      "Epoch: 210, loss: 707.19, accuracy: 59.0%\n",
      "Epoch: 220, loss: 707.89, accuracy: 53.5%\n",
      "Epoch: 230, loss: 708.75, accuracy: 50.5%\n",
      "Epoch: 240, loss: 709.18, accuracy: 49.1%\n",
      "Epoch: 250, loss: 709.17, accuracy: 48.5%\n",
      "Epoch: 260, loss: 708.77, accuracy: 48.5%\n",
      "Epoch: 270, loss: 708.01, accuracy: 48.8%\n",
      "Epoch: 280, loss: 706.86, accuracy: 49.0%\n",
      "Epoch: 290, loss: 705.35, accuracy: 49.8%\n",
      "Epoch: 300, loss: 703.48, accuracy: 50.6%\n",
      "Mean accuracy on T3: 56.09\n",
      "Mean losses on T3: 739.19\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 910.16, accuracy: 32.9%\n",
      "Epoch: 20, loss: 800.74, accuracy: 32.9%\n",
      "Epoch: 30, loss: 758.15, accuracy: 32.9%\n",
      "Epoch: 40, loss: 740.01, accuracy: 54.7%\n",
      "Epoch: 50, loss: 731.54, accuracy: 54.7%\n",
      "Epoch: 60, loss: 727.39, accuracy: 54.7%\n",
      "Epoch: 70, loss: 725.24, accuracy: 54.7%\n",
      "Epoch: 80, loss: 723.97, accuracy: 54.7%\n",
      "Epoch: 90, loss: 723.03, accuracy: 54.7%\n",
      "Epoch: 100, loss: 722.14, accuracy: 54.7%\n",
      "Epoch: 110, loss: 721.13, accuracy: 54.7%\n",
      "Epoch: 120, loss: 719.94, accuracy: 54.7%\n",
      "Epoch: 130, loss: 718.59, accuracy: 54.7%\n",
      "Epoch: 140, loss: 717.41, accuracy: 54.7%\n",
      "Epoch: 150, loss: 717.77, accuracy: 54.7%\n",
      "Epoch: 160, loss: 722.12, accuracy: 57.2%\n",
      "Epoch: 170, loss: 726.77, accuracy: 69.3%\n",
      "Epoch: 180, loss: 725.68, accuracy: 68.4%\n",
      "Epoch: 190, loss: 722.58, accuracy: 67.0%\n",
      "Epoch: 200, loss: 720.38, accuracy: 64.6%\n",
      "Epoch: 210, loss: 719.35, accuracy: 59.7%\n",
      "Epoch: 220, loss: 718.74, accuracy: 55.8%\n",
      "Epoch: 230, loss: 717.91, accuracy: 53.3%\n",
      "Epoch: 240, loss: 716.72, accuracy: 52.1%\n",
      "Epoch: 250, loss: 715.20, accuracy: 52.0%\n",
      "Epoch: 260, loss: 713.37, accuracy: 52.0%\n",
      "Epoch: 270, loss: 711.24, accuracy: 52.1%\n",
      "Epoch: 280, loss: 708.82, accuracy: 52.1%\n",
      "Epoch: 290, loss: 706.12, accuracy: 52.6%\n",
      "Epoch: 300, loss: 703.19, accuracy: 53.7%\n",
      "Mean accuracy on T4: 54.10\n",
      "Mean losses on T4: 773.44\n",
      "\n",
      "Accuracy list on Ti sets: [54.90538764783179, 55.79675865089795, 56.41349102058695, 56.09461235216821, 54.100877192982466]\n",
      "Losses list on Ti sets: [763.7224779130598, 742.9649963463462, 744.5938556917863, 739.1935480873309, 773.4396998153521]\n",
      "Mean accuracy on all T sets: 55.46\n",
      "Mean losses on all T sets: 752.78\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ trzecia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: relu\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 703.97, accuracy: 58.3%\n",
      "Epoch: 20, loss: 703.70, accuracy: 58.3%\n",
      "Epoch: 30, loss: 701.61, accuracy: 58.3%\n",
      "Epoch: 40, loss: 687.43, accuracy: 58.3%\n",
      "Epoch: 50, loss: 801.25, accuracy: 58.3%\n",
      "Epoch: 60, loss: 703.99, accuracy: 58.3%\n",
      "Epoch: 70, loss: 703.82, accuracy: 58.3%\n",
      "Epoch: 80, loss: 702.56, accuracy: 58.3%\n",
      "Epoch: 90, loss: 693.92, accuracy: 58.3%\n",
      "Epoch: 100, loss: 672.62, accuracy: 58.3%\n",
      "Epoch: 110, loss: 702.98, accuracy: 58.3%\n",
      "Epoch: 120, loss: 695.82, accuracy: 58.3%\n",
      "Epoch: 130, loss: 703.25, accuracy: 58.3%\n",
      "Epoch: 140, loss: 702.90, accuracy: 58.3%\n",
      "Epoch: 150, loss: 678.97, accuracy: 60.7%\n",
      "Epoch: 160, loss: 693.20, accuracy: 59.1%\n",
      "Epoch: 170, loss: 695.44, accuracy: 59.3%\n",
      "Epoch: 180, loss: 873.78, accuracy: 31.5%\n",
      "Epoch: 190, loss: 703.98, accuracy: 58.3%\n",
      "Epoch: 200, loss: 703.49, accuracy: 58.3%\n",
      "Epoch: 210, loss: 699.55, accuracy: 58.3%\n",
      "Epoch: 220, loss: 808.65, accuracy: 58.3%\n",
      "Epoch: 230, loss: 699.41, accuracy: 58.5%\n",
      "Epoch: 240, loss: 698.39, accuracy: 58.7%\n",
      "Epoch: 250, loss: 688.37, accuracy: 61.1%\n",
      "Epoch: 260, loss: 705.24, accuracy: 58.3%\n",
      "Epoch: 270, loss: 700.68, accuracy: 58.6%\n",
      "Epoch: 280, loss: 670.77, accuracy: 59.3%\n",
      "Epoch: 290, loss: 694.36, accuracy: 58.7%\n",
      "Epoch: 300, loss: 698.91, accuracy: 59.5%\n",
      "Mean accuracy on T0: 57.74\n",
      "Mean losses on T0: 713.49\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 698.04, accuracy: 58.7%\n",
      "Epoch: 20, loss: 697.81, accuracy: 58.7%\n",
      "Epoch: 30, loss: 696.17, accuracy: 58.7%\n",
      "Epoch: 40, loss: 685.14, accuracy: 58.7%\n",
      "Epoch: 50, loss: 778.22, accuracy: 58.7%\n",
      "Epoch: 60, loss: 697.16, accuracy: 58.7%\n",
      "Epoch: 70, loss: 691.42, accuracy: 58.7%\n",
      "Epoch: 80, loss: 690.16, accuracy: 58.7%\n",
      "Epoch: 90, loss: 697.85, accuracy: 58.7%\n",
      "Epoch: 100, loss: 695.79, accuracy: 58.7%\n",
      "Epoch: 110, loss: 683.67, accuracy: 58.7%\n",
      "Epoch: 120, loss: 700.21, accuracy: 58.7%\n",
      "Epoch: 130, loss: 697.26, accuracy: 58.7%\n",
      "Epoch: 140, loss: 692.47, accuracy: 58.7%\n",
      "Epoch: 150, loss: 666.81, accuracy: 58.7%\n",
      "Epoch: 160, loss: 696.64, accuracy: 58.7%\n",
      "Epoch: 170, loss: 689.77, accuracy: 58.7%\n",
      "Epoch: 180, loss: 674.61, accuracy: 59.0%\n",
      "Epoch: 190, loss: 697.79, accuracy: 58.7%\n",
      "Epoch: 200, loss: 695.41, accuracy: 58.7%\n",
      "Epoch: 210, loss: 685.68, accuracy: 58.7%\n",
      "Epoch: 220, loss: 698.05, accuracy: 58.7%\n",
      "Epoch: 230, loss: 693.83, accuracy: 58.7%\n",
      "Epoch: 240, loss: 673.11, accuracy: 58.7%\n",
      "Epoch: 250, loss: 698.11, accuracy: 58.7%\n",
      "Epoch: 260, loss: 697.49, accuracy: 58.7%\n",
      "Epoch: 270, loss: 693.77, accuracy: 58.7%\n",
      "Epoch: 280, loss: 671.92, accuracy: 58.7%\n",
      "Epoch: 290, loss: 698.08, accuracy: 58.7%\n",
      "Epoch: 300, loss: 698.03, accuracy: 58.7%\n",
      "Mean accuracy on T1: 58.08\n",
      "Mean losses on T1: 702.01\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 702.35, accuracy: 58.0%\n",
      "Epoch: 20, loss: 702.21, accuracy: 58.0%\n",
      "Epoch: 30, loss: 701.14, accuracy: 58.0%\n",
      "Epoch: 40, loss: 693.37, accuracy: 58.0%\n",
      "Epoch: 50, loss: 896.96, accuracy: 31.0%\n",
      "Epoch: 60, loss: 703.18, accuracy: 67.3%\n",
      "Epoch: 70, loss: 729.77, accuracy: 43.1%\n",
      "Epoch: 80, loss: 663.45, accuracy: 58.0%\n",
      "Epoch: 90, loss: 702.65, accuracy: 58.0%\n",
      "Epoch: 100, loss: 701.91, accuracy: 58.0%\n",
      "Epoch: 110, loss: 700.21, accuracy: 58.0%\n",
      "Epoch: 120, loss: 690.54, accuracy: 58.0%\n",
      "Epoch: 130, loss: 702.09, accuracy: 58.0%\n",
      "Epoch: 140, loss: 758.98, accuracy: 58.0%\n",
      "Epoch: 150, loss: 704.93, accuracy: 58.0%\n",
      "Epoch: 160, loss: 676.67, accuracy: 58.0%\n",
      "Epoch: 170, loss: 627.37, accuracy: 63.9%\n",
      "Epoch: 180, loss: 715.90, accuracy: 58.0%\n",
      "Epoch: 190, loss: 702.33, accuracy: 58.0%\n",
      "Epoch: 200, loss: 702.13, accuracy: 58.0%\n",
      "Epoch: 210, loss: 700.62, accuracy: 58.0%\n",
      "Epoch: 220, loss: 690.11, accuracy: 58.0%\n",
      "Epoch: 230, loss: 819.88, accuracy: 58.0%\n",
      "Epoch: 240, loss: 679.00, accuracy: 58.3%\n",
      "Epoch: 250, loss: 881.31, accuracy: 31.0%\n",
      "Epoch: 260, loss: 701.27, accuracy: 58.0%\n",
      "Epoch: 270, loss: 697.07, accuracy: 58.3%\n",
      "Epoch: 280, loss: 671.64, accuracy: 67.0%\n",
      "Epoch: 290, loss: 698.62, accuracy: 58.3%\n",
      "Epoch: 300, loss: 698.84, accuracy: 58.3%\n",
      "Mean accuracy on T2: 56.68\n",
      "Mean losses on T2: 711.78\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 727.08, accuracy: 53.9%\n",
      "Epoch: 20, loss: 726.88, accuracy: 53.9%\n",
      "Epoch: 30, loss: 725.44, accuracy: 53.9%\n",
      "Epoch: 40, loss: 715.79, accuracy: 53.9%\n",
      "Epoch: 50, loss: 1121.08, accuracy: 53.9%\n",
      "Epoch: 60, loss: 727.11, accuracy: 53.9%\n",
      "Epoch: 70, loss: 727.09, accuracy: 53.9%\n",
      "Epoch: 80, loss: 726.89, accuracy: 53.9%\n",
      "Epoch: 90, loss: 725.41, accuracy: 53.9%\n",
      "Epoch: 100, loss: 715.47, accuracy: 53.9%\n",
      "Epoch: 110, loss: 1248.86, accuracy: 53.9%\n",
      "Epoch: 120, loss: 762.93, accuracy: 53.9%\n",
      "Epoch: 130, loss: 767.42, accuracy: 53.9%\n",
      "Epoch: 140, loss: 670.24, accuracy: 63.3%\n",
      "Epoch: 150, loss: 726.86, accuracy: 53.9%\n",
      "Epoch: 160, loss: 773.01, accuracy: 53.9%\n",
      "Epoch: 170, loss: 726.05, accuracy: 53.9%\n",
      "Epoch: 180, loss: 659.46, accuracy: 71.7%\n",
      "Epoch: 190, loss: 726.83, accuracy: 53.9%\n",
      "Epoch: 200, loss: 719.60, accuracy: 54.1%\n",
      "Epoch: 210, loss: 723.82, accuracy: 43.6%\n",
      "Epoch: 220, loss: 688.51, accuracy: 59.7%\n",
      "Epoch: 230, loss: 922.92, accuracy: 53.9%\n",
      "Epoch: 240, loss: 789.94, accuracy: 53.9%\n",
      "Epoch: 250, loss: 725.30, accuracy: 53.9%\n",
      "Epoch: 260, loss: 670.96, accuracy: 60.6%\n",
      "Epoch: 270, loss: 711.04, accuracy: 56.6%\n",
      "Epoch: 280, loss: 750.16, accuracy: 54.7%\n",
      "Epoch: 290, loss: 599.46, accuracy: 73.1%\n",
      "Epoch: 300, loss: 727.20, accuracy: 53.9%\n",
      "Mean accuracy on T3: 54.78\n",
      "Mean losses on T3: 733.59\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 708.39, accuracy: 55.3%\n",
      "Epoch: 20, loss: 708.25, accuracy: 55.3%\n",
      "Epoch: 30, loss: 707.40, accuracy: 55.3%\n",
      "Epoch: 40, loss: 701.85, accuracy: 55.3%\n",
      "Epoch: 50, loss: 671.85, accuracy: 55.3%\n",
      "Epoch: 60, loss: 707.73, accuracy: 55.3%\n",
      "Epoch: 70, loss: 694.45, accuracy: 55.3%\n",
      "Epoch: 80, loss: 708.38, accuracy: 55.3%\n",
      "Epoch: 90, loss: 692.88, accuracy: 55.3%\n",
      "Epoch: 100, loss: 748.15, accuracy: 55.3%\n",
      "Epoch: 110, loss: 731.75, accuracy: 55.3%\n",
      "Epoch: 120, loss: 724.34, accuracy: 55.3%\n",
      "Epoch: 130, loss: 711.44, accuracy: 55.3%\n",
      "Epoch: 140, loss: 800.69, accuracy: 55.3%\n",
      "Epoch: 150, loss: 712.45, accuracy: 55.3%\n",
      "Epoch: 160, loss: 678.17, accuracy: 55.3%\n",
      "Epoch: 170, loss: 690.78, accuracy: 56.2%\n",
      "Epoch: 180, loss: 1068.16, accuracy: 57.0%\n",
      "Epoch: 190, loss: 757.13, accuracy: 55.3%\n",
      "Epoch: 200, loss: 740.54, accuracy: 55.3%\n",
      "Epoch: 210, loss: 742.84, accuracy: 55.3%\n",
      "Epoch: 220, loss: 600.98, accuracy: 55.4%\n",
      "Epoch: 230, loss: 672.27, accuracy: 55.3%\n",
      "Epoch: 240, loss: 693.94, accuracy: 55.3%\n",
      "Epoch: 250, loss: 716.00, accuracy: 55.3%\n",
      "Epoch: 260, loss: 814.29, accuracy: 35.1%\n",
      "Epoch: 270, loss: 725.66, accuracy: 37.8%\n",
      "Epoch: 280, loss: 693.99, accuracy: 55.9%\n",
      "Epoch: 290, loss: 804.92, accuracy: 55.8%\n",
      "Epoch: 300, loss: 702.51, accuracy: 56.8%\n",
      "Mean accuracy on T4: 54.72\n",
      "Mean losses on T4: 716.71\n",
      "\n",
      "Accuracy list on Ti sets: [57.73762593079283, 58.08366184844503, 56.678055190538764, 54.77529566360052, 54.717982456140355]\n",
      "Losses list on Ti sets: [713.4864609617978, 702.0121267110607, 711.7775303398655, 733.5894913787262, 716.7133742556173]\n",
      "Mean accuracy on all T sets: 56.40\n",
      "Mean losses on all T sets: 715.52\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ czwarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: sigmoid\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 2040.49, accuracy: 56.5%\n",
      "Epoch: 20, loss: 931.51, accuracy: 56.5%\n",
      "Epoch: 30, loss: 913.58, accuracy: 56.5%\n",
      "Epoch: 40, loss: 832.77, accuracy: 56.5%\n",
      "Epoch: 50, loss: 788.88, accuracy: 56.5%\n",
      "Epoch: 60, loss: 761.84, accuracy: 56.5%\n",
      "Epoch: 70, loss: 744.17, accuracy: 56.5%\n",
      "Epoch: 80, loss: 732.14, accuracy: 56.5%\n",
      "Epoch: 90, loss: 723.71, accuracy: 56.5%\n",
      "Epoch: 100, loss: 717.66, accuracy: 56.5%\n",
      "Epoch: 110, loss: 713.22, accuracy: 56.5%\n",
      "Epoch: 120, loss: 709.92, accuracy: 56.5%\n",
      "Epoch: 130, loss: 707.41, accuracy: 56.5%\n",
      "Epoch: 140, loss: 705.48, accuracy: 56.5%\n",
      "Epoch: 150, loss: 704.02, accuracy: 56.5%\n",
      "Epoch: 160, loss: 702.96, accuracy: 56.5%\n",
      "Epoch: 170, loss: 702.29, accuracy: 56.5%\n",
      "Epoch: 180, loss: 702.09, accuracy: 56.5%\n",
      "Epoch: 190, loss: 702.43, accuracy: 56.5%\n",
      "Epoch: 200, loss: 703.38, accuracy: 56.5%\n",
      "Epoch: 210, loss: 704.86, accuracy: 56.5%\n",
      "Epoch: 220, loss: 706.56, accuracy: 56.5%\n",
      "Epoch: 230, loss: 708.13, accuracy: 56.5%\n",
      "Epoch: 240, loss: 709.34, accuracy: 56.5%\n",
      "Epoch: 250, loss: 710.21, accuracy: 56.5%\n",
      "Epoch: 260, loss: 710.89, accuracy: 56.5%\n",
      "Epoch: 270, loss: 711.48, accuracy: 56.5%\n",
      "Epoch: 280, loss: 712.05, accuracy: 56.5%\n",
      "Epoch: 290, loss: 712.57, accuracy: 56.5%\n",
      "Epoch: 300, loss: 713.00, accuracy: 56.5%\n",
      "Mean accuracy on T0: 55.24\n",
      "Mean losses on T0: 829.73\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 1966.62, accuracy: 54.3%\n",
      "Epoch: 20, loss: 936.14, accuracy: 54.3%\n",
      "Epoch: 30, loss: 1005.30, accuracy: 54.3%\n",
      "Epoch: 40, loss: 897.14, accuracy: 54.3%\n",
      "Epoch: 50, loss: 843.36, accuracy: 54.3%\n",
      "Epoch: 60, loss: 810.61, accuracy: 54.3%\n",
      "Epoch: 70, loss: 789.27, accuracy: 54.3%\n",
      "Epoch: 80, loss: 774.71, accuracy: 54.3%\n",
      "Epoch: 90, loss: 764.45, accuracy: 54.3%\n",
      "Epoch: 100, loss: 757.05, accuracy: 54.3%\n",
      "Epoch: 110, loss: 751.60, accuracy: 54.3%\n",
      "Epoch: 120, loss: 747.53, accuracy: 54.3%\n",
      "Epoch: 130, loss: 744.45, accuracy: 54.3%\n",
      "Epoch: 140, loss: 742.12, accuracy: 54.3%\n",
      "Epoch: 150, loss: 740.37, accuracy: 54.3%\n",
      "Epoch: 160, loss: 739.12, accuracy: 54.3%\n",
      "Epoch: 170, loss: 738.36, accuracy: 54.3%\n",
      "Epoch: 180, loss: 738.14, accuracy: 54.3%\n",
      "Epoch: 190, loss: 738.54, accuracy: 54.3%\n",
      "Epoch: 200, loss: 739.64, accuracy: 54.3%\n",
      "Epoch: 210, loss: 741.41, accuracy: 54.3%\n",
      "Epoch: 220, loss: 743.59, accuracy: 54.3%\n",
      "Epoch: 230, loss: 745.77, accuracy: 54.3%\n",
      "Epoch: 240, loss: 747.62, accuracy: 54.3%\n",
      "Epoch: 250, loss: 749.07, accuracy: 54.3%\n",
      "Epoch: 260, loss: 750.27, accuracy: 54.3%\n",
      "Epoch: 270, loss: 751.35, accuracy: 54.3%\n",
      "Epoch: 280, loss: 752.38, accuracy: 54.3%\n",
      "Epoch: 290, loss: 753.36, accuracy: 54.3%\n",
      "Epoch: 300, loss: 754.25, accuracy: 54.3%\n",
      "Mean accuracy on T1: 53.01\n",
      "Mean losses on T1: 867.45\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 1534.19, accuracy: 55.6%\n",
      "Epoch: 20, loss: 1270.69, accuracy: 55.6%\n",
      "Epoch: 30, loss: 944.65, accuracy: 55.6%\n",
      "Epoch: 40, loss: 862.62, accuracy: 55.6%\n",
      "Epoch: 50, loss: 814.50, accuracy: 55.6%\n",
      "Epoch: 60, loss: 785.12, accuracy: 55.6%\n",
      "Epoch: 70, loss: 766.00, accuracy: 55.6%\n",
      "Epoch: 80, loss: 753.01, accuracy: 55.6%\n",
      "Epoch: 90, loss: 743.90, accuracy: 55.6%\n",
      "Epoch: 100, loss: 737.36, accuracy: 55.6%\n",
      "Epoch: 110, loss: 732.56, accuracy: 55.6%\n",
      "Epoch: 120, loss: 728.98, accuracy: 55.6%\n",
      "Epoch: 130, loss: 726.27, accuracy: 55.6%\n",
      "Epoch: 140, loss: 724.19, accuracy: 55.6%\n",
      "Epoch: 150, loss: 722.60, accuracy: 55.6%\n",
      "Epoch: 160, loss: 721.44, accuracy: 55.6%\n",
      "Epoch: 170, loss: 720.69, accuracy: 55.6%\n",
      "Epoch: 180, loss: 720.40, accuracy: 55.6%\n",
      "Epoch: 190, loss: 720.65, accuracy: 55.6%\n",
      "Epoch: 200, loss: 721.52, accuracy: 55.6%\n",
      "Epoch: 210, loss: 722.98, accuracy: 55.6%\n",
      "Epoch: 220, loss: 724.79, accuracy: 55.6%\n",
      "Epoch: 230, loss: 726.58, accuracy: 55.6%\n",
      "Epoch: 240, loss: 728.07, accuracy: 55.6%\n",
      "Epoch: 250, loss: 729.22, accuracy: 55.6%\n",
      "Epoch: 260, loss: 730.15, accuracy: 55.6%\n",
      "Epoch: 270, loss: 731.00, accuracy: 55.6%\n",
      "Epoch: 280, loss: 731.83, accuracy: 55.6%\n",
      "Epoch: 290, loss: 732.64, accuracy: 55.6%\n",
      "Epoch: 300, loss: 733.40, accuracy: 55.6%\n",
      "Mean accuracy on T2: 54.28\n",
      "Mean losses on T2: 849.62\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 819.07, accuracy: 59.3%\n",
      "Epoch: 20, loss: 864.57, accuracy: 30.2%\n",
      "Epoch: 30, loss: 886.11, accuracy: 30.2%\n",
      "Epoch: 40, loss: 821.38, accuracy: 30.2%\n",
      "Epoch: 50, loss: 784.38, accuracy: 30.2%\n",
      "Epoch: 60, loss: 760.06, accuracy: 30.2%\n",
      "Epoch: 70, loss: 743.23, accuracy: 30.2%\n",
      "Epoch: 80, loss: 731.19, accuracy: 30.2%\n",
      "Epoch: 90, loss: 722.38, accuracy: 59.3%\n",
      "Epoch: 100, loss: 715.82, accuracy: 59.3%\n",
      "Epoch: 110, loss: 710.88, accuracy: 59.3%\n",
      "Epoch: 120, loss: 707.11, accuracy: 59.3%\n",
      "Epoch: 130, loss: 704.21, accuracy: 59.3%\n",
      "Epoch: 140, loss: 702.00, accuracy: 59.3%\n",
      "Epoch: 150, loss: 700.33, accuracy: 59.3%\n",
      "Epoch: 160, loss: 699.13, accuracy: 59.3%\n",
      "Epoch: 170, loss: 698.39, accuracy: 59.3%\n",
      "Epoch: 180, loss: 698.15, accuracy: 59.3%\n",
      "Epoch: 190, loss: 698.46, accuracy: 59.3%\n",
      "Epoch: 200, loss: 699.40, accuracy: 59.3%\n",
      "Epoch: 210, loss: 700.91, accuracy: 60.1%\n",
      "Epoch: 220, loss: 702.76, accuracy: 62.5%\n",
      "Epoch: 230, loss: 704.57, accuracy: 67.7%\n",
      "Epoch: 240, loss: 706.04, accuracy: 69.6%\n",
      "Epoch: 250, loss: 707.12, accuracy: 69.3%\n",
      "Epoch: 260, loss: 707.90, accuracy: 66.1%\n",
      "Epoch: 270, loss: 708.51, accuracy: 62.9%\n",
      "Epoch: 280, loss: 709.03, accuracy: 58.1%\n",
      "Epoch: 290, loss: 709.44, accuracy: 55.2%\n",
      "Epoch: 300, loss: 709.73, accuracy: 52.4%\n",
      "Mean accuracy on T3: 56.12\n",
      "Mean losses on T3: 821.24\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 900.88, accuracy: 58.6%\n",
      "Epoch: 20, loss: 1179.84, accuracy: 58.6%\n",
      "Epoch: 30, loss: 878.08, accuracy: 58.6%\n",
      "Epoch: 40, loss: 806.41, accuracy: 58.6%\n",
      "Epoch: 50, loss: 765.46, accuracy: 58.6%\n",
      "Epoch: 60, loss: 741.07, accuracy: 58.6%\n",
      "Epoch: 70, loss: 725.62, accuracy: 58.6%\n",
      "Epoch: 80, loss: 715.44, accuracy: 58.6%\n",
      "Epoch: 90, loss: 708.53, accuracy: 58.6%\n",
      "Epoch: 100, loss: 703.72, accuracy: 58.6%\n",
      "Epoch: 110, loss: 700.29, accuracy: 58.6%\n",
      "Epoch: 120, loss: 697.78, accuracy: 58.6%\n",
      "Epoch: 130, loss: 695.87, accuracy: 58.6%\n",
      "Epoch: 140, loss: 694.36, accuracy: 58.6%\n",
      "Epoch: 150, loss: 693.11, accuracy: 58.6%\n",
      "Epoch: 160, loss: 692.06, accuracy: 58.6%\n",
      "Epoch: 170, loss: 691.18, accuracy: 58.6%\n",
      "Epoch: 180, loss: 690.48, accuracy: 58.6%\n",
      "Epoch: 190, loss: 690.02, accuracy: 58.6%\n",
      "Epoch: 200, loss: 689.91, accuracy: 58.6%\n",
      "Epoch: 210, loss: 690.20, accuracy: 58.6%\n",
      "Epoch: 220, loss: 690.82, accuracy: 58.6%\n",
      "Epoch: 230, loss: 691.53, accuracy: 58.6%\n",
      "Epoch: 240, loss: 692.10, accuracy: 58.6%\n",
      "Epoch: 250, loss: 692.38, accuracy: 58.6%\n",
      "Epoch: 260, loss: 692.44, accuracy: 58.6%\n",
      "Epoch: 270, loss: 692.38, accuracy: 58.6%\n",
      "Epoch: 280, loss: 692.27, accuracy: 58.6%\n",
      "Epoch: 290, loss: 692.14, accuracy: 58.6%\n",
      "Epoch: 300, loss: 691.97, accuracy: 58.6%\n",
      "Mean accuracy on T4: 56.15\n",
      "Mean losses on T4: 823.65\n",
      "\n",
      "Accuracy list on Ti sets: [55.24178712220763, 53.0096364432764, 54.28339903635566, 56.12308366184845, 56.149122807017555]\n",
      "Losses list on Ti sets: [829.7260237720002, 867.4541317785919, 849.6190753175, 821.244447668975, 823.6548485476734]\n",
      "Mean accuracy on all T sets: 54.96\n",
      "Mean losses on all T sets: 838.34\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ piÄ…ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: relu\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 702.00, accuracy: 57.0%\n",
      "Epoch: 20, loss: 701.28, accuracy: 57.0%\n",
      "Epoch: 30, loss: 696.30, accuracy: 57.0%\n",
      "Epoch: 40, loss: 668.49, accuracy: 57.0%\n",
      "Epoch: 50, loss: 699.77, accuracy: 57.0%\n",
      "Epoch: 60, loss: 679.54, accuracy: 57.0%\n",
      "Epoch: 70, loss: 702.04, accuracy: 57.0%\n",
      "Epoch: 80, loss: 701.56, accuracy: 57.0%\n",
      "Epoch: 90, loss: 699.02, accuracy: 57.0%\n",
      "Epoch: 100, loss: 683.15, accuracy: 57.0%\n",
      "Epoch: 110, loss: 702.66, accuracy: 57.0%\n",
      "Epoch: 120, loss: 701.29, accuracy: 57.0%\n",
      "Epoch: 130, loss: 697.12, accuracy: 57.0%\n",
      "Epoch: 140, loss: 716.56, accuracy: 46.0%\n",
      "Epoch: 150, loss: 684.55, accuracy: 57.0%\n",
      "Epoch: 160, loss: 704.39, accuracy: 57.0%\n",
      "Epoch: 170, loss: 701.76, accuracy: 57.0%\n",
      "Epoch: 180, loss: 693.98, accuracy: 57.0%\n",
      "Epoch: 190, loss: 664.98, accuracy: 61.2%\n",
      "Epoch: 200, loss: 702.18, accuracy: 57.0%\n",
      "Epoch: 210, loss: 699.46, accuracy: 57.0%\n",
      "Epoch: 220, loss: 685.13, accuracy: 57.0%\n",
      "Epoch: 230, loss: 702.09, accuracy: 57.0%\n",
      "Epoch: 240, loss: 701.59, accuracy: 57.0%\n",
      "Epoch: 250, loss: 700.19, accuracy: 57.0%\n",
      "Epoch: 260, loss: 692.75, accuracy: 57.0%\n",
      "Epoch: 270, loss: 701.73, accuracy: 57.0%\n",
      "Epoch: 280, loss: 697.57, accuracy: 57.0%\n",
      "Epoch: 290, loss: 675.59, accuracy: 57.0%\n",
      "Epoch: 300, loss: 702.12, accuracy: 57.0%\n",
      "Mean accuracy on T0: 56.45\n",
      "Mean losses on T0: 708.03\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 704.63, accuracy: 58.7%\n",
      "Epoch: 20, loss: 704.17, accuracy: 58.7%\n",
      "Epoch: 30, loss: 700.75, accuracy: 58.7%\n",
      "Epoch: 40, loss: 679.46, accuracy: 58.7%\n",
      "Epoch: 50, loss: 689.52, accuracy: 59.1%\n",
      "Epoch: 60, loss: 704.10, accuracy: 58.7%\n",
      "Epoch: 70, loss: 700.63, accuracy: 58.7%\n",
      "Epoch: 80, loss: 691.54, accuracy: 64.1%\n",
      "Epoch: 90, loss: 704.32, accuracy: 58.7%\n",
      "Epoch: 100, loss: 702.68, accuracy: 58.7%\n",
      "Epoch: 110, loss: 692.67, accuracy: 58.7%\n",
      "Epoch: 120, loss: 706.41, accuracy: 58.7%\n",
      "Epoch: 130, loss: 712.39, accuracy: 58.7%\n",
      "Epoch: 140, loss: 703.21, accuracy: 58.7%\n",
      "Epoch: 150, loss: 709.46, accuracy: 58.7%\n",
      "Epoch: 160, loss: 682.44, accuracy: 58.7%\n",
      "Epoch: 170, loss: 660.20, accuracy: 58.7%\n",
      "Epoch: 180, loss: 825.81, accuracy: 58.7%\n",
      "Epoch: 190, loss: 721.93, accuracy: 58.7%\n",
      "Epoch: 200, loss: 638.93, accuracy: 58.7%\n",
      "Epoch: 210, loss: 698.76, accuracy: 58.7%\n",
      "Epoch: 220, loss: 744.65, accuracy: 58.7%\n",
      "Epoch: 230, loss: 680.60, accuracy: 59.3%\n",
      "Epoch: 240, loss: 900.77, accuracy: 29.6%\n",
      "Epoch: 250, loss: 1045.11, accuracy: 29.6%\n",
      "Epoch: 260, loss: 675.39, accuracy: 59.0%\n",
      "Epoch: 270, loss: 682.80, accuracy: 58.7%\n",
      "Epoch: 280, loss: 642.86, accuracy: 71.6%\n",
      "Epoch: 290, loss: 703.16, accuracy: 52.4%\n",
      "Epoch: 300, loss: 618.89, accuracy: 59.9%\n",
      "Mean accuracy on T1: 55.77\n",
      "Mean losses on T1: 721.79\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 733.78, accuracy: 54.8%\n",
      "Epoch: 20, loss: 733.36, accuracy: 54.8%\n",
      "Epoch: 30, loss: 729.95, accuracy: 54.8%\n",
      "Epoch: 40, loss: 707.82, accuracy: 54.9%\n",
      "Epoch: 50, loss: 732.71, accuracy: 54.8%\n",
      "Epoch: 60, loss: 733.78, accuracy: 54.8%\n",
      "Epoch: 70, loss: 732.95, accuracy: 54.8%\n",
      "Epoch: 80, loss: 727.61, accuracy: 54.8%\n",
      "Epoch: 90, loss: 699.93, accuracy: 54.8%\n",
      "Epoch: 100, loss: 733.56, accuracy: 54.8%\n",
      "Epoch: 110, loss: 732.47, accuracy: 54.8%\n",
      "Epoch: 120, loss: 724.08, accuracy: 54.8%\n",
      "Epoch: 130, loss: 758.28, accuracy: 54.8%\n",
      "Epoch: 140, loss: 733.45, accuracy: 54.8%\n",
      "Epoch: 150, loss: 730.73, accuracy: 54.8%\n",
      "Epoch: 160, loss: 713.15, accuracy: 54.8%\n",
      "Epoch: 170, loss: 733.65, accuracy: 54.8%\n",
      "Epoch: 180, loss: 733.63, accuracy: 54.8%\n",
      "Epoch: 190, loss: 731.78, accuracy: 54.8%\n",
      "Epoch: 200, loss: 719.45, accuracy: 54.8%\n",
      "Epoch: 210, loss: 731.62, accuracy: 54.8%\n",
      "Epoch: 220, loss: 730.00, accuracy: 54.8%\n",
      "Epoch: 230, loss: 730.18, accuracy: 54.9%\n",
      "Epoch: 240, loss: 1084.46, accuracy: 32.1%\n",
      "Epoch: 250, loss: 727.66, accuracy: 54.8%\n",
      "Epoch: 260, loss: 685.97, accuracy: 54.8%\n",
      "Epoch: 270, loss: 733.67, accuracy: 54.8%\n",
      "Epoch: 280, loss: 733.60, accuracy: 54.8%\n",
      "Epoch: 290, loss: 731.71, accuracy: 54.8%\n",
      "Epoch: 300, loss: 719.23, accuracy: 54.8%\n",
      "Mean accuracy on T2: 54.28\n",
      "Mean losses on T2: 739.33\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 697.00, accuracy: 58.2%\n",
      "Epoch: 20, loss: 696.61, accuracy: 58.2%\n",
      "Epoch: 30, loss: 693.93, accuracy: 58.2%\n",
      "Epoch: 40, loss: 676.57, accuracy: 58.2%\n",
      "Epoch: 50, loss: 698.28, accuracy: 58.2%\n",
      "Epoch: 60, loss: 696.98, accuracy: 58.2%\n",
      "Epoch: 70, loss: 696.47, accuracy: 58.2%\n",
      "Epoch: 80, loss: 692.92, accuracy: 58.2%\n",
      "Epoch: 90, loss: 671.94, accuracy: 58.2%\n",
      "Epoch: 100, loss: 694.78, accuracy: 58.2%\n",
      "Epoch: 110, loss: 673.15, accuracy: 58.3%\n",
      "Epoch: 120, loss: 695.89, accuracy: 58.2%\n",
      "Epoch: 130, loss: 689.49, accuracy: 58.3%\n",
      "Epoch: 140, loss: 688.19, accuracy: 58.3%\n",
      "Epoch: 150, loss: 730.81, accuracy: 58.2%\n",
      "Epoch: 160, loss: 736.43, accuracy: 58.2%\n",
      "Epoch: 170, loss: 664.06, accuracy: 58.2%\n",
      "Epoch: 180, loss: 764.37, accuracy: 58.2%\n",
      "Epoch: 190, loss: 1217.59, accuracy: 58.2%\n",
      "Epoch: 200, loss: 696.90, accuracy: 58.2%\n",
      "Epoch: 210, loss: 678.19, accuracy: 58.3%\n",
      "Epoch: 220, loss: 702.25, accuracy: 58.2%\n",
      "Epoch: 230, loss: 697.00, accuracy: 58.2%\n",
      "Epoch: 240, loss: 696.76, accuracy: 58.2%\n",
      "Epoch: 250, loss: 695.41, accuracy: 58.2%\n",
      "Epoch: 260, loss: 686.67, accuracy: 58.2%\n",
      "Epoch: 270, loss: 700.30, accuracy: 58.2%\n",
      "Epoch: 280, loss: 710.07, accuracy: 58.2%\n",
      "Epoch: 290, loss: 838.43, accuracy: 58.2%\n",
      "Epoch: 300, loss: 672.45, accuracy: 58.3%\n",
      "Mean accuracy on T3: 57.37\n",
      "Mean losses on T3: 707.51\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 702.89, accuracy: 55.4%\n",
      "Epoch: 20, loss: 702.42, accuracy: 55.4%\n",
      "Epoch: 30, loss: 699.35, accuracy: 55.4%\n",
      "Epoch: 40, loss: 680.61, accuracy: 55.4%\n",
      "Epoch: 50, loss: 710.50, accuracy: 55.4%\n",
      "Epoch: 60, loss: 914.52, accuracy: 55.4%\n",
      "Epoch: 70, loss: 686.75, accuracy: 55.4%\n",
      "Epoch: 80, loss: 683.47, accuracy: 55.4%\n",
      "Epoch: 90, loss: 701.08, accuracy: 55.4%\n",
      "Epoch: 100, loss: 857.55, accuracy: 55.4%\n",
      "Epoch: 110, loss: 739.73, accuracy: 37.1%\n",
      "Epoch: 120, loss: 684.79, accuracy: 55.4%\n",
      "Epoch: 130, loss: 653.50, accuracy: 55.4%\n",
      "Epoch: 140, loss: 703.75, accuracy: 55.9%\n",
      "Epoch: 150, loss: 696.92, accuracy: 55.4%\n",
      "Epoch: 160, loss: 916.01, accuracy: 55.4%\n",
      "Epoch: 170, loss: 696.05, accuracy: 55.4%\n",
      "Epoch: 180, loss: 733.23, accuracy: 39.3%\n",
      "Epoch: 190, loss: 703.06, accuracy: 55.4%\n",
      "Epoch: 200, loss: 687.38, accuracy: 55.4%\n",
      "Epoch: 210, loss: 694.95, accuracy: 55.4%\n",
      "Epoch: 220, loss: 716.88, accuracy: 38.3%\n",
      "Epoch: 230, loss: 676.95, accuracy: 70.4%\n",
      "Epoch: 240, loss: 772.82, accuracy: 34.9%\n",
      "Epoch: 250, loss: 695.48, accuracy: 55.4%\n",
      "Epoch: 260, loss: 721.69, accuracy: 55.4%\n",
      "Epoch: 270, loss: 677.37, accuracy: 55.4%\n",
      "Epoch: 280, loss: 702.81, accuracy: 55.4%\n",
      "Epoch: 290, loss: 686.76, accuracy: 55.4%\n",
      "Epoch: 300, loss: 699.85, accuracy: 55.4%\n",
      "Mean accuracy on T4: 53.95\n",
      "Mean losses on T4: 718.72\n",
      "\n",
      "Accuracy list on Ti sets: [56.44546649145861, 55.772229522558035, 54.279456855015326, 57.36574682435392, 53.94999999999999]\n",
      "Losses list on Ti sets: [708.0268746393457, 721.7931772839507, 739.3279725446633, 707.5065359675914, 718.7196993198684]\n",
      "Mean accuracy on all T sets: 55.56\n",
      "Mean losses on all T sets: 719.07\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ szÃ³sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: sigmoid\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 3440.50, accuracy: 57.6%\n",
      "Epoch: 20, loss: 701.26, accuracy: 57.6%\n",
      "Epoch: 30, loss: 920.55, accuracy: 32.3%\n",
      "Epoch: 40, loss: 945.97, accuracy: 32.3%\n",
      "Epoch: 50, loss: 863.00, accuracy: 32.3%\n",
      "Epoch: 60, loss: 829.68, accuracy: 32.3%\n",
      "Epoch: 70, loss: 803.83, accuracy: 32.3%\n",
      "Epoch: 80, loss: 784.11, accuracy: 32.3%\n",
      "Epoch: 90, loss: 768.72, accuracy: 32.3%\n",
      "Epoch: 100, loss: 756.49, accuracy: 32.3%\n",
      "Epoch: 110, loss: 746.63, accuracy: 32.3%\n",
      "Epoch: 120, loss: 738.58, accuracy: 32.3%\n",
      "Epoch: 130, loss: 731.96, accuracy: 32.3%\n",
      "Epoch: 140, loss: 726.48, accuracy: 33.0%\n",
      "Epoch: 150, loss: 721.90, accuracy: 64.1%\n",
      "Epoch: 160, loss: 718.06, accuracy: 61.8%\n",
      "Epoch: 170, loss: 714.82, accuracy: 58.7%\n",
      "Epoch: 180, loss: 712.09, accuracy: 58.5%\n",
      "Epoch: 190, loss: 709.78, accuracy: 58.5%\n",
      "Epoch: 200, loss: 707.82, accuracy: 58.5%\n",
      "Epoch: 210, loss: 706.17, accuracy: 58.5%\n",
      "Epoch: 220, loss: 704.79, accuracy: 58.7%\n",
      "Epoch: 230, loss: 703.65, accuracy: 58.9%\n",
      "Epoch: 240, loss: 702.74, accuracy: 59.3%\n",
      "Epoch: 250, loss: 702.02, accuracy: 61.0%\n",
      "Epoch: 260, loss: 701.48, accuracy: 62.7%\n",
      "Epoch: 270, loss: 701.10, accuracy: 63.9%\n",
      "Epoch: 280, loss: 700.85, accuracy: 65.8%\n",
      "Epoch: 290, loss: 700.68, accuracy: 69.8%\n",
      "Epoch: 300, loss: 700.56, accuracy: 69.5%\n",
      "Mean accuracy on T0: 52.67\n",
      "Mean losses on T0: 986.17\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 3955.46, accuracy: 55.2%\n",
      "Epoch: 20, loss: 1825.02, accuracy: 55.2%\n",
      "Epoch: 30, loss: 1092.22, accuracy: 55.2%\n",
      "Epoch: 40, loss: 1242.04, accuracy: 55.2%\n",
      "Epoch: 50, loss: 1004.81, accuracy: 55.2%\n",
      "Epoch: 60, loss: 943.39, accuracy: 55.2%\n",
      "Epoch: 70, loss: 894.25, accuracy: 55.2%\n",
      "Epoch: 80, loss: 859.33, accuracy: 55.2%\n",
      "Epoch: 90, loss: 833.51, accuracy: 55.2%\n",
      "Epoch: 100, loss: 813.88, accuracy: 55.2%\n",
      "Epoch: 110, loss: 798.62, accuracy: 55.2%\n",
      "Epoch: 120, loss: 786.54, accuracy: 55.2%\n",
      "Epoch: 130, loss: 776.85, accuracy: 55.2%\n",
      "Epoch: 140, loss: 768.99, accuracy: 55.2%\n",
      "Epoch: 150, loss: 762.54, accuracy: 55.2%\n",
      "Epoch: 160, loss: 757.22, accuracy: 55.2%\n",
      "Epoch: 170, loss: 752.79, accuracy: 55.2%\n",
      "Epoch: 180, loss: 749.09, accuracy: 55.2%\n",
      "Epoch: 190, loss: 745.99, accuracy: 55.2%\n",
      "Epoch: 200, loss: 743.38, accuracy: 55.2%\n",
      "Epoch: 210, loss: 741.19, accuracy: 55.2%\n",
      "Epoch: 220, loss: 739.36, accuracy: 55.2%\n",
      "Epoch: 230, loss: 737.86, accuracy: 55.2%\n",
      "Epoch: 240, loss: 736.64, accuracy: 55.2%\n",
      "Epoch: 250, loss: 735.68, accuracy: 55.2%\n",
      "Epoch: 260, loss: 734.98, accuracy: 55.2%\n",
      "Epoch: 270, loss: 734.49, accuracy: 55.2%\n",
      "Epoch: 280, loss: 734.20, accuracy: 55.2%\n",
      "Epoch: 290, loss: 734.08, accuracy: 55.2%\n",
      "Epoch: 300, loss: 734.09, accuracy: 55.2%\n",
      "Mean accuracy on T1: 50.58\n",
      "Mean losses on T1: 1027.73\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 3757.07, accuracy: 57.0%\n",
      "Epoch: 20, loss: 1538.59, accuracy: 31.3%\n",
      "Epoch: 30, loss: 980.51, accuracy: 31.3%\n",
      "Epoch: 40, loss: 915.68, accuracy: 31.3%\n",
      "Epoch: 50, loss: 872.09, accuracy: 31.3%\n",
      "Epoch: 60, loss: 839.60, accuracy: 31.3%\n",
      "Epoch: 70, loss: 815.30, accuracy: 31.3%\n",
      "Epoch: 80, loss: 796.61, accuracy: 31.3%\n",
      "Epoch: 90, loss: 781.90, accuracy: 31.3%\n",
      "Epoch: 100, loss: 770.11, accuracy: 31.3%\n",
      "Epoch: 110, loss: 760.55, accuracy: 31.3%\n",
      "Epoch: 120, loss: 752.69, accuracy: 31.3%\n",
      "Epoch: 130, loss: 746.18, accuracy: 31.3%\n",
      "Epoch: 140, loss: 740.73, accuracy: 31.3%\n",
      "Epoch: 150, loss: 736.15, accuracy: 67.0%\n",
      "Epoch: 160, loss: 732.28, accuracy: 60.7%\n",
      "Epoch: 170, loss: 728.97, accuracy: 57.8%\n",
      "Epoch: 180, loss: 726.15, accuracy: 57.6%\n",
      "Epoch: 190, loss: 723.71, accuracy: 57.4%\n",
      "Epoch: 200, loss: 721.61, accuracy: 57.4%\n",
      "Epoch: 210, loss: 719.79, accuracy: 57.4%\n",
      "Epoch: 220, loss: 718.21, accuracy: 57.4%\n",
      "Epoch: 230, loss: 716.84, accuracy: 57.6%\n",
      "Epoch: 240, loss: 715.65, accuracy: 57.8%\n",
      "Epoch: 250, loss: 714.64, accuracy: 58.1%\n",
      "Epoch: 260, loss: 713.78, accuracy: 58.7%\n",
      "Epoch: 270, loss: 713.04, accuracy: 59.9%\n",
      "Epoch: 280, loss: 712.41, accuracy: 62.8%\n",
      "Epoch: 290, loss: 711.86, accuracy: 67.0%\n",
      "Epoch: 300, loss: 711.35, accuracy: 67.9%\n",
      "Mean accuracy on T2: 51.60\n",
      "Mean losses on T2: 1006.39\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 3422.52, accuracy: 57.7%\n",
      "Epoch: 20, loss: 699.69, accuracy: 57.7%\n",
      "Epoch: 30, loss: 914.10, accuracy: 32.3%\n",
      "Epoch: 40, loss: 946.60, accuracy: 32.3%\n",
      "Epoch: 50, loss: 862.53, accuracy: 32.3%\n",
      "Epoch: 60, loss: 828.91, accuracy: 32.3%\n",
      "Epoch: 70, loss: 802.83, accuracy: 32.3%\n",
      "Epoch: 80, loss: 782.93, accuracy: 32.3%\n",
      "Epoch: 90, loss: 767.38, accuracy: 32.3%\n",
      "Epoch: 100, loss: 755.02, accuracy: 32.3%\n",
      "Epoch: 110, loss: 745.03, accuracy: 32.3%\n",
      "Epoch: 120, loss: 736.86, accuracy: 32.3%\n",
      "Epoch: 130, loss: 730.12, accuracy: 32.3%\n",
      "Epoch: 140, loss: 724.52, accuracy: 33.9%\n",
      "Epoch: 150, loss: 719.82, accuracy: 66.6%\n",
      "Epoch: 160, loss: 715.85, accuracy: 61.4%\n",
      "Epoch: 170, loss: 712.49, accuracy: 58.2%\n",
      "Epoch: 180, loss: 709.62, accuracy: 57.7%\n",
      "Epoch: 190, loss: 707.15, accuracy: 57.7%\n",
      "Epoch: 200, loss: 705.03, accuracy: 57.7%\n",
      "Epoch: 210, loss: 703.20, accuracy: 57.7%\n",
      "Epoch: 220, loss: 701.62, accuracy: 57.7%\n",
      "Epoch: 230, loss: 700.26, accuracy: 58.0%\n",
      "Epoch: 240, loss: 699.10, accuracy: 58.3%\n",
      "Epoch: 250, loss: 698.12, accuracy: 58.9%\n",
      "Epoch: 260, loss: 697.30, accuracy: 60.2%\n",
      "Epoch: 270, loss: 696.62, accuracy: 62.5%\n",
      "Epoch: 280, loss: 696.06, accuracy: 64.4%\n",
      "Epoch: 290, loss: 695.59, accuracy: 67.5%\n",
      "Epoch: 300, loss: 695.17, accuracy: 70.0%\n",
      "Mean accuracy on T3: 52.56\n",
      "Mean losses on T3: 982.35\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 3622.74, accuracy: 56.7%\n",
      "Epoch: 20, loss: 730.57, accuracy: 56.7%\n",
      "Epoch: 30, loss: 850.80, accuracy: 32.0%\n",
      "Epoch: 40, loss: 949.67, accuracy: 32.0%\n",
      "Epoch: 50, loss: 865.21, accuracy: 32.0%\n",
      "Epoch: 60, loss: 834.11, accuracy: 32.0%\n",
      "Epoch: 70, loss: 809.66, accuracy: 32.0%\n",
      "Epoch: 80, loss: 790.95, accuracy: 32.0%\n",
      "Epoch: 90, loss: 776.31, accuracy: 32.0%\n",
      "Epoch: 100, loss: 764.63, accuracy: 32.0%\n",
      "Epoch: 110, loss: 755.20, accuracy: 32.0%\n",
      "Epoch: 120, loss: 747.48, accuracy: 32.0%\n",
      "Epoch: 130, loss: 741.11, accuracy: 32.0%\n",
      "Epoch: 140, loss: 735.81, accuracy: 32.2%\n",
      "Epoch: 150, loss: 731.37, accuracy: 64.1%\n",
      "Epoch: 160, loss: 727.62, accuracy: 60.3%\n",
      "Epoch: 170, loss: 724.44, accuracy: 57.9%\n",
      "Epoch: 180, loss: 721.72, accuracy: 57.4%\n",
      "Epoch: 190, loss: 719.39, accuracy: 57.2%\n",
      "Epoch: 200, loss: 717.37, accuracy: 57.2%\n",
      "Epoch: 210, loss: 715.63, accuracy: 57.2%\n",
      "Epoch: 220, loss: 714.11, accuracy: 57.4%\n",
      "Epoch: 230, loss: 712.80, accuracy: 57.5%\n",
      "Epoch: 240, loss: 711.67, accuracy: 58.0%\n",
      "Epoch: 250, loss: 710.69, accuracy: 58.6%\n",
      "Epoch: 260, loss: 709.86, accuracy: 59.3%\n",
      "Epoch: 270, loss: 709.15, accuracy: 61.1%\n",
      "Epoch: 280, loss: 708.53, accuracy: 63.4%\n",
      "Epoch: 290, loss: 707.99, accuracy: 67.0%\n",
      "Epoch: 300, loss: 707.47, accuracy: 69.1%\n",
      "Mean accuracy on T4: 51.67\n",
      "Mean losses on T4: 1002.86\n",
      "\n",
      "Accuracy list on Ti sets: [52.674551029347356, 50.580814717477004, 51.60359176522121, 52.56110381077531, 51.67149122807018]\n",
      "Losses list on Ti sets: [986.1687115949776, 1027.7287045663688, 1006.3947000277632, 982.352987387282, 1002.8553689715267]\n",
      "Mean accuracy on all T sets: 51.82\n",
      "Mean losses on all T sets: 1001.10\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ siÃ³dma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 1000, activation: sigmoid\n",
      "2. Layer - input_dim: 1000, output_dim: 100, activation: sigmoid\n",
      "3. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=1000, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=1000, output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 725.89, accuracy: 56.9%\n",
      "Epoch: 20, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 30, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 40, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 50, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 60, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 70, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 80, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 90, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 100, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 110, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 120, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 130, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 140, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 150, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 160, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 170, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 180, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 190, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 200, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 210, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 220, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 230, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 240, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 250, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 260, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 270, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 280, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 290, loss: 726.06, accuracy: 56.9%\n",
      "Epoch: 300, loss: 726.06, accuracy: 56.9%\n",
      "Mean accuracy on T0: 56.81\n",
      "Mean losses on T0: 751.17\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 690.61, accuracy: 58.1%\n",
      "Epoch: 20, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 30, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 40, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 50, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 60, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 70, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 80, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 90, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 100, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 110, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 120, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 130, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 140, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 150, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 160, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 170, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 180, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 190, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 200, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 210, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 220, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 230, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 240, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 250, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 260, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 270, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 280, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 290, loss: 690.54, accuracy: 58.1%\n",
      "Epoch: 300, loss: 690.54, accuracy: 58.1%\n",
      "Mean accuracy on T1: 58.00\n",
      "Mean losses on T1: 714.59\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 690.31, accuracy: 57.3%\n",
      "Epoch: 20, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 30, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 40, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 50, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 60, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 70, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 80, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 90, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 100, loss: 690.21, accuracy: 57.3%\n",
      "Epoch: 110, loss: 690.21, accuracy: 57.3%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-77c65f5451b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_fold_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Dropbox\\version-control\\pszt-projekt\\pszt\\net.py\u001b[0m in \u001b[0;36mk_fold_validation\u001b[1;34m(self, x, y_true, k, epochs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# Calculate accuracy of the trained network on test set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0maccuracies_curr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Dropbox\\version-control\\pszt-projekt\\pszt\\net.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, x, y_true, forward)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;31m# Perform forward step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;31m# Get y_pred values from memory.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Dropbox\\version-control\\pszt-projekt\\pszt\\net.py\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;31m# Calculate input with weights and perform activation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation(x_train, y_train, k=5, epochs=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
