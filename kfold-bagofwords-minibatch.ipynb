{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pszt import net\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_bagofwords.npy')\n",
    "y_train = np.load('y_bagofwords.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ pierwsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: relu\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 595.48, accuracy: 68.9%\n",
      "Epoch: 20, loss: 541.74, accuracy: 73.1%\n",
      "Epoch: 30, loss: 511.25, accuracy: 72.9%\n",
      "Epoch: 40, loss: 511.96, accuracy: 75.2%\n",
      "Epoch: 50, loss: 495.79, accuracy: 74.5%\n",
      "Epoch: 60, loss: 511.81, accuracy: 74.4%\n",
      "Epoch: 70, loss: 539.41, accuracy: 74.5%\n",
      "Epoch: 80, loss: 566.09, accuracy: 74.0%\n",
      "Epoch: 90, loss: 590.16, accuracy: 74.0%\n",
      "Epoch: 100, loss: 612.69, accuracy: 74.4%\n",
      "Epoch: 110, loss: 627.85, accuracy: 74.5%\n",
      "Epoch: 120, loss: 642.34, accuracy: 74.5%\n",
      "Epoch: 130, loss: 657.70, accuracy: 74.6%\n",
      "Epoch: 140, loss: 672.04, accuracy: 74.2%\n",
      "Epoch: 150, loss: 684.47, accuracy: 74.0%\n",
      "Epoch: 160, loss: 695.61, accuracy: 73.9%\n",
      "Epoch: 170, loss: 704.49, accuracy: 73.6%\n",
      "Epoch: 180, loss: 715.31, accuracy: 73.6%\n",
      "Epoch: 190, loss: 727.90, accuracy: 73.6%\n",
      "Epoch: 200, loss: 738.45, accuracy: 73.6%\n",
      "Epoch: 210, loss: 748.41, accuracy: 73.7%\n",
      "Epoch: 220, loss: 752.00, accuracy: 73.7%\n",
      "Epoch: 230, loss: 756.46, accuracy: 73.7%\n",
      "Epoch: 240, loss: 760.82, accuracy: 73.7%\n",
      "Epoch: 250, loss: 765.15, accuracy: 73.6%\n",
      "Epoch: 260, loss: 772.40, accuracy: 73.6%\n",
      "Epoch: 270, loss: 778.42, accuracy: 73.6%\n",
      "Epoch: 280, loss: 780.41, accuracy: 73.6%\n",
      "Epoch: 290, loss: 786.77, accuracy: 73.6%\n",
      "Epoch: 300, loss: 791.22, accuracy: 73.7%\n",
      "Mean accuracy on T0: 73.10\n",
      "Mean losses on T0: 678.53\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 614.49, accuracy: 67.5%\n",
      "Epoch: 20, loss: 572.42, accuracy: 69.9%\n",
      "Epoch: 30, loss: 559.84, accuracy: 70.3%\n",
      "Epoch: 40, loss: 537.82, accuracy: 70.7%\n",
      "Epoch: 50, loss: 533.00, accuracy: 72.4%\n",
      "Epoch: 60, loss: 549.45, accuracy: 74.0%\n",
      "Epoch: 70, loss: 570.99, accuracy: 73.9%\n",
      "Epoch: 80, loss: 595.30, accuracy: 73.9%\n",
      "Epoch: 90, loss: 617.12, accuracy: 73.9%\n",
      "Epoch: 100, loss: 635.60, accuracy: 73.6%\n",
      "Epoch: 110, loss: 653.25, accuracy: 73.2%\n",
      "Epoch: 120, loss: 669.32, accuracy: 73.3%\n",
      "Epoch: 130, loss: 684.85, accuracy: 73.3%\n",
      "Epoch: 140, loss: 698.95, accuracy: 73.3%\n",
      "Epoch: 150, loss: 712.52, accuracy: 72.9%\n",
      "Epoch: 160, loss: 724.47, accuracy: 72.9%\n",
      "Epoch: 170, loss: 735.21, accuracy: 73.2%\n",
      "Epoch: 180, loss: 746.06, accuracy: 73.3%\n",
      "Epoch: 190, loss: 756.12, accuracy: 73.3%\n",
      "Epoch: 200, loss: 764.46, accuracy: 73.3%\n",
      "Epoch: 210, loss: 772.91, accuracy: 73.3%\n",
      "Epoch: 220, loss: 780.47, accuracy: 73.1%\n",
      "Epoch: 230, loss: 787.80, accuracy: 70.4%\n",
      "Epoch: 240, loss: 794.35, accuracy: 70.3%\n",
      "Epoch: 250, loss: 800.02, accuracy: 70.3%\n",
      "Epoch: 260, loss: 806.97, accuracy: 70.4%\n",
      "Epoch: 270, loss: 811.91, accuracy: 70.4%\n",
      "Epoch: 280, loss: 817.65, accuracy: 70.3%\n",
      "Epoch: 290, loss: 821.65, accuracy: 70.3%\n",
      "Epoch: 300, loss: 826.46, accuracy: 70.4%\n",
      "Mean accuracy on T1: 71.96\n",
      "Mean losses on T1: 715.66\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 614.89, accuracy: 66.2%\n",
      "Epoch: 20, loss: 555.05, accuracy: 71.2%\n",
      "Epoch: 30, loss: 514.51, accuracy: 72.9%\n",
      "Epoch: 40, loss: 483.70, accuracy: 75.2%\n",
      "Epoch: 50, loss: 481.18, accuracy: 78.1%\n",
      "Epoch: 60, loss: 475.67, accuracy: 76.2%\n",
      "Epoch: 70, loss: 491.10, accuracy: 76.6%\n",
      "Epoch: 80, loss: 513.69, accuracy: 76.6%\n",
      "Epoch: 90, loss: 534.95, accuracy: 76.5%\n",
      "Epoch: 100, loss: 554.25, accuracy: 75.8%\n",
      "Epoch: 110, loss: 571.27, accuracy: 75.8%\n",
      "Epoch: 120, loss: 586.22, accuracy: 75.6%\n",
      "Epoch: 130, loss: 599.29, accuracy: 75.7%\n",
      "Epoch: 140, loss: 611.31, accuracy: 75.6%\n",
      "Epoch: 150, loss: 621.97, accuracy: 75.6%\n",
      "Epoch: 160, loss: 631.98, accuracy: 75.8%\n",
      "Epoch: 170, loss: 641.16, accuracy: 75.7%\n",
      "Epoch: 180, loss: 649.26, accuracy: 75.7%\n",
      "Epoch: 190, loss: 656.75, accuracy: 75.8%\n",
      "Epoch: 200, loss: 663.80, accuracy: 76.0%\n",
      "Epoch: 210, loss: 670.17, accuracy: 76.0%\n",
      "Epoch: 220, loss: 676.09, accuracy: 76.0%\n",
      "Epoch: 230, loss: 681.71, accuracy: 76.0%\n",
      "Epoch: 240, loss: 686.35, accuracy: 76.2%\n",
      "Epoch: 250, loss: 691.74, accuracy: 76.2%\n",
      "Epoch: 260, loss: 695.26, accuracy: 73.2%\n",
      "Epoch: 270, loss: 699.37, accuracy: 73.2%\n",
      "Epoch: 280, loss: 703.35, accuracy: 73.1%\n",
      "Epoch: 290, loss: 706.65, accuracy: 73.1%\n",
      "Epoch: 300, loss: 709.66, accuracy: 73.2%\n",
      "Mean accuracy on T2: 75.04\n",
      "Mean losses on T2: 625.15\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 581.94, accuracy: 70.2%\n",
      "Epoch: 20, loss: 535.92, accuracy: 72.5%\n",
      "Epoch: 30, loss: 518.97, accuracy: 72.8%\n",
      "Epoch: 40, loss: 477.81, accuracy: 75.3%\n",
      "Epoch: 50, loss: 468.31, accuracy: 75.7%\n",
      "Epoch: 60, loss: 517.84, accuracy: 75.6%\n",
      "Epoch: 70, loss: 499.58, accuracy: 76.2%\n",
      "Epoch: 80, loss: 503.16, accuracy: 75.4%\n",
      "Epoch: 90, loss: 531.02, accuracy: 75.3%\n",
      "Epoch: 100, loss: 555.14, accuracy: 75.0%\n",
      "Epoch: 110, loss: 577.22, accuracy: 74.2%\n",
      "Epoch: 120, loss: 594.95, accuracy: 74.2%\n",
      "Epoch: 130, loss: 612.13, accuracy: 74.2%\n",
      "Epoch: 140, loss: 627.40, accuracy: 74.1%\n",
      "Epoch: 150, loss: 640.75, accuracy: 74.0%\n",
      "Epoch: 160, loss: 652.96, accuracy: 73.9%\n",
      "Epoch: 170, loss: 663.66, accuracy: 74.0%\n",
      "Epoch: 180, loss: 672.85, accuracy: 74.1%\n",
      "Epoch: 190, loss: 681.69, accuracy: 74.1%\n",
      "Epoch: 200, loss: 689.37, accuracy: 71.0%\n",
      "Epoch: 210, loss: 694.84, accuracy: 70.7%\n",
      "Epoch: 220, loss: 700.21, accuracy: 70.8%\n",
      "Epoch: 230, loss: 705.66, accuracy: 70.6%\n",
      "Epoch: 240, loss: 710.69, accuracy: 70.6%\n",
      "Epoch: 250, loss: 715.18, accuracy: 70.7%\n",
      "Epoch: 260, loss: 719.09, accuracy: 70.7%\n",
      "Epoch: 270, loss: 722.44, accuracy: 70.7%\n",
      "Epoch: 280, loss: 725.70, accuracy: 70.8%\n",
      "Epoch: 290, loss: 728.03, accuracy: 70.8%\n",
      "Epoch: 300, loss: 730.79, accuracy: 70.8%\n",
      "Mean accuracy on T3: 73.60\n",
      "Mean losses on T3: 626.36\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 576.80, accuracy: 70.1%\n",
      "Epoch: 20, loss: 515.82, accuracy: 74.1%\n",
      "Epoch: 30, loss: 490.06, accuracy: 75.7%\n",
      "Epoch: 40, loss: 444.88, accuracy: 76.6%\n",
      "Epoch: 50, loss: 437.30, accuracy: 78.4%\n",
      "Epoch: 60, loss: 452.39, accuracy: 78.8%\n",
      "Epoch: 70, loss: 460.80, accuracy: 78.8%\n",
      "Epoch: 80, loss: 481.96, accuracy: 79.3%\n",
      "Epoch: 90, loss: 503.42, accuracy: 79.2%\n",
      "Epoch: 100, loss: 520.17, accuracy: 78.9%\n",
      "Epoch: 110, loss: 539.15, accuracy: 78.2%\n",
      "Epoch: 120, loss: 559.72, accuracy: 78.6%\n",
      "Epoch: 130, loss: 581.19, accuracy: 78.8%\n",
      "Epoch: 140, loss: 598.03, accuracy: 78.7%\n",
      "Epoch: 150, loss: 609.40, accuracy: 78.7%\n",
      "Epoch: 160, loss: 617.24, accuracy: 78.7%\n",
      "Epoch: 170, loss: 628.66, accuracy: 78.3%\n",
      "Epoch: 180, loss: 635.92, accuracy: 78.3%\n",
      "Epoch: 190, loss: 634.93, accuracy: 78.4%\n",
      "Epoch: 200, loss: 645.07, accuracy: 78.2%\n",
      "Epoch: 210, loss: 647.33, accuracy: 78.0%\n",
      "Epoch: 220, loss: 653.99, accuracy: 78.0%\n",
      "Epoch: 230, loss: 656.47, accuracy: 78.2%\n",
      "Epoch: 240, loss: 659.80, accuracy: 77.9%\n",
      "Epoch: 250, loss: 662.76, accuracy: 77.9%\n",
      "Epoch: 260, loss: 664.52, accuracy: 77.9%\n",
      "Epoch: 270, loss: 666.67, accuracy: 77.9%\n",
      "Epoch: 280, loss: 663.97, accuracy: 78.2%\n",
      "Epoch: 290, loss: 669.88, accuracy: 77.9%\n",
      "Epoch: 300, loss: 671.24, accuracy: 77.9%\n",
      "Mean accuracy on T4: 76.63\n",
      "Mean losses on T4: 576.98\n",
      "\n",
      "Accuracy list on Ti sets: [73.0995765805227, 71.95962914294059, 75.036866695868, 73.59742298145714, 76.63011695906432]\n",
      "Losses list on Ti sets: [678.5336908847922, 715.6565223324175, 625.1528105559767, 626.3578227906468, 576.976121846934]\n",
      "Mean accuracy on all T sets: 74.06\n",
      "Mean losses on all T sets: 644.54\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ druga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: sigmoid\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 711.81, accuracy: 58.1%\n",
      "Epoch: 20, loss: 665.14, accuracy: 62.4%\n",
      "Epoch: 30, loss: 617.67, accuracy: 65.6%\n",
      "Epoch: 40, loss: 600.88, accuracy: 67.9%\n",
      "Epoch: 50, loss: 584.22, accuracy: 69.8%\n",
      "Epoch: 60, loss: 569.35, accuracy: 71.6%\n",
      "Epoch: 70, loss: 556.86, accuracy: 71.4%\n",
      "Epoch: 80, loss: 546.33, accuracy: 71.2%\n",
      "Epoch: 90, loss: 537.36, accuracy: 71.7%\n",
      "Epoch: 100, loss: 529.81, accuracy: 71.4%\n",
      "Epoch: 110, loss: 523.62, accuracy: 72.0%\n",
      "Epoch: 120, loss: 518.64, accuracy: 72.4%\n",
      "Epoch: 130, loss: 514.70, accuracy: 72.4%\n",
      "Epoch: 140, loss: 511.66, accuracy: 72.4%\n",
      "Epoch: 150, loss: 509.45, accuracy: 72.8%\n",
      "Epoch: 160, loss: 508.12, accuracy: 73.2%\n",
      "Epoch: 170, loss: 507.68, accuracy: 73.6%\n",
      "Epoch: 180, loss: 508.14, accuracy: 73.6%\n",
      "Epoch: 190, loss: 509.48, accuracy: 73.2%\n",
      "Epoch: 200, loss: 511.67, accuracy: 73.3%\n",
      "Epoch: 210, loss: 514.66, accuracy: 73.2%\n",
      "Epoch: 220, loss: 518.36, accuracy: 73.1%\n",
      "Epoch: 230, loss: 522.66, accuracy: 73.1%\n",
      "Epoch: 240, loss: 527.46, accuracy: 73.1%\n",
      "Epoch: 250, loss: 532.64, accuracy: 73.2%\n",
      "Epoch: 260, loss: 538.10, accuracy: 73.2%\n",
      "Epoch: 270, loss: 543.78, accuracy: 73.2%\n",
      "Epoch: 280, loss: 549.61, accuracy: 72.9%\n",
      "Epoch: 290, loss: 555.53, accuracy: 72.7%\n",
      "Epoch: 300, loss: 561.49, accuracy: 72.5%\n",
      "Mean accuracy on T0: 71.13\n",
      "Mean losses on T0: 550.97\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 691.46, accuracy: 56.6%\n",
      "Epoch: 20, loss: 658.29, accuracy: 57.3%\n",
      "Epoch: 30, loss: 600.04, accuracy: 67.4%\n",
      "Epoch: 40, loss: 578.21, accuracy: 70.0%\n",
      "Epoch: 50, loss: 561.23, accuracy: 71.2%\n",
      "Epoch: 60, loss: 544.87, accuracy: 72.1%\n",
      "Epoch: 70, loss: 529.27, accuracy: 73.7%\n",
      "Epoch: 80, loss: 515.49, accuracy: 73.3%\n",
      "Epoch: 90, loss: 504.18, accuracy: 73.9%\n",
      "Epoch: 100, loss: 495.29, accuracy: 74.4%\n",
      "Epoch: 110, loss: 488.41, accuracy: 74.9%\n",
      "Epoch: 120, loss: 483.14, accuracy: 74.8%\n",
      "Epoch: 130, loss: 479.16, accuracy: 75.7%\n",
      "Epoch: 140, loss: 476.30, accuracy: 76.1%\n",
      "Epoch: 150, loss: 474.49, accuracy: 77.0%\n",
      "Epoch: 160, loss: 473.70, accuracy: 76.5%\n",
      "Epoch: 170, loss: 473.88, accuracy: 76.2%\n",
      "Epoch: 180, loss: 475.00, accuracy: 76.5%\n",
      "Epoch: 190, loss: 476.99, accuracy: 76.3%\n",
      "Epoch: 200, loss: 479.75, accuracy: 76.2%\n",
      "Epoch: 210, loss: 483.18, accuracy: 76.2%\n",
      "Epoch: 220, loss: 487.15, accuracy: 76.1%\n",
      "Epoch: 230, loss: 491.54, accuracy: 75.8%\n",
      "Epoch: 240, loss: 496.23, accuracy: 76.1%\n",
      "Epoch: 250, loss: 501.12, accuracy: 76.2%\n",
      "Epoch: 260, loss: 506.15, accuracy: 76.0%\n",
      "Epoch: 270, loss: 511.26, accuracy: 75.6%\n",
      "Epoch: 280, loss: 516.40, accuracy: 75.6%\n",
      "Epoch: 290, loss: 521.56, accuracy: 75.6%\n",
      "Epoch: 300, loss: 526.71, accuracy: 75.6%\n",
      "Mean accuracy on T1: 73.37\n",
      "Mean losses on T1: 521.94\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 694.51, accuracy: 56.4%\n",
      "Epoch: 20, loss: 657.46, accuracy: 57.8%\n",
      "Epoch: 30, loss: 591.44, accuracy: 70.6%\n",
      "Epoch: 40, loss: 567.93, accuracy: 70.8%\n",
      "Epoch: 50, loss: 553.57, accuracy: 71.0%\n",
      "Epoch: 60, loss: 541.04, accuracy: 71.0%\n",
      "Epoch: 70, loss: 529.59, accuracy: 71.5%\n",
      "Epoch: 80, loss: 519.44, accuracy: 71.6%\n",
      "Epoch: 90, loss: 510.53, accuracy: 72.1%\n",
      "Epoch: 100, loss: 502.70, accuracy: 72.5%\n",
      "Epoch: 110, loss: 495.92, accuracy: 72.4%\n",
      "Epoch: 120, loss: 490.24, accuracy: 72.7%\n",
      "Epoch: 130, loss: 485.69, accuracy: 73.1%\n",
      "Epoch: 140, loss: 482.25, accuracy: 74.2%\n",
      "Epoch: 150, loss: 479.85, accuracy: 75.0%\n",
      "Epoch: 160, loss: 478.36, accuracy: 75.0%\n",
      "Epoch: 170, loss: 477.69, accuracy: 74.8%\n",
      "Epoch: 180, loss: 477.73, accuracy: 75.2%\n",
      "Epoch: 190, loss: 478.39, accuracy: 75.4%\n",
      "Epoch: 200, loss: 479.61, accuracy: 75.4%\n",
      "Epoch: 210, loss: 481.33, accuracy: 75.6%\n",
      "Epoch: 220, loss: 483.52, accuracy: 75.8%\n",
      "Epoch: 230, loss: 486.16, accuracy: 75.4%\n",
      "Epoch: 240, loss: 489.21, accuracy: 75.6%\n",
      "Epoch: 250, loss: 492.63, accuracy: 74.9%\n",
      "Epoch: 260, loss: 496.37, accuracy: 74.9%\n",
      "Epoch: 270, loss: 500.38, accuracy: 74.9%\n",
      "Epoch: 280, loss: 504.63, accuracy: 75.3%\n",
      "Epoch: 290, loss: 509.05, accuracy: 75.3%\n",
      "Epoch: 300, loss: 513.61, accuracy: 75.4%\n",
      "Mean accuracy on T2: 72.12\n",
      "Mean losses on T2: 522.18\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 691.01, accuracy: 58.3%\n",
      "Epoch: 20, loss: 653.74, accuracy: 59.9%\n",
      "Epoch: 30, loss: 591.01, accuracy: 68.7%\n",
      "Epoch: 40, loss: 569.89, accuracy: 70.0%\n",
      "Epoch: 50, loss: 552.68, accuracy: 71.9%\n",
      "Epoch: 60, loss: 537.04, accuracy: 71.6%\n",
      "Epoch: 70, loss: 523.44, accuracy: 72.9%\n",
      "Epoch: 80, loss: 512.05, accuracy: 73.9%\n",
      "Epoch: 90, loss: 502.44, accuracy: 73.9%\n",
      "Epoch: 100, loss: 494.20, accuracy: 73.6%\n",
      "Epoch: 110, loss: 487.12, accuracy: 74.5%\n",
      "Epoch: 120, loss: 481.17, accuracy: 75.7%\n",
      "Epoch: 130, loss: 476.36, accuracy: 76.3%\n",
      "Epoch: 140, loss: 472.68, accuracy: 77.0%\n",
      "Epoch: 150, loss: 470.16, accuracy: 76.9%\n",
      "Epoch: 160, loss: 468.85, accuracy: 77.1%\n",
      "Epoch: 170, loss: 468.77, accuracy: 76.7%\n",
      "Epoch: 180, loss: 469.90, accuracy: 76.9%\n",
      "Epoch: 190, loss: 472.13, accuracy: 76.6%\n",
      "Epoch: 200, loss: 475.29, accuracy: 76.6%\n",
      "Epoch: 210, loss: 479.20, accuracy: 76.7%\n",
      "Epoch: 220, loss: 483.70, accuracy: 76.3%\n",
      "Epoch: 230, loss: 488.65, accuracy: 75.8%\n",
      "Epoch: 240, loss: 493.92, accuracy: 75.8%\n",
      "Epoch: 250, loss: 499.44, accuracy: 75.8%\n",
      "Epoch: 260, loss: 505.12, accuracy: 75.7%\n",
      "Epoch: 270, loss: 510.91, accuracy: 75.8%\n",
      "Epoch: 280, loss: 516.76, accuracy: 75.7%\n",
      "Epoch: 290, loss: 522.63, accuracy: 75.3%\n",
      "Epoch: 300, loss: 528.50, accuracy: 75.3%\n",
      "Mean accuracy on T3: 73.87\n",
      "Mean losses on T3: 517.10\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 722.40, accuracy: 54.7%\n",
      "Epoch: 20, loss: 677.11, accuracy: 55.0%\n",
      "Epoch: 30, loss: 622.12, accuracy: 68.6%\n",
      "Epoch: 40, loss: 605.32, accuracy: 69.1%\n",
      "Epoch: 50, loss: 593.22, accuracy: 69.9%\n",
      "Epoch: 60, loss: 582.99, accuracy: 70.4%\n",
      "Epoch: 70, loss: 573.99, accuracy: 71.4%\n",
      "Epoch: 80, loss: 565.84, accuracy: 71.3%\n",
      "Epoch: 90, loss: 558.52, accuracy: 71.4%\n",
      "Epoch: 100, loss: 552.21, accuracy: 71.1%\n",
      "Epoch: 110, loss: 547.09, accuracy: 71.4%\n",
      "Epoch: 120, loss: 543.26, accuracy: 71.7%\n",
      "Epoch: 130, loss: 540.75, accuracy: 72.6%\n",
      "Epoch: 140, loss: 539.50, accuracy: 72.4%\n",
      "Epoch: 150, loss: 539.40, accuracy: 71.8%\n",
      "Epoch: 160, loss: 540.33, accuracy: 72.0%\n",
      "Epoch: 170, loss: 542.18, accuracy: 72.0%\n",
      "Epoch: 180, loss: 544.84, accuracy: 72.1%\n",
      "Epoch: 190, loss: 548.23, accuracy: 72.1%\n",
      "Epoch: 200, loss: 552.30, accuracy: 72.5%\n",
      "Epoch: 210, loss: 557.01, accuracy: 72.8%\n",
      "Epoch: 220, loss: 562.30, accuracy: 72.6%\n",
      "Epoch: 230, loss: 568.09, accuracy: 73.0%\n",
      "Epoch: 240, loss: 574.30, accuracy: 73.9%\n",
      "Epoch: 250, loss: 580.83, accuracy: 73.8%\n",
      "Epoch: 260, loss: 587.59, accuracy: 74.3%\n",
      "Epoch: 270, loss: 594.49, accuracy: 74.6%\n",
      "Epoch: 280, loss: 601.46, accuracy: 74.6%\n",
      "Epoch: 290, loss: 608.46, accuracy: 74.3%\n",
      "Epoch: 300, loss: 615.43, accuracy: 73.9%\n",
      "Mean accuracy on T4: 70.82\n",
      "Mean losses on T4: 572.28\n",
      "\n",
      "Accuracy list on Ti sets: [71.12910643889619, 73.36611184114467, 72.11768141334501, 73.86673236968902, 70.81692251461988]\n",
      "Losses list on Ti sets: [550.971394954996, 521.9366931007972, 522.1838229055854, 517.09665581, 572.282492696901]\n",
      "Mean accuracy on all T sets: 72.26\n",
      "Mean losses on all T sets: 536.89\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ trzecia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: relu\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 613.88, accuracy: 64.3%\n",
      "Epoch: 20, loss: 561.98, accuracy: 68.9%\n",
      "Epoch: 30, loss: 528.82, accuracy: 69.4%\n",
      "Epoch: 40, loss: 506.34, accuracy: 72.7%\n",
      "Epoch: 50, loss: 537.50, accuracy: 72.5%\n",
      "Epoch: 60, loss: 523.04, accuracy: 72.8%\n",
      "Epoch: 70, loss: 549.21, accuracy: 73.3%\n",
      "Epoch: 80, loss: 576.03, accuracy: 73.9%\n",
      "Epoch: 90, loss: 600.57, accuracy: 73.5%\n",
      "Epoch: 100, loss: 623.49, accuracy: 73.6%\n",
      "Epoch: 110, loss: 642.14, accuracy: 73.7%\n",
      "Epoch: 120, loss: 657.45, accuracy: 73.6%\n",
      "Epoch: 130, loss: 671.75, accuracy: 73.3%\n",
      "Epoch: 140, loss: 680.09, accuracy: 73.2%\n",
      "Epoch: 150, loss: 695.98, accuracy: 69.3%\n",
      "Epoch: 160, loss: 708.75, accuracy: 69.4%\n",
      "Epoch: 170, loss: 713.86, accuracy: 69.0%\n",
      "Epoch: 180, loss: 721.08, accuracy: 69.0%\n",
      "Epoch: 190, loss: 727.32, accuracy: 69.5%\n",
      "Epoch: 200, loss: 731.43, accuracy: 69.3%\n",
      "Epoch: 210, loss: 736.29, accuracy: 69.3%\n",
      "Epoch: 220, loss: 741.01, accuracy: 69.4%\n",
      "Epoch: 230, loss: 744.50, accuracy: 69.3%\n",
      "Epoch: 240, loss: 748.98, accuracy: 69.3%\n",
      "Epoch: 250, loss: 751.47, accuracy: 69.3%\n",
      "Epoch: 260, loss: 753.80, accuracy: 72.9%\n",
      "Epoch: 270, loss: 763.65, accuracy: 69.3%\n",
      "Epoch: 280, loss: 758.84, accuracy: 72.9%\n",
      "Epoch: 290, loss: 761.24, accuracy: 72.9%\n",
      "Epoch: 300, loss: 763.37, accuracy: 73.1%\n",
      "Mean accuracy on T0: 72.27\n",
      "Mean losses on T0: 665.78\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 583.89, accuracy: 67.1%\n",
      "Epoch: 20, loss: 530.10, accuracy: 72.7%\n",
      "Epoch: 30, loss: 491.35, accuracy: 74.4%\n",
      "Epoch: 40, loss: 468.09, accuracy: 75.4%\n",
      "Epoch: 50, loss: 471.84, accuracy: 76.1%\n",
      "Epoch: 60, loss: 490.89, accuracy: 76.0%\n",
      "Epoch: 70, loss: 514.35, accuracy: 76.3%\n",
      "Epoch: 80, loss: 537.56, accuracy: 76.1%\n",
      "Epoch: 90, loss: 559.45, accuracy: 76.0%\n",
      "Epoch: 100, loss: 578.17, accuracy: 76.0%\n",
      "Epoch: 110, loss: 594.68, accuracy: 76.2%\n",
      "Epoch: 120, loss: 611.92, accuracy: 73.3%\n",
      "Epoch: 130, loss: 623.40, accuracy: 72.9%\n",
      "Epoch: 140, loss: 633.73, accuracy: 72.9%\n",
      "Epoch: 150, loss: 641.92, accuracy: 73.1%\n",
      "Epoch: 160, loss: 649.04, accuracy: 73.1%\n",
      "Epoch: 170, loss: 657.42, accuracy: 73.3%\n",
      "Epoch: 180, loss: 663.13, accuracy: 76.6%\n",
      "Epoch: 190, loss: 668.73, accuracy: 76.5%\n",
      "Epoch: 200, loss: 672.24, accuracy: 76.5%\n",
      "Epoch: 210, loss: 677.00, accuracy: 76.2%\n",
      "Epoch: 220, loss: 681.01, accuracy: 76.2%\n",
      "Epoch: 230, loss: 684.92, accuracy: 76.5%\n",
      "Epoch: 240, loss: 690.04, accuracy: 76.6%\n",
      "Epoch: 250, loss: 693.74, accuracy: 76.6%\n",
      "Epoch: 260, loss: 696.89, accuracy: 76.6%\n",
      "Epoch: 270, loss: 700.33, accuracy: 76.6%\n",
      "Epoch: 280, loss: 703.50, accuracy: 76.6%\n",
      "Epoch: 290, loss: 706.72, accuracy: 76.6%\n",
      "Epoch: 300, loss: 709.28, accuracy: 76.5%\n",
      "Mean accuracy on T1: 75.14\n",
      "Mean losses on T1: 613.59\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 582.20, accuracy: 70.6%\n",
      "Epoch: 20, loss: 537.62, accuracy: 71.7%\n",
      "Epoch: 30, loss: 510.99, accuracy: 72.8%\n",
      "Epoch: 40, loss: 487.58, accuracy: 74.2%\n",
      "Epoch: 50, loss: 491.81, accuracy: 74.9%\n",
      "Epoch: 60, loss: 506.41, accuracy: 75.3%\n",
      "Epoch: 70, loss: 525.07, accuracy: 74.6%\n",
      "Epoch: 80, loss: 543.25, accuracy: 74.0%\n",
      "Epoch: 90, loss: 564.99, accuracy: 74.5%\n",
      "Epoch: 100, loss: 578.34, accuracy: 74.5%\n",
      "Epoch: 110, loss: 592.70, accuracy: 74.5%\n",
      "Epoch: 120, loss: 605.83, accuracy: 74.9%\n",
      "Epoch: 130, loss: 617.38, accuracy: 74.8%\n",
      "Epoch: 140, loss: 627.53, accuracy: 74.8%\n",
      "Epoch: 150, loss: 636.36, accuracy: 74.6%\n",
      "Epoch: 160, loss: 644.37, accuracy: 74.6%\n",
      "Epoch: 170, loss: 651.72, accuracy: 74.9%\n",
      "Epoch: 180, loss: 658.47, accuracy: 74.8%\n",
      "Epoch: 190, loss: 664.71, accuracy: 74.6%\n",
      "Epoch: 200, loss: 670.56, accuracy: 74.6%\n",
      "Epoch: 210, loss: 674.90, accuracy: 74.6%\n",
      "Epoch: 220, loss: 681.11, accuracy: 74.2%\n",
      "Epoch: 230, loss: 685.72, accuracy: 74.2%\n",
      "Epoch: 240, loss: 690.10, accuracy: 74.2%\n",
      "Epoch: 250, loss: 694.71, accuracy: 74.2%\n",
      "Epoch: 260, loss: 698.77, accuracy: 74.2%\n",
      "Epoch: 270, loss: 701.39, accuracy: 74.4%\n",
      "Epoch: 280, loss: 706.06, accuracy: 74.2%\n",
      "Epoch: 290, loss: 709.31, accuracy: 74.5%\n",
      "Epoch: 300, loss: 711.84, accuracy: 74.4%\n",
      "Mean accuracy on T2: 73.73\n",
      "Mean losses on T2: 643.64\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 604.53, accuracy: 68.3%\n",
      "Epoch: 20, loss: 534.02, accuracy: 73.1%\n",
      "Epoch: 30, loss: 499.08, accuracy: 73.5%\n",
      "Epoch: 40, loss: 458.37, accuracy: 76.2%\n",
      "Epoch: 50, loss: 516.63, accuracy: 75.8%\n",
      "Epoch: 60, loss: 503.24, accuracy: 75.4%\n",
      "Epoch: 70, loss: 499.78, accuracy: 75.2%\n",
      "Epoch: 80, loss: 510.48, accuracy: 75.3%\n",
      "Epoch: 90, loss: 534.83, accuracy: 74.9%\n",
      "Epoch: 100, loss: 552.61, accuracy: 74.9%\n",
      "Epoch: 110, loss: 568.27, accuracy: 74.9%\n",
      "Epoch: 120, loss: 582.07, accuracy: 74.6%\n",
      "Epoch: 130, loss: 591.69, accuracy: 74.9%\n",
      "Epoch: 140, loss: 601.21, accuracy: 74.9%\n",
      "Epoch: 150, loss: 610.13, accuracy: 74.6%\n",
      "Epoch: 160, loss: 617.31, accuracy: 74.6%\n",
      "Epoch: 170, loss: 624.08, accuracy: 74.5%\n",
      "Epoch: 180, loss: 630.92, accuracy: 74.5%\n",
      "Epoch: 190, loss: 637.41, accuracy: 74.6%\n",
      "Epoch: 200, loss: 643.66, accuracy: 74.5%\n",
      "Epoch: 210, loss: 648.82, accuracy: 74.5%\n",
      "Epoch: 220, loss: 657.24, accuracy: 72.0%\n",
      "Epoch: 230, loss: 659.30, accuracy: 74.5%\n",
      "Epoch: 240, loss: 660.24, accuracy: 74.6%\n",
      "Epoch: 250, loss: 664.76, accuracy: 74.6%\n",
      "Epoch: 260, loss: 670.23, accuracy: 74.5%\n",
      "Epoch: 270, loss: 673.50, accuracy: 74.6%\n",
      "Epoch: 280, loss: 674.51, accuracy: 74.6%\n",
      "Epoch: 290, loss: 677.00, accuracy: 74.6%\n",
      "Epoch: 300, loss: 682.57, accuracy: 74.6%\n",
      "Mean accuracy on T3: 73.96\n",
      "Mean losses on T3: 622.77\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 572.55, accuracy: 73.0%\n",
      "Epoch: 20, loss: 509.92, accuracy: 75.9%\n",
      "Epoch: 30, loss: 474.85, accuracy: 78.4%\n",
      "Epoch: 40, loss: 493.98, accuracy: 78.2%\n",
      "Epoch: 50, loss: 491.44, accuracy: 79.1%\n",
      "Epoch: 60, loss: 482.37, accuracy: 78.9%\n",
      "Epoch: 70, loss: 487.91, accuracy: 78.7%\n",
      "Epoch: 80, loss: 507.08, accuracy: 78.4%\n",
      "Epoch: 90, loss: 528.04, accuracy: 78.2%\n",
      "Epoch: 100, loss: 546.90, accuracy: 77.9%\n",
      "Epoch: 110, loss: 562.09, accuracy: 78.0%\n",
      "Epoch: 120, loss: 575.64, accuracy: 78.0%\n",
      "Epoch: 130, loss: 587.38, accuracy: 78.0%\n",
      "Epoch: 140, loss: 597.75, accuracy: 77.9%\n",
      "Epoch: 150, loss: 607.25, accuracy: 77.8%\n",
      "Epoch: 160, loss: 615.83, accuracy: 77.8%\n",
      "Epoch: 170, loss: 623.17, accuracy: 77.6%\n",
      "Epoch: 180, loss: 630.53, accuracy: 77.5%\n",
      "Epoch: 190, loss: 638.05, accuracy: 77.6%\n",
      "Epoch: 200, loss: 642.58, accuracy: 77.6%\n",
      "Epoch: 210, loss: 650.43, accuracy: 77.5%\n",
      "Epoch: 220, loss: 651.50, accuracy: 77.6%\n",
      "Epoch: 230, loss: 656.48, accuracy: 77.6%\n",
      "Epoch: 240, loss: 659.24, accuracy: 77.6%\n",
      "Epoch: 250, loss: 664.08, accuracy: 77.8%\n",
      "Epoch: 260, loss: 667.99, accuracy: 77.8%\n",
      "Epoch: 270, loss: 669.29, accuracy: 77.8%\n",
      "Epoch: 280, loss: 671.56, accuracy: 77.6%\n",
      "Epoch: 290, loss: 674.41, accuracy: 77.6%\n",
      "Epoch: 300, loss: 676.92, accuracy: 77.4%\n",
      "Mean accuracy on T4: 76.65\n",
      "Mean losses on T4: 602.27\n",
      "\n",
      "Accuracy list on Ti sets: [72.26792232442692, 75.1372828150095, 73.72631041027887, 73.96295079573659, 76.65084064327485]\n",
      "Losses list on Ti sets: [665.7840719982775, 613.5885447643608, 643.6407228951653, 622.7655938662352, 602.2691755702269]\n",
      "Mean accuracy on all T sets: 74.35\n",
      "Mean losses on all T sets: 629.61\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ czwarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: sigmoid\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 696.15, accuracy: 56.5%\n",
      "Epoch: 20, loss: 634.48, accuracy: 62.9%\n",
      "Epoch: 30, loss: 574.66, accuracy: 69.3%\n",
      "Epoch: 40, loss: 559.59, accuracy: 70.8%\n",
      "Epoch: 50, loss: 548.58, accuracy: 72.8%\n",
      "Epoch: 60, loss: 539.60, accuracy: 73.1%\n",
      "Epoch: 70, loss: 532.24, accuracy: 73.1%\n",
      "Epoch: 80, loss: 525.88, accuracy: 72.5%\n",
      "Epoch: 90, loss: 520.01, accuracy: 72.7%\n",
      "Epoch: 100, loss: 514.41, accuracy: 73.2%\n",
      "Epoch: 110, loss: 509.05, accuracy: 74.1%\n",
      "Epoch: 120, loss: 504.11, accuracy: 74.1%\n",
      "Epoch: 130, loss: 500.06, accuracy: 74.6%\n",
      "Epoch: 140, loss: 497.31, accuracy: 75.3%\n",
      "Epoch: 150, loss: 495.99, accuracy: 75.3%\n",
      "Epoch: 160, loss: 496.06, accuracy: 75.3%\n",
      "Epoch: 170, loss: 497.45, accuracy: 75.3%\n",
      "Epoch: 180, loss: 500.05, accuracy: 74.8%\n",
      "Epoch: 190, loss: 503.73, accuracy: 74.6%\n",
      "Epoch: 200, loss: 508.32, accuracy: 74.8%\n",
      "Epoch: 210, loss: 513.62, accuracy: 74.9%\n",
      "Epoch: 220, loss: 519.44, accuracy: 74.8%\n",
      "Epoch: 230, loss: 525.59, accuracy: 75.2%\n",
      "Epoch: 240, loss: 531.96, accuracy: 75.2%\n",
      "Epoch: 250, loss: 538.44, accuracy: 75.2%\n",
      "Epoch: 260, loss: 544.96, accuracy: 75.3%\n",
      "Epoch: 270, loss: 551.45, accuracy: 75.3%\n",
      "Epoch: 280, loss: 557.88, accuracy: 75.6%\n",
      "Epoch: 290, loss: 564.21, accuracy: 75.6%\n",
      "Epoch: 300, loss: 570.42, accuracy: 75.6%\n",
      "Mean accuracy on T0: 72.65\n",
      "Mean losses on T0: 539.87\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 726.24, accuracy: 54.3%\n",
      "Epoch: 20, loss: 658.63, accuracy: 64.7%\n",
      "Epoch: 30, loss: 625.62, accuracy: 66.8%\n",
      "Epoch: 40, loss: 611.96, accuracy: 67.8%\n",
      "Epoch: 50, loss: 598.98, accuracy: 69.1%\n",
      "Epoch: 60, loss: 587.65, accuracy: 69.1%\n",
      "Epoch: 70, loss: 578.24, accuracy: 69.8%\n",
      "Epoch: 80, loss: 570.20, accuracy: 69.9%\n",
      "Epoch: 90, loss: 562.92, accuracy: 70.7%\n",
      "Epoch: 100, loss: 556.08, accuracy: 70.6%\n",
      "Epoch: 110, loss: 549.54, accuracy: 71.2%\n",
      "Epoch: 120, loss: 543.42, accuracy: 71.5%\n",
      "Epoch: 130, loss: 538.22, accuracy: 72.1%\n",
      "Epoch: 140, loss: 534.49, accuracy: 72.7%\n",
      "Epoch: 150, loss: 532.39, accuracy: 72.8%\n",
      "Epoch: 160, loss: 531.91, accuracy: 73.6%\n",
      "Epoch: 170, loss: 532.96, accuracy: 73.6%\n",
      "Epoch: 180, loss: 535.43, accuracy: 74.0%\n",
      "Epoch: 190, loss: 539.15, accuracy: 73.7%\n",
      "Epoch: 200, loss: 543.91, accuracy: 73.7%\n",
      "Epoch: 210, loss: 549.50, accuracy: 74.0%\n",
      "Epoch: 220, loss: 555.71, accuracy: 73.9%\n",
      "Epoch: 230, loss: 562.34, accuracy: 73.9%\n",
      "Epoch: 240, loss: 569.24, accuracy: 74.0%\n",
      "Epoch: 250, loss: 576.29, accuracy: 74.0%\n",
      "Epoch: 260, loss: 583.38, accuracy: 74.0%\n",
      "Epoch: 270, loss: 590.44, accuracy: 74.1%\n",
      "Epoch: 280, loss: 597.43, accuracy: 74.0%\n",
      "Epoch: 290, loss: 604.31, accuracy: 74.1%\n",
      "Epoch: 300, loss: 611.05, accuracy: 74.1%\n",
      "Mean accuracy on T1: 70.87\n",
      "Mean losses on T1: 578.88\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 712.89, accuracy: 55.6%\n",
      "Epoch: 20, loss: 650.10, accuracy: 62.4%\n",
      "Epoch: 30, loss: 593.79, accuracy: 70.4%\n",
      "Epoch: 40, loss: 575.31, accuracy: 72.1%\n",
      "Epoch: 50, loss: 561.79, accuracy: 71.6%\n",
      "Epoch: 60, loss: 551.08, accuracy: 72.0%\n",
      "Epoch: 70, loss: 542.03, accuracy: 72.5%\n",
      "Epoch: 80, loss: 534.00, accuracy: 72.1%\n",
      "Epoch: 90, loss: 526.65, accuracy: 72.7%\n",
      "Epoch: 100, loss: 519.85, accuracy: 72.7%\n",
      "Epoch: 110, loss: 513.49, accuracy: 73.7%\n",
      "Epoch: 120, loss: 507.62, accuracy: 74.1%\n",
      "Epoch: 130, loss: 502.64, accuracy: 74.8%\n",
      "Epoch: 140, loss: 499.19, accuracy: 75.3%\n",
      "Epoch: 150, loss: 497.66, accuracy: 75.8%\n",
      "Epoch: 160, loss: 498.13, accuracy: 75.8%\n",
      "Epoch: 170, loss: 500.40, accuracy: 76.2%\n",
      "Epoch: 180, loss: 504.20, accuracy: 75.8%\n",
      "Epoch: 190, loss: 509.23, accuracy: 75.8%\n",
      "Epoch: 200, loss: 515.20, accuracy: 76.2%\n",
      "Epoch: 210, loss: 521.86, accuracy: 76.2%\n",
      "Epoch: 220, loss: 529.03, accuracy: 76.3%\n",
      "Epoch: 230, loss: 536.53, accuracy: 76.3%\n",
      "Epoch: 240, loss: 544.24, accuracy: 76.3%\n",
      "Epoch: 250, loss: 552.03, accuracy: 76.3%\n",
      "Epoch: 260, loss: 559.83, accuracy: 76.2%\n",
      "Epoch: 270, loss: 567.56, accuracy: 76.1%\n",
      "Epoch: 280, loss: 575.18, accuracy: 76.1%\n",
      "Epoch: 290, loss: 582.65, accuracy: 75.8%\n",
      "Epoch: 300, loss: 589.96, accuracy: 75.6%\n",
      "Mean accuracy on T2: 73.01\n",
      "Mean losses on T2: 550.57\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 688.25, accuracy: 59.3%\n",
      "Epoch: 20, loss: 625.64, accuracy: 68.5%\n",
      "Epoch: 30, loss: 597.36, accuracy: 67.3%\n",
      "Epoch: 40, loss: 583.84, accuracy: 69.9%\n",
      "Epoch: 50, loss: 571.17, accuracy: 70.7%\n",
      "Epoch: 60, loss: 559.24, accuracy: 71.2%\n",
      "Epoch: 70, loss: 548.38, accuracy: 72.5%\n",
      "Epoch: 80, loss: 538.92, accuracy: 72.8%\n",
      "Epoch: 90, loss: 530.90, accuracy: 72.3%\n",
      "Epoch: 100, loss: 524.29, accuracy: 72.3%\n",
      "Epoch: 110, loss: 518.97, accuracy: 72.5%\n",
      "Epoch: 120, loss: 514.90, accuracy: 72.8%\n",
      "Epoch: 130, loss: 512.09, accuracy: 73.5%\n",
      "Epoch: 140, loss: 510.49, accuracy: 73.7%\n",
      "Epoch: 150, loss: 510.06, accuracy: 74.0%\n",
      "Epoch: 160, loss: 510.87, accuracy: 74.2%\n",
      "Epoch: 170, loss: 512.95, accuracy: 73.7%\n",
      "Epoch: 180, loss: 516.22, accuracy: 73.9%\n",
      "Epoch: 190, loss: 520.50, accuracy: 73.9%\n",
      "Epoch: 200, loss: 525.61, accuracy: 74.1%\n",
      "Epoch: 210, loss: 531.34, accuracy: 74.1%\n",
      "Epoch: 220, loss: 537.53, accuracy: 74.0%\n",
      "Epoch: 230, loss: 544.03, accuracy: 74.1%\n",
      "Epoch: 240, loss: 550.72, accuracy: 74.1%\n",
      "Epoch: 250, loss: 557.51, accuracy: 74.2%\n",
      "Epoch: 260, loss: 564.32, accuracy: 74.4%\n",
      "Epoch: 270, loss: 571.12, accuracy: 74.4%\n",
      "Epoch: 280, loss: 577.85, accuracy: 74.5%\n",
      "Epoch: 290, loss: 584.50, accuracy: 74.5%\n",
      "Epoch: 300, loss: 591.05, accuracy: 74.6%\n",
      "Mean accuracy on T3: 71.99\n",
      "Mean losses on T3: 556.08\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 693.74, accuracy: 58.6%\n",
      "Epoch: 20, loss: 634.55, accuracy: 68.6%\n",
      "Epoch: 30, loss: 587.06, accuracy: 67.2%\n",
      "Epoch: 40, loss: 562.05, accuracy: 70.4%\n",
      "Epoch: 50, loss: 541.53, accuracy: 72.4%\n",
      "Epoch: 60, loss: 524.30, accuracy: 73.6%\n",
      "Epoch: 70, loss: 510.14, accuracy: 74.7%\n",
      "Epoch: 80, loss: 498.29, accuracy: 75.1%\n",
      "Epoch: 90, loss: 487.76, accuracy: 75.5%\n",
      "Epoch: 100, loss: 477.98, accuracy: 75.9%\n",
      "Epoch: 110, loss: 468.93, accuracy: 76.2%\n",
      "Epoch: 120, loss: 460.83, accuracy: 76.6%\n",
      "Epoch: 130, loss: 453.76, accuracy: 76.8%\n",
      "Epoch: 140, loss: 447.71, accuracy: 77.6%\n",
      "Epoch: 150, loss: 442.67, accuracy: 77.1%\n",
      "Epoch: 160, loss: 438.75, accuracy: 77.1%\n",
      "Epoch: 170, loss: 436.03, accuracy: 77.9%\n",
      "Epoch: 180, loss: 434.50, accuracy: 77.9%\n",
      "Epoch: 190, loss: 434.06, accuracy: 77.6%\n",
      "Epoch: 200, loss: 434.56, accuracy: 76.7%\n",
      "Epoch: 210, loss: 435.83, accuracy: 76.2%\n",
      "Epoch: 220, loss: 437.73, accuracy: 76.1%\n",
      "Epoch: 230, loss: 440.12, accuracy: 76.2%\n",
      "Epoch: 240, loss: 442.89, accuracy: 75.9%\n",
      "Epoch: 250, loss: 445.96, accuracy: 76.1%\n",
      "Epoch: 260, loss: 449.26, accuracy: 76.1%\n",
      "Epoch: 270, loss: 452.73, accuracy: 76.2%\n",
      "Epoch: 280, loss: 456.32, accuracy: 76.1%\n",
      "Epoch: 290, loss: 459.99, accuracy: 75.9%\n",
      "Epoch: 300, loss: 463.71, accuracy: 75.7%\n",
      "Mean accuracy on T4: 74.86\n",
      "Mean losses on T4: 487.20\n",
      "\n",
      "Accuracy list on Ti sets: [72.64677325156957, 70.8691414805081, 73.0081763761133, 71.98956051978391, 74.86092836257309]\n",
      "Losses list on Ti sets: [539.8651683996003, 578.8794003263583, 550.5743857631852, 556.0766579132229, 487.199093964241]\n",
      "Mean accuracy on all T sets: 72.67\n",
      "Mean losses on all T sets: 542.52\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ piÄ…ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: relu\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 581.78, accuracy: 71.1%\n",
      "Epoch: 20, loss: 523.13, accuracy: 72.9%\n",
      "Epoch: 30, loss: 491.19, accuracy: 74.9%\n",
      "Epoch: 40, loss: 468.09, accuracy: 76.1%\n",
      "Epoch: 50, loss: 465.77, accuracy: 75.7%\n",
      "Epoch: 60, loss: 476.26, accuracy: 75.7%\n",
      "Epoch: 70, loss: 487.60, accuracy: 74.9%\n",
      "Epoch: 80, loss: 505.39, accuracy: 70.7%\n",
      "Epoch: 90, loss: 515.00, accuracy: 69.9%\n",
      "Epoch: 100, loss: 526.87, accuracy: 70.0%\n",
      "Epoch: 110, loss: 533.49, accuracy: 70.2%\n",
      "Epoch: 120, loss: 539.72, accuracy: 69.8%\n",
      "Epoch: 130, loss: 545.76, accuracy: 69.8%\n",
      "Epoch: 140, loss: 551.26, accuracy: 69.4%\n",
      "Epoch: 150, loss: 556.07, accuracy: 69.4%\n",
      "Epoch: 160, loss: 560.86, accuracy: 69.4%\n",
      "Epoch: 170, loss: 565.80, accuracy: 69.6%\n",
      "Epoch: 180, loss: 570.34, accuracy: 73.9%\n",
      "Epoch: 190, loss: 574.80, accuracy: 74.1%\n",
      "Epoch: 200, loss: 578.43, accuracy: 74.2%\n",
      "Epoch: 210, loss: 582.56, accuracy: 74.2%\n",
      "Epoch: 220, loss: 586.12, accuracy: 74.1%\n",
      "Epoch: 230, loss: 590.85, accuracy: 74.6%\n",
      "Epoch: 240, loss: 597.54, accuracy: 74.5%\n",
      "Epoch: 250, loss: 604.33, accuracy: 74.6%\n",
      "Epoch: 260, loss: 610.77, accuracy: 74.9%\n",
      "Epoch: 270, loss: 615.73, accuracy: 74.9%\n",
      "Epoch: 280, loss: 620.42, accuracy: 74.9%\n",
      "Epoch: 290, loss: 624.28, accuracy: 74.8%\n",
      "Epoch: 300, loss: 629.13, accuracy: 74.9%\n",
      "Mean accuracy on T0: 73.42\n",
      "Mean losses on T0: 592.08\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 590.39, accuracy: 69.6%\n",
      "Epoch: 20, loss: 538.56, accuracy: 72.1%\n",
      "Epoch: 30, loss: 509.81, accuracy: 72.5%\n",
      "Epoch: 40, loss: 489.55, accuracy: 74.8%\n",
      "Epoch: 50, loss: 495.11, accuracy: 74.1%\n",
      "Epoch: 60, loss: 513.63, accuracy: 69.9%\n",
      "Epoch: 70, loss: 531.77, accuracy: 69.9%\n",
      "Epoch: 80, loss: 552.61, accuracy: 69.6%\n",
      "Epoch: 90, loss: 570.88, accuracy: 69.6%\n",
      "Epoch: 100, loss: 574.29, accuracy: 69.5%\n",
      "Epoch: 110, loss: 582.46, accuracy: 69.4%\n",
      "Epoch: 120, loss: 578.93, accuracy: 69.6%\n",
      "Epoch: 130, loss: 583.51, accuracy: 69.4%\n",
      "Epoch: 140, loss: 588.63, accuracy: 73.2%\n",
      "Epoch: 150, loss: 594.73, accuracy: 73.3%\n",
      "Epoch: 160, loss: 601.64, accuracy: 73.1%\n",
      "Epoch: 170, loss: 607.75, accuracy: 72.9%\n",
      "Epoch: 180, loss: 613.82, accuracy: 72.9%\n",
      "Epoch: 190, loss: 619.09, accuracy: 72.9%\n",
      "Epoch: 200, loss: 623.64, accuracy: 72.8%\n",
      "Epoch: 210, loss: 627.62, accuracy: 72.9%\n",
      "Epoch: 220, loss: 631.40, accuracy: 73.1%\n",
      "Epoch: 230, loss: 635.04, accuracy: 73.1%\n",
      "Epoch: 240, loss: 638.50, accuracy: 73.1%\n",
      "Epoch: 250, loss: 642.09, accuracy: 73.1%\n",
      "Epoch: 260, loss: 645.57, accuracy: 72.9%\n",
      "Epoch: 270, loss: 648.82, accuracy: 72.9%\n",
      "Epoch: 280, loss: 652.13, accuracy: 72.8%\n",
      "Epoch: 290, loss: 655.29, accuracy: 72.9%\n",
      "Epoch: 300, loss: 658.38, accuracy: 72.9%\n",
      "Mean accuracy on T1: 73.06\n",
      "Mean losses on T1: 631.53\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 614.42, accuracy: 67.5%\n",
      "Epoch: 20, loss: 565.98, accuracy: 70.0%\n",
      "Epoch: 30, loss: 536.38, accuracy: 71.0%\n",
      "Epoch: 40, loss: 518.09, accuracy: 71.9%\n",
      "Epoch: 50, loss: 517.87, accuracy: 72.7%\n",
      "Epoch: 60, loss: 527.75, accuracy: 72.3%\n",
      "Epoch: 70, loss: 542.94, accuracy: 71.5%\n",
      "Epoch: 80, loss: 561.38, accuracy: 71.7%\n",
      "Epoch: 90, loss: 577.74, accuracy: 71.7%\n",
      "Epoch: 100, loss: 593.66, accuracy: 71.9%\n",
      "Epoch: 110, loss: 607.77, accuracy: 72.7%\n",
      "Epoch: 120, loss: 620.85, accuracy: 72.9%\n",
      "Epoch: 130, loss: 632.58, accuracy: 73.1%\n",
      "Epoch: 140, loss: 643.73, accuracy: 72.9%\n",
      "Epoch: 150, loss: 655.16, accuracy: 72.8%\n",
      "Epoch: 160, loss: 667.21, accuracy: 72.8%\n",
      "Epoch: 170, loss: 677.92, accuracy: 72.8%\n",
      "Epoch: 180, loss: 686.65, accuracy: 72.9%\n",
      "Epoch: 190, loss: 695.28, accuracy: 72.7%\n",
      "Epoch: 200, loss: 704.80, accuracy: 72.9%\n",
      "Epoch: 210, loss: 714.65, accuracy: 72.8%\n",
      "Epoch: 220, loss: 723.63, accuracy: 72.8%\n",
      "Epoch: 230, loss: 731.38, accuracy: 72.8%\n",
      "Epoch: 240, loss: 738.43, accuracy: 72.7%\n",
      "Epoch: 250, loss: 745.32, accuracy: 72.8%\n",
      "Epoch: 260, loss: 749.22, accuracy: 72.8%\n",
      "Epoch: 270, loss: 756.59, accuracy: 72.8%\n",
      "Epoch: 280, loss: 762.23, accuracy: 72.9%\n",
      "Epoch: 290, loss: 767.42, accuracy: 72.9%\n",
      "Epoch: 300, loss: 772.14, accuracy: 72.9%\n",
      "Mean accuracy on T2: 71.66\n",
      "Mean losses on T2: 679.07\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 566.05, accuracy: 69.3%\n",
      "Epoch: 20, loss: 513.92, accuracy: 72.7%\n",
      "Epoch: 30, loss: 482.71, accuracy: 73.3%\n",
      "Epoch: 40, loss: 456.06, accuracy: 75.8%\n",
      "Epoch: 50, loss: 455.96, accuracy: 75.6%\n",
      "Epoch: 60, loss: 455.94, accuracy: 77.5%\n",
      "Epoch: 70, loss: 471.57, accuracy: 77.0%\n",
      "Epoch: 80, loss: 483.03, accuracy: 77.3%\n",
      "Epoch: 90, loss: 495.21, accuracy: 77.3%\n",
      "Epoch: 100, loss: 506.83, accuracy: 77.0%\n",
      "Epoch: 110, loss: 517.41, accuracy: 76.6%\n",
      "Epoch: 120, loss: 527.33, accuracy: 76.5%\n",
      "Epoch: 130, loss: 535.26, accuracy: 76.3%\n",
      "Epoch: 140, loss: 543.73, accuracy: 76.3%\n",
      "Epoch: 150, loss: 551.40, accuracy: 76.3%\n",
      "Epoch: 160, loss: 558.23, accuracy: 76.3%\n",
      "Epoch: 170, loss: 565.13, accuracy: 76.5%\n",
      "Epoch: 180, loss: 570.99, accuracy: 76.5%\n",
      "Epoch: 190, loss: 576.44, accuracy: 76.7%\n",
      "Epoch: 200, loss: 581.33, accuracy: 76.6%\n",
      "Epoch: 210, loss: 585.98, accuracy: 76.9%\n",
      "Epoch: 220, loss: 590.39, accuracy: 76.9%\n",
      "Epoch: 230, loss: 594.42, accuracy: 76.9%\n",
      "Epoch: 240, loss: 597.61, accuracy: 77.0%\n",
      "Epoch: 250, loss: 601.27, accuracy: 76.9%\n",
      "Epoch: 260, loss: 604.44, accuracy: 76.9%\n",
      "Epoch: 270, loss: 607.39, accuracy: 76.9%\n",
      "Epoch: 280, loss: 610.18, accuracy: 76.9%\n",
      "Epoch: 290, loss: 612.85, accuracy: 76.7%\n",
      "Epoch: 300, loss: 615.51, accuracy: 77.1%\n",
      "Mean accuracy on T3: 75.60\n",
      "Mean losses on T3: 565.26\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 572.13, accuracy: 70.0%\n",
      "Epoch: 20, loss: 513.00, accuracy: 74.2%\n",
      "Epoch: 30, loss: 544.18, accuracy: 74.5%\n",
      "Epoch: 40, loss: 508.10, accuracy: 76.3%\n",
      "Epoch: 50, loss: 459.96, accuracy: 76.7%\n",
      "Epoch: 60, loss: 480.04, accuracy: 78.2%\n",
      "Epoch: 70, loss: 504.70, accuracy: 77.5%\n",
      "Epoch: 80, loss: 526.30, accuracy: 77.2%\n",
      "Epoch: 90, loss: 543.96, accuracy: 77.0%\n",
      "Epoch: 100, loss: 559.76, accuracy: 76.7%\n",
      "Epoch: 110, loss: 573.43, accuracy: 77.2%\n",
      "Epoch: 120, loss: 586.39, accuracy: 77.2%\n",
      "Epoch: 130, loss: 597.55, accuracy: 77.2%\n",
      "Epoch: 140, loss: 607.37, accuracy: 77.2%\n",
      "Epoch: 150, loss: 616.47, accuracy: 77.0%\n",
      "Epoch: 160, loss: 625.68, accuracy: 76.8%\n",
      "Epoch: 170, loss: 633.02, accuracy: 77.0%\n",
      "Epoch: 180, loss: 639.11, accuracy: 76.7%\n",
      "Epoch: 190, loss: 645.94, accuracy: 76.7%\n",
      "Epoch: 200, loss: 651.77, accuracy: 76.8%\n",
      "Epoch: 210, loss: 656.52, accuracy: 76.7%\n",
      "Epoch: 220, loss: 660.58, accuracy: 76.7%\n",
      "Epoch: 230, loss: 665.66, accuracy: 76.7%\n",
      "Epoch: 240, loss: 670.27, accuracy: 76.7%\n",
      "Epoch: 250, loss: 674.26, accuracy: 76.7%\n",
      "Epoch: 260, loss: 678.06, accuracy: 76.6%\n",
      "Epoch: 270, loss: 681.92, accuracy: 76.6%\n",
      "Epoch: 280, loss: 685.20, accuracy: 76.6%\n",
      "Epoch: 290, loss: 688.89, accuracy: 76.6%\n",
      "Epoch: 300, loss: 691.77, accuracy: 76.6%\n",
      "Mean accuracy on T4: 75.54\n",
      "Mean losses on T4: 618.43\n",
      "\n",
      "Accuracy list on Ti sets: [73.41827274054607, 73.06278288801286, 71.65578916630166, 75.60089064096948, 75.54470029239766]\n",
      "Losses list on Ti sets: [592.0811825315517, 631.5315759991746, 679.0667127055887, 565.2644153407579, 618.4321337868021]\n",
      "Mean accuracy on all T sets: 73.86\n",
      "Mean losses on all T sets: 617.28\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ szÃ³sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: sigmoid\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 726.61, accuracy: 33.2%\n",
      "Epoch: 20, loss: 664.17, accuracy: 59.8%\n",
      "Epoch: 30, loss: 620.82, accuracy: 68.5%\n",
      "Epoch: 40, loss: 604.37, accuracy: 69.8%\n",
      "Epoch: 50, loss: 589.89, accuracy: 71.1%\n",
      "Epoch: 60, loss: 577.46, accuracy: 70.8%\n",
      "Epoch: 70, loss: 567.34, accuracy: 71.2%\n",
      "Epoch: 80, loss: 558.82, accuracy: 71.7%\n",
      "Epoch: 90, loss: 551.34, accuracy: 72.7%\n",
      "Epoch: 100, loss: 544.73, accuracy: 72.7%\n",
      "Epoch: 110, loss: 538.84, accuracy: 72.9%\n",
      "Epoch: 120, loss: 533.82, accuracy: 73.2%\n",
      "Epoch: 130, loss: 530.07, accuracy: 72.5%\n",
      "Epoch: 140, loss: 527.97, accuracy: 72.5%\n",
      "Epoch: 150, loss: 527.64, accuracy: 73.3%\n",
      "Epoch: 160, loss: 529.06, accuracy: 73.6%\n",
      "Epoch: 170, loss: 532.07, accuracy: 73.3%\n",
      "Epoch: 180, loss: 536.44, accuracy: 73.3%\n",
      "Epoch: 190, loss: 541.94, accuracy: 73.7%\n",
      "Epoch: 200, loss: 548.30, accuracy: 73.1%\n",
      "Epoch: 210, loss: 555.32, accuracy: 72.7%\n",
      "Epoch: 220, loss: 562.79, accuracy: 72.9%\n",
      "Epoch: 230, loss: 570.54, accuracy: 72.7%\n",
      "Epoch: 240, loss: 578.43, accuracy: 72.9%\n",
      "Epoch: 250, loss: 586.34, accuracy: 72.8%\n",
      "Epoch: 260, loss: 594.19, accuracy: 72.7%\n",
      "Epoch: 270, loss: 601.90, accuracy: 72.5%\n",
      "Epoch: 280, loss: 609.41, accuracy: 72.5%\n",
      "Epoch: 290, loss: 616.69, accuracy: 72.4%\n",
      "Epoch: 300, loss: 623.70, accuracy: 72.5%\n",
      "Mean accuracy on T0: 70.76\n",
      "Mean losses on T0: 587.07\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 751.01, accuracy: 32.3%\n",
      "Epoch: 20, loss: 677.81, accuracy: 57.2%\n",
      "Epoch: 30, loss: 601.57, accuracy: 69.4%\n",
      "Epoch: 40, loss: 579.26, accuracy: 71.0%\n",
      "Epoch: 50, loss: 561.99, accuracy: 72.0%\n",
      "Epoch: 60, loss: 547.73, accuracy: 72.8%\n",
      "Epoch: 70, loss: 535.54, accuracy: 73.1%\n",
      "Epoch: 80, loss: 524.42, accuracy: 73.7%\n",
      "Epoch: 90, loss: 514.06, accuracy: 73.7%\n",
      "Epoch: 100, loss: 504.82, accuracy: 74.1%\n",
      "Epoch: 110, loss: 497.04, accuracy: 74.5%\n",
      "Epoch: 120, loss: 490.78, accuracy: 74.1%\n",
      "Epoch: 130, loss: 485.90, accuracy: 74.5%\n",
      "Epoch: 140, loss: 482.34, accuracy: 74.9%\n",
      "Epoch: 150, loss: 480.33, accuracy: 75.2%\n",
      "Epoch: 160, loss: 480.07, accuracy: 75.2%\n",
      "Epoch: 170, loss: 481.47, accuracy: 75.7%\n",
      "Epoch: 180, loss: 484.31, accuracy: 75.6%\n",
      "Epoch: 190, loss: 488.32, accuracy: 76.0%\n",
      "Epoch: 200, loss: 493.26, accuracy: 75.7%\n",
      "Epoch: 210, loss: 498.92, accuracy: 75.6%\n",
      "Epoch: 220, loss: 505.10, accuracy: 75.3%\n",
      "Epoch: 230, loss: 511.64, accuracy: 74.9%\n",
      "Epoch: 240, loss: 518.39, accuracy: 74.8%\n",
      "Epoch: 250, loss: 525.19, accuracy: 74.8%\n",
      "Epoch: 260, loss: 531.95, accuracy: 74.4%\n",
      "Epoch: 270, loss: 538.59, accuracy: 74.6%\n",
      "Epoch: 280, loss: 545.09, accuracy: 74.8%\n",
      "Epoch: 290, loss: 551.43, accuracy: 75.0%\n",
      "Epoch: 300, loss: 557.62, accuracy: 75.3%\n",
      "Mean accuracy on T1: 72.44\n",
      "Mean losses on T1: 541.70\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 742.66, accuracy: 31.4%\n",
      "Epoch: 20, loss: 662.46, accuracy: 57.8%\n",
      "Epoch: 30, loss: 593.31, accuracy: 68.9%\n",
      "Epoch: 40, loss: 572.47, accuracy: 70.0%\n",
      "Epoch: 50, loss: 555.10, accuracy: 71.7%\n",
      "Epoch: 60, loss: 540.67, accuracy: 72.5%\n",
      "Epoch: 70, loss: 528.24, accuracy: 73.6%\n",
      "Epoch: 80, loss: 516.86, accuracy: 74.5%\n",
      "Epoch: 90, loss: 506.17, accuracy: 74.6%\n",
      "Epoch: 100, loss: 496.10, accuracy: 74.0%\n",
      "Epoch: 110, loss: 486.83, accuracy: 75.0%\n",
      "Epoch: 120, loss: 478.92, accuracy: 75.7%\n",
      "Epoch: 130, loss: 472.80, accuracy: 75.4%\n",
      "Epoch: 140, loss: 468.57, accuracy: 76.5%\n",
      "Epoch: 150, loss: 466.20, accuracy: 76.6%\n",
      "Epoch: 160, loss: 465.63, accuracy: 76.6%\n",
      "Epoch: 170, loss: 466.71, accuracy: 77.4%\n",
      "Epoch: 180, loss: 469.23, accuracy: 77.8%\n",
      "Epoch: 190, loss: 472.96, accuracy: 77.9%\n",
      "Epoch: 200, loss: 477.67, accuracy: 77.8%\n",
      "Epoch: 210, loss: 483.15, accuracy: 77.7%\n",
      "Epoch: 220, loss: 489.21, accuracy: 77.7%\n",
      "Epoch: 230, loss: 495.69, accuracy: 77.5%\n",
      "Epoch: 240, loss: 502.46, accuracy: 77.4%\n",
      "Epoch: 250, loss: 509.41, accuracy: 77.4%\n",
      "Epoch: 260, loss: 516.44, accuracy: 77.3%\n",
      "Epoch: 270, loss: 523.47, accuracy: 77.0%\n",
      "Epoch: 280, loss: 530.44, accuracy: 76.9%\n",
      "Epoch: 290, loss: 537.31, accuracy: 76.9%\n",
      "Epoch: 300, loss: 544.05, accuracy: 76.9%\n",
      "Mean accuracy on T2: 73.40\n",
      "Mean losses on T2: 528.62\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 723.75, accuracy: 33.5%\n",
      "Epoch: 20, loss: 638.91, accuracy: 60.2%\n",
      "Epoch: 30, loss: 582.93, accuracy: 68.9%\n",
      "Epoch: 40, loss: 566.44, accuracy: 69.9%\n",
      "Epoch: 50, loss: 554.43, accuracy: 71.5%\n",
      "Epoch: 60, loss: 544.96, accuracy: 72.1%\n",
      "Epoch: 70, loss: 536.59, accuracy: 72.4%\n",
      "Epoch: 80, loss: 528.87, accuracy: 72.9%\n",
      "Epoch: 90, loss: 521.74, accuracy: 73.2%\n",
      "Epoch: 100, loss: 514.97, accuracy: 73.6%\n",
      "Epoch: 110, loss: 508.72, accuracy: 74.4%\n",
      "Epoch: 120, loss: 503.77, accuracy: 74.4%\n",
      "Epoch: 130, loss: 500.99, accuracy: 75.0%\n",
      "Epoch: 140, loss: 500.68, accuracy: 74.8%\n",
      "Epoch: 150, loss: 502.64, accuracy: 74.9%\n",
      "Epoch: 160, loss: 506.43, accuracy: 75.3%\n",
      "Epoch: 170, loss: 511.57, accuracy: 75.4%\n",
      "Epoch: 180, loss: 517.64, accuracy: 75.6%\n",
      "Epoch: 190, loss: 524.31, accuracy: 75.7%\n",
      "Epoch: 200, loss: 531.37, accuracy: 75.6%\n",
      "Epoch: 210, loss: 538.64, accuracy: 75.4%\n",
      "Epoch: 220, loss: 546.01, accuracy: 75.8%\n",
      "Epoch: 230, loss: 553.39, accuracy: 76.0%\n",
      "Epoch: 240, loss: 560.70, accuracy: 76.0%\n",
      "Epoch: 250, loss: 567.90, accuracy: 76.0%\n",
      "Epoch: 260, loss: 574.89, accuracy: 75.6%\n",
      "Epoch: 270, loss: 581.52, accuracy: 75.2%\n",
      "Epoch: 280, loss: 587.78, accuracy: 75.2%\n",
      "Epoch: 290, loss: 593.71, accuracy: 75.0%\n",
      "Epoch: 300, loss: 599.37, accuracy: 75.2%\n",
      "Mean accuracy on T3: 72.61\n",
      "Mean losses on T3: 560.96\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 769.55, accuracy: 32.0%\n",
      "Epoch: 20, loss: 635.30, accuracy: 68.0%\n",
      "Epoch: 30, loss: 594.96, accuracy: 68.8%\n",
      "Epoch: 40, loss: 579.99, accuracy: 70.0%\n",
      "Epoch: 50, loss: 566.71, accuracy: 70.0%\n",
      "Epoch: 60, loss: 553.69, accuracy: 70.4%\n",
      "Epoch: 70, loss: 541.55, accuracy: 70.5%\n",
      "Epoch: 80, loss: 530.74, accuracy: 71.4%\n",
      "Epoch: 90, loss: 521.28, accuracy: 71.8%\n",
      "Epoch: 100, loss: 512.93, accuracy: 72.0%\n",
      "Epoch: 110, loss: 505.77, accuracy: 72.2%\n",
      "Epoch: 120, loss: 500.25, accuracy: 72.6%\n",
      "Epoch: 130, loss: 496.51, accuracy: 73.2%\n",
      "Epoch: 140, loss: 494.34, accuracy: 72.6%\n",
      "Epoch: 150, loss: 493.58, accuracy: 72.4%\n",
      "Epoch: 160, loss: 494.15, accuracy: 72.9%\n",
      "Epoch: 170, loss: 495.95, accuracy: 72.9%\n",
      "Epoch: 180, loss: 498.84, accuracy: 73.2%\n",
      "Epoch: 190, loss: 502.61, accuracy: 73.2%\n",
      "Epoch: 200, loss: 507.05, accuracy: 73.0%\n",
      "Epoch: 210, loss: 511.98, accuracy: 73.3%\n",
      "Epoch: 220, loss: 517.24, accuracy: 73.7%\n",
      "Epoch: 230, loss: 522.70, accuracy: 74.3%\n",
      "Epoch: 240, loss: 528.27, accuracy: 74.3%\n",
      "Epoch: 250, loss: 533.87, accuracy: 74.3%\n",
      "Epoch: 260, loss: 539.46, accuracy: 74.6%\n",
      "Epoch: 270, loss: 545.00, accuracy: 74.5%\n",
      "Epoch: 280, loss: 550.47, accuracy: 74.6%\n",
      "Epoch: 290, loss: 555.86, accuracy: 74.7%\n",
      "Epoch: 300, loss: 561.15, accuracy: 74.7%\n",
      "Mean accuracy on T4: 71.33\n",
      "Mean losses on T4: 549.01\n",
      "\n",
      "Accuracy list on Ti sets: [70.75711782742006, 72.44382391590013, 73.40403708570595, 72.61454226894436, 71.32796052631579]\n",
      "Losses list on Ti sets: [587.0730626846956, 541.7037174512859, 528.6214058962632, 560.9585344866902, 549.0087578418496]\n",
      "Mean accuracy on all T sets: 72.11\n",
      "Mean losses on all T sets: 553.47\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ siÃ³dma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 1000, activation: sigmoid\n",
      "2. Layer - input_dim: 1000, output_dim: 100, activation: sigmoid\n",
      "3. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=1000, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=1000, output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 728.78, accuracy: 56.9%\n",
      "Epoch: 20, loss: 727.86, accuracy: 56.9%\n",
      "Epoch: 30, loss: 727.34, accuracy: 56.9%\n",
      "Epoch: 40, loss: 726.89, accuracy: 56.9%\n",
      "Epoch: 50, loss: 726.40, accuracy: 56.9%\n",
      "Epoch: 60, loss: 725.79, accuracy: 56.9%\n",
      "Epoch: 70, loss: 724.97, accuracy: 56.9%\n",
      "Epoch: 80, loss: 723.75, accuracy: 56.9%\n",
      "Epoch: 90, loss: 721.67, accuracy: 56.9%\n",
      "Epoch: 100, loss: 716.59, accuracy: 56.9%\n",
      "Epoch: 110, loss: 695.83, accuracy: 56.9%\n",
      "Epoch: 120, loss: 668.92, accuracy: 58.0%\n",
      "Epoch: 130, loss: 645.97, accuracy: 64.1%\n",
      "Epoch: 140, loss: 632.71, accuracy: 66.8%\n",
      "Epoch: 150, loss: 625.59, accuracy: 66.8%\n",
      "Epoch: 160, loss: 620.42, accuracy: 67.4%\n",
      "Epoch: 170, loss: 615.21, accuracy: 67.9%\n",
      "Epoch: 180, loss: 609.87, accuracy: 68.9%\n",
      "Epoch: 190, loss: 604.45, accuracy: 69.0%\n",
      "Epoch: 200, loss: 599.01, accuracy: 68.9%\n",
      "Epoch: 210, loss: 593.56, accuracy: 69.1%\n",
      "Epoch: 220, loss: 588.15, accuracy: 69.1%\n",
      "Epoch: 230, loss: 582.85, accuracy: 69.3%\n",
      "Epoch: 240, loss: 577.73, accuracy: 69.9%\n",
      "Epoch: 250, loss: 572.82, accuracy: 70.0%\n",
      "Epoch: 260, loss: 593.63, accuracy: 69.3%\n",
      "Epoch: 270, loss: 591.50, accuracy: 69.1%\n",
      "Epoch: 280, loss: 591.37, accuracy: 68.7%\n",
      "Epoch: 290, loss: 592.08, accuracy: 68.7%\n",
      "Epoch: 300, loss: 593.73, accuracy: 68.5%\n",
      "Mean accuracy on T0: 63.05\n",
      "Mean losses on T0: 663.71\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 694.55, accuracy: 58.1%\n",
      "Epoch: 20, loss: 693.70, accuracy: 58.1%\n",
      "Epoch: 30, loss: 693.12, accuracy: 58.1%\n",
      "Epoch: 40, loss: 692.55, accuracy: 58.1%\n",
      "Epoch: 50, loss: 691.93, accuracy: 58.1%\n",
      "Epoch: 60, loss: 691.20, accuracy: 58.1%\n",
      "Epoch: 70, loss: 690.23, accuracy: 58.1%\n",
      "Epoch: 80, loss: 688.80, accuracy: 58.1%\n",
      "Epoch: 90, loss: 686.48, accuracy: 58.1%\n",
      "Epoch: 100, loss: 682.36, accuracy: 58.1%\n",
      "Epoch: 110, loss: 674.30, accuracy: 58.1%\n",
      "Epoch: 120, loss: 657.78, accuracy: 58.2%\n",
      "Epoch: 130, loss: 628.26, accuracy: 66.0%\n",
      "Epoch: 140, loss: 607.55, accuracy: 69.0%\n",
      "Epoch: 150, loss: 593.64, accuracy: 69.3%\n",
      "Epoch: 160, loss: 584.47, accuracy: 69.5%\n",
      "Epoch: 170, loss: 577.60, accuracy: 69.6%\n",
      "Epoch: 180, loss: 571.53, accuracy: 69.9%\n",
      "Epoch: 190, loss: 566.13, accuracy: 70.7%\n",
      "Epoch: 200, loss: 561.49, accuracy: 71.4%\n",
      "Epoch: 210, loss: 557.60, accuracy: 71.5%\n",
      "Epoch: 220, loss: 554.42, accuracy: 71.0%\n",
      "Epoch: 230, loss: 551.82, accuracy: 71.1%\n",
      "Epoch: 240, loss: 549.72, accuracy: 71.1%\n",
      "Epoch: 250, loss: 548.03, accuracy: 71.5%\n",
      "Epoch: 260, loss: 546.68, accuracy: 71.5%\n",
      "Epoch: 270, loss: 545.68, accuracy: 71.9%\n",
      "Epoch: 280, loss: 545.00, accuracy: 71.9%\n",
      "Epoch: 290, loss: 544.65, accuracy: 72.1%\n",
      "Epoch: 300, loss: 544.60, accuracy: 72.3%\n",
      "Mean accuracy on T1: 64.57\n",
      "Mean losses on T1: 629.11\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 692.38, accuracy: 57.3%\n",
      "Epoch: 20, loss: 691.77, accuracy: 57.3%\n",
      "Epoch: 30, loss: 691.30, accuracy: 57.3%\n",
      "Epoch: 40, loss: 690.83, accuracy: 57.3%\n",
      "Epoch: 50, loss: 690.30, accuracy: 57.3%\n",
      "Epoch: 60, loss: 689.62, accuracy: 57.3%\n",
      "Epoch: 70, loss: 688.64, accuracy: 57.3%\n",
      "Epoch: 80, loss: 687.13, accuracy: 57.3%\n",
      "Epoch: 90, loss: 684.59, accuracy: 57.3%\n",
      "Epoch: 100, loss: 680.04, accuracy: 57.3%\n",
      "Epoch: 110, loss: 671.11, accuracy: 57.3%\n",
      "Epoch: 120, loss: 650.27, accuracy: 58.1%\n",
      "Epoch: 130, loss: 619.11, accuracy: 63.9%\n",
      "Epoch: 140, loss: 600.96, accuracy: 69.1%\n",
      "Epoch: 150, loss: 589.45, accuracy: 69.4%\n",
      "Epoch: 160, loss: 582.28, accuracy: 69.5%\n",
      "Epoch: 170, loss: 577.03, accuracy: 70.3%\n",
      "Epoch: 180, loss: 572.45, accuracy: 71.1%\n",
      "Epoch: 190, loss: 568.42, accuracy: 71.9%\n",
      "Epoch: 200, loss: 564.88, accuracy: 71.1%\n",
      "Epoch: 210, loss: 561.75, accuracy: 71.2%\n",
      "Epoch: 220, loss: 558.92, accuracy: 71.6%\n",
      "Epoch: 230, loss: 556.35, accuracy: 71.9%\n",
      "Epoch: 240, loss: 554.04, accuracy: 72.0%\n",
      "Epoch: 250, loss: 552.02, accuracy: 72.1%\n",
      "Epoch: 260, loss: 550.38, accuracy: 72.3%\n",
      "Epoch: 270, loss: 549.20, accuracy: 72.3%\n",
      "Epoch: 280, loss: 548.61, accuracy: 72.3%\n",
      "Epoch: 290, loss: 548.72, accuracy: 72.5%\n",
      "Epoch: 300, loss: 549.61, accuracy: 72.9%\n",
      "Mean accuracy on T2: 64.26\n",
      "Mean losses on T2: 628.74\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 716.36, accuracy: 58.1%\n",
      "Epoch: 20, loss: 715.75, accuracy: 58.1%\n",
      "Epoch: 30, loss: 715.25, accuracy: 58.1%\n",
      "Epoch: 40, loss: 714.72, accuracy: 58.1%\n",
      "Epoch: 50, loss: 714.11, accuracy: 58.1%\n",
      "Epoch: 60, loss: 713.37, accuracy: 58.1%\n",
      "Epoch: 70, loss: 712.42, accuracy: 58.1%\n",
      "Epoch: 80, loss: 711.11, accuracy: 58.1%\n",
      "Epoch: 90, loss: 709.16, accuracy: 58.1%\n",
      "Epoch: 100, loss: 705.99, accuracy: 58.1%\n",
      "Epoch: 110, loss: 700.18, accuracy: 58.1%\n",
      "Epoch: 120, loss: 685.85, accuracy: 58.1%\n",
      "Epoch: 130, loss: 650.85, accuracy: 58.9%\n",
      "Epoch: 140, loss: 628.62, accuracy: 66.5%\n",
      "Epoch: 150, loss: 614.72, accuracy: 66.8%\n",
      "Epoch: 160, loss: 607.64, accuracy: 66.8%\n",
      "Epoch: 170, loss: 603.01, accuracy: 66.5%\n",
      "Epoch: 180, loss: 598.26, accuracy: 67.1%\n",
      "Epoch: 190, loss: 593.37, accuracy: 68.2%\n",
      "Epoch: 200, loss: 588.70, accuracy: 68.6%\n",
      "Epoch: 210, loss: 584.32, accuracy: 68.7%\n",
      "Epoch: 220, loss: 580.22, accuracy: 69.6%\n",
      "Epoch: 230, loss: 576.38, accuracy: 69.9%\n",
      "Epoch: 240, loss: 572.77, accuracy: 70.6%\n",
      "Epoch: 250, loss: 569.43, accuracy: 71.1%\n",
      "Epoch: 260, loss: 566.39, accuracy: 71.4%\n",
      "Epoch: 270, loss: 563.73, accuracy: 71.4%\n",
      "Epoch: 280, loss: 621.75, accuracy: 68.1%\n",
      "Epoch: 290, loss: 621.06, accuracy: 67.3%\n",
      "Epoch: 300, loss: 617.88, accuracy: 67.3%\n",
      "Mean accuracy on T3: 63.39\n",
      "Mean losses on T3: 652.33\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 722.31, accuracy: 53.8%\n",
      "Epoch: 20, loss: 722.52, accuracy: 53.8%\n",
      "Epoch: 30, loss: 722.57, accuracy: 53.8%\n",
      "Epoch: 40, loss: 722.43, accuracy: 53.8%\n",
      "Epoch: 50, loss: 722.07, accuracy: 53.8%\n",
      "Epoch: 60, loss: 721.42, accuracy: 53.8%\n",
      "Epoch: 70, loss: 720.29, accuracy: 53.8%\n",
      "Epoch: 80, loss: 718.33, accuracy: 53.8%\n",
      "Epoch: 90, loss: 714.84, accuracy: 53.8%\n",
      "Epoch: 100, loss: 708.44, accuracy: 53.8%\n",
      "Epoch: 110, loss: 696.44, accuracy: 53.8%\n",
      "Epoch: 120, loss: 676.28, accuracy: 56.4%\n",
      "Epoch: 130, loss: 652.96, accuracy: 65.7%\n",
      "Epoch: 140, loss: 639.77, accuracy: 66.4%\n",
      "Epoch: 150, loss: 631.63, accuracy: 67.0%\n",
      "Epoch: 160, loss: 624.84, accuracy: 67.4%\n",
      "Epoch: 170, loss: 620.04, accuracy: 68.7%\n",
      "Epoch: 180, loss: 616.69, accuracy: 69.7%\n",
      "Epoch: 190, loss: 614.18, accuracy: 68.9%\n",
      "Epoch: 200, loss: 612.12, accuracy: 68.8%\n",
      "Epoch: 210, loss: 610.21, accuracy: 68.7%\n",
      "Epoch: 220, loss: 608.22, accuracy: 68.6%\n",
      "Epoch: 230, loss: 605.96, accuracy: 68.9%\n",
      "Epoch: 240, loss: 603.33, accuracy: 69.5%\n",
      "Epoch: 250, loss: 600.34, accuracy: 70.0%\n",
      "Epoch: 260, loss: 597.16, accuracy: 70.1%\n",
      "Epoch: 270, loss: 594.11, accuracy: 70.4%\n",
      "Epoch: 280, loss: 591.59, accuracy: 70.7%\n",
      "Epoch: 290, loss: 589.90, accuracy: 70.9%\n",
      "Epoch: 300, loss: 552.26, accuracy: 70.8%\n",
      "Mean accuracy on T4: 61.81\n",
      "Mean losses on T4: 658.64\n",
      "\n",
      "Accuracy list on Ti sets: [63.05015330705211, 64.57015622718646, 64.26416265148197, 63.386078259599955, 61.81308479532163]\n",
      "Losses list on Ti sets: [663.7134064434073, 629.1109856456708, 628.7364711107316, 652.3329173838531, 658.6400601476553]\n",
      "Mean accuracy on all T sets: 63.42\n",
      "Mean losses on all T sets: 646.51\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
