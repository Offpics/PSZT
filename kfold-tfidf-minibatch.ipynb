{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pszt import net\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_tfidf.npy')\n",
    "y_train = np.load('y_tfidf.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ pierwsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: relu\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 720.01, accuracy: 56.4%\n",
      "Epoch: 20, loss: 719.97, accuracy: 56.4%\n",
      "Epoch: 30, loss: 718.74, accuracy: 56.4%\n",
      "Epoch: 40, loss: 709.43, accuracy: 56.4%\n",
      "Epoch: 50, loss: 662.38, accuracy: 61.2%\n",
      "Epoch: 60, loss: 586.84, accuracy: 72.1%\n",
      "Epoch: 70, loss: 549.64, accuracy: 71.9%\n",
      "Epoch: 80, loss: 535.91, accuracy: 71.7%\n",
      "Epoch: 90, loss: 532.59, accuracy: 72.3%\n",
      "Epoch: 100, loss: 531.88, accuracy: 72.1%\n",
      "Epoch: 110, loss: 534.59, accuracy: 71.9%\n",
      "Epoch: 120, loss: 525.10, accuracy: 73.2%\n",
      "Epoch: 130, loss: 529.45, accuracy: 72.1%\n",
      "Epoch: 140, loss: 535.41, accuracy: 72.5%\n",
      "Epoch: 150, loss: 528.47, accuracy: 73.5%\n",
      "Epoch: 160, loss: 526.22, accuracy: 73.7%\n",
      "Epoch: 170, loss: 532.49, accuracy: 73.5%\n",
      "Epoch: 180, loss: 544.07, accuracy: 73.7%\n",
      "Epoch: 190, loss: 554.38, accuracy: 73.5%\n",
      "Epoch: 200, loss: 564.17, accuracy: 73.3%\n",
      "Epoch: 210, loss: 573.66, accuracy: 73.2%\n",
      "Epoch: 220, loss: 582.75, accuracy: 73.3%\n",
      "Epoch: 230, loss: 591.46, accuracy: 73.1%\n",
      "Epoch: 240, loss: 599.81, accuracy: 73.1%\n",
      "Epoch: 250, loss: 607.80, accuracy: 72.9%\n",
      "Epoch: 260, loss: 615.44, accuracy: 73.1%\n",
      "Epoch: 270, loss: 622.71, accuracy: 73.1%\n",
      "Epoch: 280, loss: 629.65, accuracy: 73.1%\n",
      "Epoch: 290, loss: 636.24, accuracy: 73.2%\n",
      "Epoch: 300, loss: 642.51, accuracy: 73.2%\n",
      "Mean accuracy on T0: 70.27\n",
      "Mean losses on T0: 601.38\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 709.83, accuracy: 56.0%\n",
      "Epoch: 20, loss: 709.72, accuracy: 56.0%\n",
      "Epoch: 30, loss: 708.99, accuracy: 56.0%\n",
      "Epoch: 40, loss: 703.14, accuracy: 56.0%\n",
      "Epoch: 50, loss: 668.04, accuracy: 58.9%\n",
      "Epoch: 60, loss: 604.73, accuracy: 69.3%\n",
      "Epoch: 70, loss: 580.33, accuracy: 71.0%\n",
      "Epoch: 80, loss: 572.57, accuracy: 71.1%\n",
      "Epoch: 90, loss: 570.89, accuracy: 71.9%\n",
      "Epoch: 100, loss: 571.09, accuracy: 71.9%\n",
      "Epoch: 110, loss: 576.38, accuracy: 71.6%\n",
      "Epoch: 120, loss: 596.46, accuracy: 71.9%\n",
      "Epoch: 130, loss: 590.94, accuracy: 72.7%\n",
      "Epoch: 140, loss: 580.72, accuracy: 72.9%\n",
      "Epoch: 150, loss: 583.79, accuracy: 73.3%\n",
      "Epoch: 160, loss: 580.74, accuracy: 73.9%\n",
      "Epoch: 170, loss: 579.72, accuracy: 73.9%\n",
      "Epoch: 180, loss: 587.22, accuracy: 74.1%\n",
      "Epoch: 190, loss: 595.30, accuracy: 73.5%\n",
      "Epoch: 200, loss: 603.68, accuracy: 73.3%\n",
      "Epoch: 210, loss: 612.23, accuracy: 73.7%\n",
      "Epoch: 220, loss: 620.73, accuracy: 73.9%\n",
      "Epoch: 230, loss: 629.13, accuracy: 74.0%\n",
      "Epoch: 240, loss: 637.27, accuracy: 74.1%\n",
      "Epoch: 250, loss: 645.17, accuracy: 74.2%\n",
      "Epoch: 260, loss: 652.73, accuracy: 74.4%\n",
      "Epoch: 270, loss: 659.95, accuracy: 74.1%\n",
      "Epoch: 280, loss: 666.89, accuracy: 74.0%\n",
      "Epoch: 290, loss: 673.55, accuracy: 74.1%\n",
      "Epoch: 300, loss: 679.82, accuracy: 74.2%\n",
      "Mean accuracy on T1: 69.74\n",
      "Mean losses on T1: 633.80\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 710.55, accuracy: 57.3%\n",
      "Epoch: 20, loss: 710.50, accuracy: 57.3%\n",
      "Epoch: 30, loss: 709.53, accuracy: 57.3%\n",
      "Epoch: 40, loss: 702.08, accuracy: 57.3%\n",
      "Epoch: 50, loss: 662.89, accuracy: 59.8%\n",
      "Epoch: 60, loss: 594.31, accuracy: 70.8%\n",
      "Epoch: 70, loss: 560.92, accuracy: 71.2%\n",
      "Epoch: 80, loss: 547.87, accuracy: 71.2%\n",
      "Epoch: 90, loss: 541.42, accuracy: 72.3%\n",
      "Epoch: 100, loss: 535.31, accuracy: 72.4%\n",
      "Epoch: 110, loss: 632.63, accuracy: 67.3%\n",
      "Epoch: 120, loss: 610.37, accuracy: 69.5%\n",
      "Epoch: 130, loss: 594.66, accuracy: 69.9%\n",
      "Epoch: 140, loss: 576.58, accuracy: 71.5%\n",
      "Epoch: 150, loss: 556.81, accuracy: 73.2%\n",
      "Epoch: 160, loss: 544.65, accuracy: 73.9%\n",
      "Epoch: 170, loss: 543.57, accuracy: 75.4%\n",
      "Epoch: 180, loss: 547.84, accuracy: 75.0%\n",
      "Epoch: 190, loss: 540.08, accuracy: 75.6%\n",
      "Epoch: 200, loss: 539.02, accuracy: 75.8%\n",
      "Epoch: 210, loss: 545.05, accuracy: 75.8%\n",
      "Epoch: 220, loss: 552.21, accuracy: 75.7%\n",
      "Epoch: 230, loss: 557.61, accuracy: 76.1%\n",
      "Epoch: 240, loss: 561.52, accuracy: 75.7%\n",
      "Epoch: 250, loss: 567.73, accuracy: 75.8%\n",
      "Epoch: 260, loss: 573.83, accuracy: 75.8%\n",
      "Epoch: 270, loss: 579.87, accuracy: 75.7%\n",
      "Epoch: 280, loss: 585.72, accuracy: 75.7%\n",
      "Epoch: 290, loss: 591.19, accuracy: 75.7%\n",
      "Epoch: 300, loss: 596.67, accuracy: 75.6%\n",
      "Mean accuracy on T2: 70.95\n",
      "Mean losses on T2: 594.37\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 699.24, accuracy: 57.7%\n",
      "Epoch: 20, loss: 699.08, accuracy: 57.7%\n",
      "Epoch: 30, loss: 698.07, accuracy: 57.7%\n",
      "Epoch: 40, loss: 690.72, accuracy: 57.7%\n",
      "Epoch: 50, loss: 653.12, accuracy: 58.2%\n",
      "Epoch: 60, loss: 583.20, accuracy: 71.4%\n",
      "Epoch: 70, loss: 545.68, accuracy: 73.1%\n",
      "Epoch: 80, loss: 531.79, accuracy: 73.2%\n",
      "Epoch: 90, loss: 524.86, accuracy: 73.5%\n",
      "Epoch: 100, loss: 517.34, accuracy: 73.6%\n",
      "Epoch: 110, loss: 544.49, accuracy: 72.1%\n",
      "Epoch: 120, loss: 545.71, accuracy: 71.5%\n",
      "Epoch: 130, loss: 540.60, accuracy: 71.9%\n",
      "Epoch: 140, loss: 534.37, accuracy: 72.3%\n",
      "Epoch: 150, loss: 529.03, accuracy: 73.5%\n",
      "Epoch: 160, loss: 529.47, accuracy: 74.6%\n",
      "Epoch: 170, loss: 520.53, accuracy: 75.0%\n",
      "Epoch: 180, loss: 513.30, accuracy: 75.7%\n",
      "Epoch: 190, loss: 518.97, accuracy: 75.4%\n",
      "Epoch: 200, loss: 525.63, accuracy: 75.7%\n",
      "Epoch: 210, loss: 531.17, accuracy: 75.8%\n",
      "Epoch: 220, loss: 536.95, accuracy: 75.8%\n",
      "Epoch: 230, loss: 542.78, accuracy: 76.1%\n",
      "Epoch: 240, loss: 548.48, accuracy: 76.1%\n",
      "Epoch: 250, loss: 554.05, accuracy: 76.1%\n",
      "Epoch: 260, loss: 559.46, accuracy: 76.0%\n",
      "Epoch: 270, loss: 564.71, accuracy: 76.0%\n",
      "Epoch: 280, loss: 569.79, accuracy: 75.8%\n",
      "Epoch: 290, loss: 574.68, accuracy: 75.7%\n",
      "Epoch: 300, loss: 579.39, accuracy: 75.6%\n",
      "Mean accuracy on T3: 71.34\n",
      "Mean losses on T3: 575.07\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 699.11, accuracy: 56.8%\n",
      "Epoch: 20, loss: 698.87, accuracy: 56.8%\n",
      "Epoch: 30, loss: 697.65, accuracy: 56.8%\n",
      "Epoch: 40, loss: 688.98, accuracy: 56.8%\n",
      "Epoch: 50, loss: 644.89, accuracy: 59.1%\n",
      "Epoch: 60, loss: 568.89, accuracy: 72.4%\n",
      "Epoch: 70, loss: 528.64, accuracy: 74.1%\n",
      "Epoch: 80, loss: 511.86, accuracy: 74.9%\n",
      "Epoch: 90, loss: 505.83, accuracy: 74.7%\n",
      "Epoch: 100, loss: 508.24, accuracy: 74.1%\n",
      "Epoch: 110, loss: 544.38, accuracy: 73.0%\n",
      "Epoch: 120, loss: 491.14, accuracy: 76.3%\n",
      "Epoch: 130, loss: 484.23, accuracy: 76.6%\n",
      "Epoch: 140, loss: 478.46, accuracy: 76.4%\n",
      "Epoch: 150, loss: 474.49, accuracy: 76.7%\n",
      "Epoch: 160, loss: 478.46, accuracy: 76.1%\n",
      "Epoch: 170, loss: 476.32, accuracy: 77.1%\n",
      "Epoch: 180, loss: 476.89, accuracy: 78.6%\n",
      "Epoch: 190, loss: 481.94, accuracy: 79.2%\n",
      "Epoch: 200, loss: 487.56, accuracy: 78.9%\n",
      "Epoch: 210, loss: 493.75, accuracy: 78.9%\n",
      "Epoch: 220, loss: 500.06, accuracy: 78.8%\n",
      "Epoch: 230, loss: 506.33, accuracy: 78.7%\n",
      "Epoch: 240, loss: 512.46, accuracy: 78.7%\n",
      "Epoch: 250, loss: 518.32, accuracy: 78.6%\n",
      "Epoch: 260, loss: 523.91, accuracy: 78.6%\n",
      "Epoch: 270, loss: 529.22, accuracy: 78.6%\n",
      "Epoch: 280, loss: 534.26, accuracy: 78.4%\n",
      "Epoch: 290, loss: 539.03, accuracy: 78.4%\n",
      "Epoch: 300, loss: 543.55, accuracy: 78.3%\n",
      "Mean accuracy on T4: 72.54\n",
      "Mean losses on T4: 551.68\n",
      "\n",
      "Accuracy list on Ti sets: [70.27427361658636, 69.73601985691343, 70.9453204847423, 71.33844356840414, 72.54323830409358]\n",
      "Losses list on Ti sets: [601.3823366284164, 633.7964557646683, 594.369398203258, 575.0661533270893, 551.6833258109648]\n",
      "Mean accuracy on all T sets: 70.97\n",
      "Mean losses on all T sets: 591.26\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ druga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 20, activation: sigmoid\n",
      "2. Layer - input_dim: 20, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 719.39, accuracy: 58.1%\n",
      "Epoch: 20, loss: 719.31, accuracy: 58.1%\n",
      "Epoch: 30, loss: 719.22, accuracy: 58.1%\n",
      "Epoch: 40, loss: 719.12, accuracy: 58.1%\n",
      "Epoch: 50, loss: 719.01, accuracy: 58.1%\n",
      "Epoch: 60, loss: 718.88, accuracy: 58.1%\n",
      "Epoch: 70, loss: 718.71, accuracy: 58.1%\n",
      "Epoch: 80, loss: 718.50, accuracy: 58.1%\n",
      "Epoch: 90, loss: 718.21, accuracy: 58.1%\n",
      "Epoch: 100, loss: 717.83, accuracy: 58.1%\n",
      "Epoch: 110, loss: 717.28, accuracy: 58.1%\n",
      "Epoch: 120, loss: 716.48, accuracy: 58.1%\n",
      "Epoch: 130, loss: 715.27, accuracy: 58.1%\n",
      "Epoch: 140, loss: 713.40, accuracy: 58.1%\n",
      "Epoch: 150, loss: 710.50, accuracy: 58.1%\n",
      "Epoch: 160, loss: 705.98, accuracy: 58.1%\n",
      "Epoch: 170, loss: 699.06, accuracy: 58.1%\n",
      "Epoch: 180, loss: 688.89, accuracy: 58.6%\n",
      "Epoch: 190, loss: 674.91, accuracy: 60.6%\n",
      "Epoch: 200, loss: 657.46, accuracy: 63.7%\n",
      "Epoch: 210, loss: 638.13, accuracy: 67.5%\n",
      "Epoch: 220, loss: 619.16, accuracy: 70.2%\n",
      "Epoch: 230, loss: 602.40, accuracy: 71.1%\n",
      "Epoch: 240, loss: 588.63, accuracy: 71.5%\n",
      "Epoch: 250, loss: 577.76, accuracy: 72.1%\n",
      "Epoch: 260, loss: 569.30, accuracy: 72.1%\n",
      "Epoch: 270, loss: 562.67, accuracy: 72.8%\n",
      "Epoch: 280, loss: 557.39, accuracy: 73.2%\n",
      "Epoch: 290, loss: 553.09, accuracy: 73.5%\n",
      "Epoch: 300, loss: 549.52, accuracy: 73.5%\n",
      "Mean accuracy on T0: 62.39\n",
      "Mean losses on T0: 669.79\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 697.78, accuracy: 56.6%\n",
      "Epoch: 20, loss: 697.70, accuracy: 56.6%\n",
      "Epoch: 30, loss: 697.62, accuracy: 56.6%\n",
      "Epoch: 40, loss: 697.52, accuracy: 56.6%\n",
      "Epoch: 50, loss: 697.42, accuracy: 56.6%\n",
      "Epoch: 60, loss: 697.29, accuracy: 56.6%\n",
      "Epoch: 70, loss: 697.14, accuracy: 56.6%\n",
      "Epoch: 80, loss: 696.95, accuracy: 56.6%\n",
      "Epoch: 90, loss: 696.72, accuracy: 56.6%\n",
      "Epoch: 100, loss: 696.43, accuracy: 56.6%\n",
      "Epoch: 110, loss: 696.04, accuracy: 56.6%\n",
      "Epoch: 120, loss: 695.53, accuracy: 56.6%\n",
      "Epoch: 130, loss: 694.82, accuracy: 56.6%\n",
      "Epoch: 140, loss: 693.83, accuracy: 56.6%\n",
      "Epoch: 150, loss: 692.40, accuracy: 56.6%\n",
      "Epoch: 160, loss: 690.31, accuracy: 56.6%\n",
      "Epoch: 170, loss: 687.20, accuracy: 56.6%\n",
      "Epoch: 180, loss: 682.58, accuracy: 56.6%\n",
      "Epoch: 190, loss: 675.79, accuracy: 56.6%\n",
      "Epoch: 200, loss: 666.11, accuracy: 57.8%\n",
      "Epoch: 210, loss: 653.08, accuracy: 58.9%\n",
      "Epoch: 220, loss: 636.93, accuracy: 63.3%\n",
      "Epoch: 230, loss: 618.96, accuracy: 67.9%\n",
      "Epoch: 240, loss: 601.17, accuracy: 70.8%\n",
      "Epoch: 250, loss: 585.30, accuracy: 71.7%\n",
      "Epoch: 260, loss: 572.12, accuracy: 72.5%\n",
      "Epoch: 270, loss: 561.54, accuracy: 72.3%\n",
      "Epoch: 280, loss: 553.06, accuracy: 72.5%\n",
      "Epoch: 290, loss: 546.15, accuracy: 72.4%\n",
      "Epoch: 300, loss: 540.41, accuracy: 73.1%\n",
      "Mean accuracy on T1: 60.41\n",
      "Mean losses on T1: 660.43\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 701.00, accuracy: 56.4%\n",
      "Epoch: 20, loss: 700.92, accuracy: 56.4%\n",
      "Epoch: 30, loss: 700.84, accuracy: 56.4%\n",
      "Epoch: 40, loss: 700.75, accuracy: 56.4%\n",
      "Epoch: 50, loss: 700.65, accuracy: 56.4%\n",
      "Epoch: 60, loss: 700.53, accuracy: 56.4%\n",
      "Epoch: 70, loss: 700.38, accuracy: 56.4%\n",
      "Epoch: 80, loss: 700.19, accuracy: 56.4%\n",
      "Epoch: 90, loss: 699.96, accuracy: 56.4%\n",
      "Epoch: 100, loss: 699.66, accuracy: 56.4%\n",
      "Epoch: 110, loss: 699.26, accuracy: 56.4%\n",
      "Epoch: 120, loss: 698.72, accuracy: 56.4%\n",
      "Epoch: 130, loss: 697.97, accuracy: 56.4%\n",
      "Epoch: 140, loss: 696.89, accuracy: 56.4%\n",
      "Epoch: 150, loss: 695.33, accuracy: 56.4%\n",
      "Epoch: 160, loss: 693.00, accuracy: 56.4%\n",
      "Epoch: 170, loss: 689.52, accuracy: 56.4%\n",
      "Epoch: 180, loss: 684.31, accuracy: 56.4%\n",
      "Epoch: 190, loss: 676.65, accuracy: 56.5%\n",
      "Epoch: 200, loss: 665.79, accuracy: 57.6%\n",
      "Epoch: 210, loss: 651.29, accuracy: 60.2%\n",
      "Epoch: 220, loss: 633.51, accuracy: 63.7%\n",
      "Epoch: 230, loss: 613.91, accuracy: 66.9%\n",
      "Epoch: 240, loss: 594.53, accuracy: 70.2%\n",
      "Epoch: 250, loss: 577.14, accuracy: 71.0%\n",
      "Epoch: 260, loss: 562.57, accuracy: 71.1%\n",
      "Epoch: 270, loss: 550.83, accuracy: 71.7%\n",
      "Epoch: 280, loss: 541.52, accuracy: 72.4%\n",
      "Epoch: 290, loss: 534.11, accuracy: 72.8%\n",
      "Epoch: 300, loss: 528.16, accuracy: 72.9%\n",
      "Mean accuracy on T2: 60.02\n",
      "Mean losses on T2: 660.13\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 697.85, accuracy: 58.3%\n",
      "Epoch: 20, loss: 697.77, accuracy: 58.3%\n",
      "Epoch: 30, loss: 697.67, accuracy: 58.3%\n",
      "Epoch: 40, loss: 697.57, accuracy: 58.3%\n",
      "Epoch: 50, loss: 697.45, accuracy: 58.3%\n",
      "Epoch: 60, loss: 697.32, accuracy: 58.3%\n",
      "Epoch: 70, loss: 697.16, accuracy: 58.3%\n",
      "Epoch: 80, loss: 696.96, accuracy: 58.3%\n",
      "Epoch: 90, loss: 696.70, accuracy: 58.3%\n",
      "Epoch: 100, loss: 696.38, accuracy: 58.3%\n",
      "Epoch: 110, loss: 695.93, accuracy: 58.3%\n",
      "Epoch: 120, loss: 695.33, accuracy: 58.3%\n",
      "Epoch: 130, loss: 694.47, accuracy: 58.3%\n",
      "Epoch: 140, loss: 693.22, accuracy: 58.3%\n",
      "Epoch: 150, loss: 691.36, accuracy: 58.3%\n",
      "Epoch: 160, loss: 688.56, accuracy: 58.3%\n",
      "Epoch: 170, loss: 684.34, accuracy: 58.3%\n",
      "Epoch: 180, loss: 678.04, accuracy: 58.3%\n",
      "Epoch: 190, loss: 668.89, accuracy: 58.7%\n",
      "Epoch: 200, loss: 656.24, accuracy: 60.4%\n",
      "Epoch: 210, loss: 640.00, accuracy: 64.5%\n",
      "Epoch: 220, loss: 621.13, accuracy: 69.4%\n",
      "Epoch: 230, loss: 601.55, accuracy: 72.3%\n",
      "Epoch: 240, loss: 583.34, accuracy: 73.2%\n",
      "Epoch: 250, loss: 567.81, accuracy: 73.5%\n",
      "Epoch: 260, loss: 555.29, accuracy: 73.6%\n",
      "Epoch: 270, loss: 545.42, accuracy: 73.7%\n",
      "Epoch: 280, loss: 537.66, accuracy: 74.0%\n",
      "Epoch: 290, loss: 531.45, accuracy: 74.2%\n",
      "Epoch: 300, loss: 526.40, accuracy: 74.4%\n",
      "Mean accuracy on T3: 62.53\n",
      "Mean losses on T3: 652.88\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 729.30, accuracy: 54.7%\n",
      "Epoch: 20, loss: 729.22, accuracy: 54.7%\n",
      "Epoch: 30, loss: 729.14, accuracy: 54.7%\n",
      "Epoch: 40, loss: 729.04, accuracy: 54.7%\n",
      "Epoch: 50, loss: 728.93, accuracy: 54.7%\n",
      "Epoch: 60, loss: 728.80, accuracy: 54.7%\n",
      "Epoch: 70, loss: 728.63, accuracy: 54.7%\n",
      "Epoch: 80, loss: 728.41, accuracy: 54.7%\n",
      "Epoch: 90, loss: 728.11, accuracy: 54.7%\n",
      "Epoch: 100, loss: 727.70, accuracy: 54.7%\n",
      "Epoch: 110, loss: 727.12, accuracy: 54.7%\n",
      "Epoch: 120, loss: 726.26, accuracy: 54.7%\n",
      "Epoch: 130, loss: 724.97, accuracy: 54.7%\n",
      "Epoch: 140, loss: 722.97, accuracy: 54.7%\n",
      "Epoch: 150, loss: 719.88, accuracy: 54.7%\n",
      "Epoch: 160, loss: 715.10, accuracy: 54.7%\n",
      "Epoch: 170, loss: 707.89, accuracy: 54.7%\n",
      "Epoch: 180, loss: 697.47, accuracy: 54.9%\n",
      "Epoch: 190, loss: 683.49, accuracy: 56.1%\n",
      "Epoch: 200, loss: 666.57, accuracy: 58.7%\n",
      "Epoch: 210, loss: 648.45, accuracy: 61.2%\n",
      "Epoch: 220, loss: 631.29, accuracy: 63.7%\n",
      "Epoch: 230, loss: 616.54, accuracy: 66.8%\n",
      "Epoch: 240, loss: 604.64, accuracy: 68.3%\n",
      "Epoch: 250, loss: 595.26, accuracy: 68.8%\n",
      "Epoch: 260, loss: 587.87, accuracy: 69.5%\n",
      "Epoch: 270, loss: 581.95, accuracy: 70.0%\n",
      "Epoch: 280, loss: 577.14, accuracy: 71.2%\n",
      "Epoch: 290, loss: 573.19, accuracy: 71.1%\n",
      "Epoch: 300, loss: 569.95, accuracy: 70.8%\n",
      "Mean accuracy on T4: 59.56\n",
      "Mean losses on T4: 678.69\n",
      "\n",
      "Accuracy list on Ti sets: [62.3863337713535, 60.40932982917214, 60.02467513505623, 62.53164695575998, 59.55983187134503]\n",
      "Losses list on Ti sets: [669.7925649671646, 660.4343583323248, 660.1317173455293, 652.8842731536612, 678.686595611749]\n",
      "Mean accuracy on all T sets: 60.98\n",
      "Mean losses on all T sets: 664.39\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ trzecia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: relu\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 704.43, accuracy: 58.3%\n",
      "Epoch: 20, loss: 704.07, accuracy: 58.3%\n",
      "Epoch: 30, loss: 700.89, accuracy: 58.3%\n",
      "Epoch: 40, loss: 679.73, accuracy: 58.3%\n",
      "Epoch: 50, loss: 621.46, accuracy: 67.0%\n",
      "Epoch: 60, loss: 583.81, accuracy: 69.6%\n",
      "Epoch: 70, loss: 568.64, accuracy: 70.3%\n",
      "Epoch: 80, loss: 561.52, accuracy: 70.2%\n",
      "Epoch: 90, loss: 559.50, accuracy: 70.2%\n",
      "Epoch: 100, loss: 559.90, accuracy: 70.6%\n",
      "Epoch: 110, loss: 568.41, accuracy: 70.8%\n",
      "Epoch: 120, loss: 712.49, accuracy: 67.8%\n",
      "Epoch: 130, loss: 579.49, accuracy: 71.0%\n",
      "Epoch: 140, loss: 555.13, accuracy: 73.2%\n",
      "Epoch: 150, loss: 553.79, accuracy: 73.7%\n",
      "Epoch: 160, loss: 563.37, accuracy: 73.5%\n",
      "Epoch: 170, loss: 566.02, accuracy: 73.7%\n",
      "Epoch: 180, loss: 567.37, accuracy: 74.4%\n",
      "Epoch: 190, loss: 574.41, accuracy: 75.0%\n",
      "Epoch: 200, loss: 582.61, accuracy: 74.8%\n",
      "Epoch: 210, loss: 590.68, accuracy: 75.0%\n",
      "Epoch: 220, loss: 598.66, accuracy: 74.6%\n",
      "Epoch: 230, loss: 606.41, accuracy: 74.5%\n",
      "Epoch: 240, loss: 613.89, accuracy: 74.4%\n",
      "Epoch: 250, loss: 621.05, accuracy: 74.4%\n",
      "Epoch: 260, loss: 627.89, accuracy: 74.4%\n",
      "Epoch: 270, loss: 634.41, accuracy: 74.4%\n",
      "Epoch: 280, loss: 640.63, accuracy: 74.4%\n",
      "Epoch: 290, loss: 646.58, accuracy: 74.2%\n",
      "Epoch: 300, loss: 652.27, accuracy: 74.1%\n",
      "Mean accuracy on T0: 70.40\n",
      "Mean losses on T0: 613.87\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 698.52, accuracy: 58.7%\n",
      "Epoch: 20, loss: 698.19, accuracy: 58.7%\n",
      "Epoch: 30, loss: 695.76, accuracy: 58.7%\n",
      "Epoch: 40, loss: 679.23, accuracy: 58.7%\n",
      "Epoch: 50, loss: 618.25, accuracy: 69.0%\n",
      "Epoch: 60, loss: 557.43, accuracy: 72.9%\n",
      "Epoch: 70, loss: 531.57, accuracy: 73.2%\n",
      "Epoch: 80, loss: 518.98, accuracy: 74.1%\n",
      "Epoch: 90, loss: 511.85, accuracy: 73.9%\n",
      "Epoch: 100, loss: 507.55, accuracy: 73.9%\n",
      "Epoch: 110, loss: 505.73, accuracy: 73.5%\n",
      "Epoch: 120, loss: 499.83, accuracy: 73.9%\n",
      "Epoch: 130, loss: 494.16, accuracy: 74.9%\n",
      "Epoch: 140, loss: 489.64, accuracy: 76.0%\n",
      "Epoch: 150, loss: 489.92, accuracy: 75.7%\n",
      "Epoch: 160, loss: 493.68, accuracy: 75.6%\n",
      "Epoch: 170, loss: 498.65, accuracy: 76.0%\n",
      "Epoch: 180, loss: 505.94, accuracy: 76.5%\n",
      "Epoch: 190, loss: 513.57, accuracy: 76.3%\n",
      "Epoch: 200, loss: 521.06, accuracy: 76.2%\n",
      "Epoch: 210, loss: 528.52, accuracy: 76.2%\n",
      "Epoch: 220, loss: 535.82, accuracy: 76.2%\n",
      "Epoch: 230, loss: 542.87, accuracy: 76.1%\n",
      "Epoch: 240, loss: 549.64, accuracy: 76.2%\n",
      "Epoch: 250, loss: 556.12, accuracy: 76.2%\n",
      "Epoch: 260, loss: 562.30, accuracy: 76.2%\n",
      "Epoch: 270, loss: 568.20, accuracy: 76.1%\n",
      "Epoch: 280, loss: 573.83, accuracy: 76.1%\n",
      "Epoch: 290, loss: 579.21, accuracy: 76.0%\n",
      "Epoch: 300, loss: 584.35, accuracy: 76.1%\n",
      "Mean accuracy on T1: 72.14\n",
      "Mean losses on T1: 564.20\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 702.71, accuracy: 58.0%\n",
      "Epoch: 20, loss: 702.51, accuracy: 58.0%\n",
      "Epoch: 30, loss: 700.85, accuracy: 58.0%\n",
      "Epoch: 40, loss: 688.39, accuracy: 58.0%\n",
      "Epoch: 50, loss: 634.15, accuracy: 66.1%\n",
      "Epoch: 60, loss: 571.63, accuracy: 71.9%\n",
      "Epoch: 70, loss: 542.78, accuracy: 72.5%\n",
      "Epoch: 80, loss: 530.32, accuracy: 72.0%\n",
      "Epoch: 90, loss: 525.45, accuracy: 72.0%\n",
      "Epoch: 100, loss: 522.43, accuracy: 72.5%\n",
      "Epoch: 110, loss: 533.06, accuracy: 72.1%\n",
      "Epoch: 120, loss: 545.18, accuracy: 72.4%\n",
      "Epoch: 130, loss: 561.18, accuracy: 72.3%\n",
      "Epoch: 140, loss: 533.14, accuracy: 72.9%\n",
      "Epoch: 150, loss: 527.37, accuracy: 73.9%\n",
      "Epoch: 160, loss: 522.02, accuracy: 73.9%\n",
      "Epoch: 170, loss: 523.47, accuracy: 74.6%\n",
      "Epoch: 180, loss: 528.61, accuracy: 74.5%\n",
      "Epoch: 190, loss: 535.33, accuracy: 74.5%\n",
      "Epoch: 200, loss: 542.36, accuracy: 74.8%\n",
      "Epoch: 210, loss: 549.40, accuracy: 74.9%\n",
      "Epoch: 220, loss: 556.33, accuracy: 74.9%\n",
      "Epoch: 230, loss: 563.08, accuracy: 74.9%\n",
      "Epoch: 240, loss: 569.62, accuracy: 74.9%\n",
      "Epoch: 250, loss: 575.91, accuracy: 74.9%\n",
      "Epoch: 260, loss: 581.96, accuracy: 75.0%\n",
      "Epoch: 270, loss: 587.77, accuracy: 74.9%\n",
      "Epoch: 280, loss: 593.33, accuracy: 74.6%\n",
      "Epoch: 290, loss: 598.67, accuracy: 74.8%\n",
      "Epoch: 300, loss: 603.78, accuracy: 74.8%\n",
      "Mean accuracy on T2: 71.08\n",
      "Mean losses on T2: 582.28\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 726.45, accuracy: 53.9%\n",
      "Epoch: 20, loss: 726.17, accuracy: 53.9%\n",
      "Epoch: 30, loss: 723.80, accuracy: 53.9%\n",
      "Epoch: 40, loss: 707.73, accuracy: 53.9%\n",
      "Epoch: 50, loss: 646.49, accuracy: 64.8%\n",
      "Epoch: 60, loss: 577.70, accuracy: 72.8%\n",
      "Epoch: 70, loss: 545.70, accuracy: 73.5%\n",
      "Epoch: 80, loss: 531.21, accuracy: 73.9%\n",
      "Epoch: 90, loss: 524.94, accuracy: 74.2%\n",
      "Epoch: 100, loss: 540.42, accuracy: 72.7%\n",
      "Epoch: 110, loss: 592.65, accuracy: 70.8%\n",
      "Epoch: 120, loss: 585.41, accuracy: 72.3%\n",
      "Epoch: 130, loss: 572.74, accuracy: 72.9%\n",
      "Epoch: 140, loss: 561.99, accuracy: 73.7%\n",
      "Epoch: 150, loss: 543.35, accuracy: 74.2%\n",
      "Epoch: 160, loss: 516.47, accuracy: 75.0%\n",
      "Epoch: 170, loss: 498.04, accuracy: 74.2%\n",
      "Epoch: 180, loss: 499.28, accuracy: 74.1%\n",
      "Epoch: 190, loss: 504.61, accuracy: 74.1%\n",
      "Epoch: 200, loss: 510.33, accuracy: 74.8%\n",
      "Epoch: 210, loss: 516.16, accuracy: 74.9%\n",
      "Epoch: 220, loss: 522.07, accuracy: 74.9%\n",
      "Epoch: 230, loss: 527.93, accuracy: 75.0%\n",
      "Epoch: 240, loss: 533.65, accuracy: 75.2%\n",
      "Epoch: 250, loss: 539.22, accuracy: 75.2%\n",
      "Epoch: 260, loss: 544.57, accuracy: 75.2%\n",
      "Epoch: 270, loss: 549.75, accuracy: 75.2%\n",
      "Epoch: 280, loss: 554.72, accuracy: 75.2%\n",
      "Epoch: 290, loss: 559.52, accuracy: 75.2%\n",
      "Epoch: 300, loss: 564.13, accuracy: 75.3%\n",
      "Mean accuracy on T3: 70.59\n",
      "Mean losses on T3: 569.43\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 708.08, accuracy: 55.3%\n",
      "Epoch: 20, loss: 707.81, accuracy: 55.3%\n",
      "Epoch: 30, loss: 706.45, accuracy: 55.3%\n",
      "Epoch: 40, loss: 697.19, accuracy: 55.3%\n",
      "Epoch: 50, loss: 652.26, accuracy: 58.0%\n",
      "Epoch: 60, loss: 570.41, accuracy: 73.7%\n",
      "Epoch: 70, loss: 524.08, accuracy: 76.2%\n",
      "Epoch: 80, loss: 505.08, accuracy: 77.1%\n",
      "Epoch: 90, loss: 496.19, accuracy: 77.0%\n",
      "Epoch: 100, loss: 487.51, accuracy: 77.2%\n",
      "Epoch: 110, loss: 487.45, accuracy: 76.4%\n",
      "Epoch: 120, loss: 485.21, accuracy: 77.1%\n",
      "Epoch: 130, loss: 479.83, accuracy: 78.2%\n",
      "Epoch: 140, loss: 473.81, accuracy: 78.7%\n",
      "Epoch: 150, loss: 465.91, accuracy: 79.1%\n",
      "Epoch: 160, loss: 465.90, accuracy: 78.9%\n",
      "Epoch: 170, loss: 480.78, accuracy: 78.7%\n",
      "Epoch: 180, loss: 494.67, accuracy: 78.6%\n",
      "Epoch: 190, loss: 490.25, accuracy: 78.4%\n",
      "Epoch: 200, loss: 493.30, accuracy: 78.0%\n",
      "Epoch: 210, loss: 498.47, accuracy: 78.2%\n",
      "Epoch: 220, loss: 503.79, accuracy: 78.0%\n",
      "Epoch: 230, loss: 509.17, accuracy: 77.9%\n",
      "Epoch: 240, loss: 514.62, accuracy: 77.9%\n",
      "Epoch: 250, loss: 519.96, accuracy: 77.6%\n",
      "Epoch: 260, loss: 525.13, accuracy: 77.6%\n",
      "Epoch: 270, loss: 530.12, accuracy: 77.8%\n",
      "Epoch: 280, loss: 534.89, accuracy: 77.9%\n",
      "Epoch: 290, loss: 539.52, accuracy: 77.9%\n",
      "Epoch: 300, loss: 543.93, accuracy: 77.9%\n",
      "Mean accuracy on T4: 73.22\n",
      "Mean losses on T4: 546.59\n",
      "\n",
      "Accuracy list on Ti sets: [70.39575120455542, 72.1374288217258, 71.08362534676596, 70.5925317564608, 73.21977339181288]\n",
      "Losses list on Ti sets: [613.8657933623543, 564.1994164649714, 582.27577804414, 569.4287480095385, 546.5866464884413]\n",
      "Mean accuracy on all T sets: 71.49\n",
      "Mean losses on all T sets: 575.27\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ czwarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: sigmoid\n",
      "2. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 700.35, accuracy: 56.5%\n",
      "Epoch: 20, loss: 700.31, accuracy: 56.5%\n",
      "Epoch: 30, loss: 700.28, accuracy: 56.5%\n",
      "Epoch: 40, loss: 700.24, accuracy: 56.5%\n",
      "Epoch: 50, loss: 700.19, accuracy: 56.5%\n",
      "Epoch: 60, loss: 700.13, accuracy: 56.5%\n",
      "Epoch: 70, loss: 700.05, accuracy: 56.5%\n",
      "Epoch: 80, loss: 699.94, accuracy: 56.5%\n",
      "Epoch: 90, loss: 699.78, accuracy: 56.5%\n",
      "Epoch: 100, loss: 699.53, accuracy: 56.5%\n",
      "Epoch: 110, loss: 699.14, accuracy: 56.5%\n",
      "Epoch: 120, loss: 698.51, accuracy: 56.5%\n",
      "Epoch: 130, loss: 697.49, accuracy: 56.5%\n",
      "Epoch: 140, loss: 695.83, accuracy: 56.5%\n",
      "Epoch: 150, loss: 693.13, accuracy: 56.5%\n",
      "Epoch: 160, loss: 688.77, accuracy: 56.5%\n",
      "Epoch: 170, loss: 681.93, accuracy: 56.5%\n",
      "Epoch: 180, loss: 671.64, accuracy: 56.9%\n",
      "Epoch: 190, loss: 657.16, accuracy: 57.8%\n",
      "Epoch: 200, loss: 638.68, accuracy: 61.0%\n",
      "Epoch: 210, loss: 617.80, accuracy: 64.8%\n",
      "Epoch: 220, loss: 597.13, accuracy: 68.7%\n",
      "Epoch: 230, loss: 578.92, accuracy: 71.5%\n",
      "Epoch: 240, loss: 564.20, accuracy: 72.9%\n",
      "Epoch: 250, loss: 552.84, accuracy: 73.6%\n",
      "Epoch: 260, loss: 544.27, accuracy: 73.6%\n",
      "Epoch: 270, loss: 537.83, accuracy: 74.4%\n",
      "Epoch: 280, loss: 533.03, accuracy: 74.4%\n",
      "Epoch: 290, loss: 529.50, accuracy: 74.4%\n",
      "Epoch: 300, loss: 526.94, accuracy: 74.5%\n",
      "Mean accuracy on T0: 61.78\n",
      "Mean losses on T0: 654.57\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 731.49, accuracy: 54.3%\n",
      "Epoch: 20, loss: 731.48, accuracy: 54.3%\n",
      "Epoch: 30, loss: 731.47, accuracy: 54.3%\n",
      "Epoch: 40, loss: 731.45, accuracy: 54.3%\n",
      "Epoch: 50, loss: 731.41, accuracy: 54.3%\n",
      "Epoch: 60, loss: 731.36, accuracy: 54.3%\n",
      "Epoch: 70, loss: 731.29, accuracy: 54.3%\n",
      "Epoch: 80, loss: 731.16, accuracy: 54.3%\n",
      "Epoch: 90, loss: 730.95, accuracy: 54.3%\n",
      "Epoch: 100, loss: 730.60, accuracy: 54.3%\n",
      "Epoch: 110, loss: 730.02, accuracy: 54.3%\n",
      "Epoch: 120, loss: 729.06, accuracy: 54.3%\n",
      "Epoch: 130, loss: 727.46, accuracy: 54.3%\n",
      "Epoch: 140, loss: 724.80, accuracy: 54.3%\n",
      "Epoch: 150, loss: 720.46, accuracy: 54.3%\n",
      "Epoch: 160, loss: 713.59, accuracy: 54.3%\n",
      "Epoch: 170, loss: 703.25, accuracy: 54.9%\n",
      "Epoch: 180, loss: 688.82, accuracy: 56.2%\n",
      "Epoch: 190, loss: 670.73, accuracy: 59.3%\n",
      "Epoch: 200, loss: 650.87, accuracy: 63.3%\n",
      "Epoch: 210, loss: 631.80, accuracy: 65.6%\n",
      "Epoch: 220, loss: 615.42, accuracy: 67.9%\n",
      "Epoch: 230, loss: 602.31, accuracy: 67.8%\n",
      "Epoch: 240, loss: 592.14, accuracy: 68.6%\n",
      "Epoch: 250, loss: 584.24, accuracy: 69.4%\n",
      "Epoch: 260, loss: 577.99, accuracy: 69.4%\n",
      "Epoch: 270, loss: 572.92, accuracy: 69.9%\n",
      "Epoch: 280, loss: 568.69, accuracy: 70.0%\n",
      "Epoch: 290, loss: 565.11, accuracy: 70.4%\n",
      "Epoch: 300, loss: 562.02, accuracy: 71.0%\n",
      "Mean accuracy on T1: 59.60\n",
      "Mean losses on T1: 682.44\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 717.63, accuracy: 55.6%\n",
      "Epoch: 20, loss: 717.60, accuracy: 55.6%\n",
      "Epoch: 30, loss: 717.57, accuracy: 55.6%\n",
      "Epoch: 40, loss: 717.54, accuracy: 55.6%\n",
      "Epoch: 50, loss: 717.49, accuracy: 55.6%\n",
      "Epoch: 60, loss: 717.43, accuracy: 55.6%\n",
      "Epoch: 70, loss: 717.35, accuracy: 55.6%\n",
      "Epoch: 80, loss: 717.23, accuracy: 55.6%\n",
      "Epoch: 90, loss: 717.06, accuracy: 55.6%\n",
      "Epoch: 100, loss: 716.78, accuracy: 55.6%\n",
      "Epoch: 110, loss: 716.33, accuracy: 55.6%\n",
      "Epoch: 120, loss: 715.61, accuracy: 55.6%\n",
      "Epoch: 130, loss: 714.43, accuracy: 55.6%\n",
      "Epoch: 140, loss: 712.49, accuracy: 55.6%\n",
      "Epoch: 150, loss: 709.32, accuracy: 55.6%\n",
      "Epoch: 160, loss: 704.23, accuracy: 55.6%\n",
      "Epoch: 170, loss: 696.29, accuracy: 55.6%\n",
      "Epoch: 180, loss: 684.56, accuracy: 56.0%\n",
      "Epoch: 190, loss: 668.50, accuracy: 57.6%\n",
      "Epoch: 200, loss: 648.74, accuracy: 60.8%\n",
      "Epoch: 210, loss: 627.28, accuracy: 64.8%\n",
      "Epoch: 220, loss: 606.67, accuracy: 68.7%\n",
      "Epoch: 230, loss: 588.76, accuracy: 70.6%\n",
      "Epoch: 240, loss: 574.22, accuracy: 72.0%\n",
      "Epoch: 250, loss: 562.83, accuracy: 72.0%\n",
      "Epoch: 260, loss: 554.04, accuracy: 72.0%\n",
      "Epoch: 270, loss: 547.28, accuracy: 72.4%\n",
      "Epoch: 280, loss: 542.06, accuracy: 72.8%\n",
      "Epoch: 290, loss: 538.01, accuracy: 73.1%\n",
      "Epoch: 300, loss: 534.86, accuracy: 73.9%\n",
      "Mean accuracy on T2: 60.70\n",
      "Mean losses on T2: 667.73\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 692.85, accuracy: 59.3%\n",
      "Epoch: 20, loss: 692.72, accuracy: 59.3%\n",
      "Epoch: 30, loss: 692.59, accuracy: 59.3%\n",
      "Epoch: 40, loss: 692.46, accuracy: 59.3%\n",
      "Epoch: 50, loss: 692.33, accuracy: 59.3%\n",
      "Epoch: 60, loss: 692.18, accuracy: 59.3%\n",
      "Epoch: 70, loss: 692.00, accuracy: 59.3%\n",
      "Epoch: 80, loss: 691.77, accuracy: 59.3%\n",
      "Epoch: 90, loss: 691.46, accuracy: 59.3%\n",
      "Epoch: 100, loss: 690.99, accuracy: 59.3%\n",
      "Epoch: 110, loss: 690.26, accuracy: 59.3%\n",
      "Epoch: 120, loss: 689.09, accuracy: 59.3%\n",
      "Epoch: 130, loss: 687.18, accuracy: 59.3%\n",
      "Epoch: 140, loss: 684.05, accuracy: 59.3%\n",
      "Epoch: 150, loss: 679.00, accuracy: 59.3%\n",
      "Epoch: 160, loss: 671.15, accuracy: 59.3%\n",
      "Epoch: 170, loss: 659.65, accuracy: 59.5%\n",
      "Epoch: 180, loss: 644.32, accuracy: 62.4%\n",
      "Epoch: 190, loss: 626.32, accuracy: 65.4%\n",
      "Epoch: 200, loss: 608.09, accuracy: 68.1%\n",
      "Epoch: 210, loss: 592.05, accuracy: 69.8%\n",
      "Epoch: 220, loss: 579.36, accuracy: 70.0%\n",
      "Epoch: 230, loss: 569.90, accuracy: 70.4%\n",
      "Epoch: 240, loss: 562.94, accuracy: 71.5%\n",
      "Epoch: 250, loss: 557.74, accuracy: 71.7%\n",
      "Epoch: 260, loss: 553.75, accuracy: 71.6%\n",
      "Epoch: 270, loss: 550.59, accuracy: 72.0%\n",
      "Epoch: 280, loss: 548.03, accuracy: 72.4%\n",
      "Epoch: 290, loss: 545.94, accuracy: 73.1%\n",
      "Epoch: 300, loss: 544.22, accuracy: 73.3%\n",
      "Mean accuracy on T3: 63.96\n",
      "Mean losses on T3: 645.96\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 698.08, accuracy: 58.6%\n",
      "Epoch: 20, loss: 697.97, accuracy: 58.6%\n",
      "Epoch: 30, loss: 697.86, accuracy: 58.6%\n",
      "Epoch: 40, loss: 697.74, accuracy: 58.6%\n",
      "Epoch: 50, loss: 697.63, accuracy: 58.6%\n",
      "Epoch: 60, loss: 697.50, accuracy: 58.6%\n",
      "Epoch: 70, loss: 697.36, accuracy: 58.6%\n",
      "Epoch: 80, loss: 697.20, accuracy: 58.6%\n",
      "Epoch: 90, loss: 697.00, accuracy: 58.6%\n",
      "Epoch: 100, loss: 696.74, accuracy: 58.6%\n",
      "Epoch: 110, loss: 696.38, accuracy: 58.6%\n",
      "Epoch: 120, loss: 695.83, accuracy: 58.6%\n",
      "Epoch: 130, loss: 695.00, accuracy: 58.6%\n",
      "Epoch: 140, loss: 693.68, accuracy: 58.6%\n",
      "Epoch: 150, loss: 691.55, accuracy: 58.6%\n",
      "Epoch: 160, loss: 688.14, accuracy: 58.6%\n",
      "Epoch: 170, loss: 682.71, accuracy: 58.6%\n",
      "Epoch: 180, loss: 674.33, accuracy: 58.7%\n",
      "Epoch: 190, loss: 662.08, accuracy: 59.7%\n",
      "Epoch: 200, loss: 645.55, accuracy: 62.8%\n",
      "Epoch: 210, loss: 625.61, accuracy: 66.6%\n",
      "Epoch: 220, loss: 604.48, accuracy: 70.0%\n",
      "Epoch: 230, loss: 584.69, accuracy: 72.1%\n",
      "Epoch: 240, loss: 567.78, accuracy: 74.2%\n",
      "Epoch: 250, loss: 553.96, accuracy: 74.6%\n",
      "Epoch: 260, loss: 542.76, accuracy: 74.5%\n",
      "Epoch: 270, loss: 533.54, accuracy: 74.6%\n",
      "Epoch: 280, loss: 525.76, accuracy: 74.7%\n",
      "Epoch: 290, loss: 519.09, accuracy: 75.0%\n",
      "Epoch: 300, loss: 513.33, accuracy: 75.3%\n",
      "Mean accuracy on T4: 63.38\n",
      "Mean losses on T4: 651.66\n",
      "\n",
      "Accuracy list on Ti sets: [61.77942035333624, 59.59764199153161, 60.69674405022629, 63.95513943641408, 63.380336257309956]\n",
      "Losses list on Ti sets: [654.57433465732, 682.4366001893009, 667.7277291026081, 645.9561109754826, 651.6629308338345]\n",
      "Mean accuracy on all T sets: 61.88\n",
      "Mean losses on all T sets: 660.47\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ piÄ…ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: relu\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"relu\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 701.96, accuracy: 57.0%\n",
      "Epoch: 20, loss: 700.90, accuracy: 57.0%\n",
      "Epoch: 30, loss: 693.43, accuracy: 57.0%\n",
      "Epoch: 40, loss: 654.07, accuracy: 59.5%\n",
      "Epoch: 50, loss: 582.61, accuracy: 71.2%\n",
      "Epoch: 60, loss: 544.66, accuracy: 74.0%\n",
      "Epoch: 70, loss: 528.34, accuracy: 74.6%\n",
      "Epoch: 80, loss: 520.28, accuracy: 74.2%\n",
      "Epoch: 90, loss: 516.24, accuracy: 74.4%\n",
      "Epoch: 100, loss: 514.43, accuracy: 74.0%\n",
      "Epoch: 110, loss: 525.69, accuracy: 74.4%\n",
      "Epoch: 120, loss: 544.31, accuracy: 74.5%\n",
      "Epoch: 130, loss: 521.15, accuracy: 75.0%\n",
      "Epoch: 140, loss: 501.06, accuracy: 76.1%\n",
      "Epoch: 150, loss: 498.57, accuracy: 76.7%\n",
      "Epoch: 160, loss: 502.56, accuracy: 76.7%\n",
      "Epoch: 170, loss: 508.36, accuracy: 76.9%\n",
      "Epoch: 180, loss: 513.37, accuracy: 76.9%\n",
      "Epoch: 190, loss: 519.26, accuracy: 77.0%\n",
      "Epoch: 200, loss: 525.68, accuracy: 77.0%\n",
      "Epoch: 210, loss: 532.29, accuracy: 77.0%\n",
      "Epoch: 220, loss: 538.86, accuracy: 76.7%\n",
      "Epoch: 230, loss: 545.27, accuracy: 76.6%\n",
      "Epoch: 240, loss: 551.46, accuracy: 76.6%\n",
      "Epoch: 250, loss: 557.39, accuracy: 76.6%\n",
      "Epoch: 260, loss: 563.07, accuracy: 76.3%\n",
      "Epoch: 270, loss: 568.49, accuracy: 76.2%\n",
      "Epoch: 280, loss: 573.66, accuracy: 76.2%\n",
      "Epoch: 290, loss: 578.60, accuracy: 76.1%\n",
      "Epoch: 300, loss: 583.32, accuracy: 76.1%\n",
      "Mean accuracy on T0: 72.53\n",
      "Mean losses on T0: 571.12\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 704.82, accuracy: 58.7%\n",
      "Epoch: 20, loss: 704.21, accuracy: 58.7%\n",
      "Epoch: 30, loss: 698.94, accuracy: 58.7%\n",
      "Epoch: 40, loss: 667.68, accuracy: 59.7%\n",
      "Epoch: 50, loss: 598.32, accuracy: 73.2%\n",
      "Epoch: 60, loss: 557.71, accuracy: 72.5%\n",
      "Epoch: 70, loss: 539.78, accuracy: 72.8%\n",
      "Epoch: 80, loss: 530.85, accuracy: 72.1%\n",
      "Epoch: 90, loss: 526.94, accuracy: 72.0%\n",
      "Epoch: 100, loss: 542.17, accuracy: 73.2%\n",
      "Epoch: 110, loss: 522.33, accuracy: 74.1%\n",
      "Epoch: 120, loss: 529.68, accuracy: 73.9%\n",
      "Epoch: 130, loss: 515.54, accuracy: 74.4%\n",
      "Epoch: 140, loss: 504.96, accuracy: 75.8%\n",
      "Epoch: 150, loss: 503.45, accuracy: 75.4%\n",
      "Epoch: 160, loss: 507.29, accuracy: 75.8%\n",
      "Epoch: 170, loss: 512.21, accuracy: 76.2%\n",
      "Epoch: 180, loss: 517.96, accuracy: 75.8%\n",
      "Epoch: 190, loss: 524.05, accuracy: 75.8%\n",
      "Epoch: 200, loss: 530.23, accuracy: 75.7%\n",
      "Epoch: 210, loss: 536.38, accuracy: 75.8%\n",
      "Epoch: 220, loss: 542.40, accuracy: 75.8%\n",
      "Epoch: 230, loss: 548.26, accuracy: 75.7%\n",
      "Epoch: 240, loss: 553.90, accuracy: 75.4%\n",
      "Epoch: 250, loss: 559.31, accuracy: 75.2%\n",
      "Epoch: 260, loss: 564.48, accuracy: 75.0%\n",
      "Epoch: 270, loss: 569.40, accuracy: 75.2%\n",
      "Epoch: 280, loss: 574.09, accuracy: 75.2%\n",
      "Epoch: 290, loss: 578.56, accuracy: 75.2%\n",
      "Epoch: 300, loss: 582.81, accuracy: 75.0%\n",
      "Mean accuracy on T1: 72.06\n",
      "Mean losses on T1: 573.77\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 733.50, accuracy: 54.8%\n",
      "Epoch: 20, loss: 733.03, accuracy: 54.8%\n",
      "Epoch: 30, loss: 727.82, accuracy: 54.8%\n",
      "Epoch: 40, loss: 695.59, accuracy: 56.1%\n",
      "Epoch: 50, loss: 625.74, accuracy: 67.4%\n",
      "Epoch: 60, loss: 588.35, accuracy: 70.0%\n",
      "Epoch: 70, loss: 573.28, accuracy: 70.8%\n",
      "Epoch: 80, loss: 565.91, accuracy: 71.2%\n",
      "Epoch: 90, loss: 562.65, accuracy: 71.6%\n",
      "Epoch: 100, loss: 560.20, accuracy: 71.2%\n",
      "Epoch: 110, loss: 580.07, accuracy: 69.0%\n",
      "Epoch: 120, loss: 616.81, accuracy: 67.7%\n",
      "Epoch: 130, loss: 563.49, accuracy: 71.9%\n",
      "Epoch: 140, loss: 555.81, accuracy: 72.8%\n",
      "Epoch: 150, loss: 560.44, accuracy: 72.1%\n",
      "Epoch: 160, loss: 565.27, accuracy: 72.1%\n",
      "Epoch: 170, loss: 570.42, accuracy: 72.4%\n",
      "Epoch: 180, loss: 577.25, accuracy: 72.1%\n",
      "Epoch: 190, loss: 584.61, accuracy: 71.9%\n",
      "Epoch: 200, loss: 592.09, accuracy: 71.6%\n",
      "Epoch: 210, loss: 599.52, accuracy: 71.6%\n",
      "Epoch: 220, loss: 606.76, accuracy: 72.0%\n",
      "Epoch: 230, loss: 613.76, accuracy: 71.9%\n",
      "Epoch: 240, loss: 620.49, accuracy: 72.0%\n",
      "Epoch: 250, loss: 626.93, accuracy: 71.9%\n",
      "Epoch: 260, loss: 633.06, accuracy: 71.9%\n",
      "Epoch: 270, loss: 638.92, accuracy: 71.9%\n",
      "Epoch: 280, loss: 644.51, accuracy: 71.9%\n",
      "Epoch: 290, loss: 649.81, accuracy: 71.9%\n",
      "Epoch: 300, loss: 654.87, accuracy: 72.0%\n",
      "Mean accuracy on T2: 68.72\n",
      "Mean losses on T2: 627.72\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 697.23, accuracy: 58.2%\n",
      "Epoch: 20, loss: 696.65, accuracy: 58.2%\n",
      "Epoch: 30, loss: 692.46, accuracy: 58.2%\n",
      "Epoch: 40, loss: 666.05, accuracy: 58.3%\n",
      "Epoch: 50, loss: 593.30, accuracy: 69.4%\n",
      "Epoch: 60, loss: 540.34, accuracy: 73.1%\n",
      "Epoch: 70, loss: 517.10, accuracy: 74.5%\n",
      "Epoch: 80, loss: 505.54, accuracy: 74.5%\n",
      "Epoch: 90, loss: 498.52, accuracy: 74.4%\n",
      "Epoch: 100, loss: 591.74, accuracy: 70.7%\n",
      "Epoch: 110, loss: 550.91, accuracy: 72.1%\n",
      "Epoch: 120, loss: 540.26, accuracy: 72.9%\n",
      "Epoch: 130, loss: 521.11, accuracy: 75.4%\n",
      "Epoch: 140, loss: 503.58, accuracy: 76.9%\n",
      "Epoch: 150, loss: 474.39, accuracy: 77.7%\n",
      "Epoch: 160, loss: 461.51, accuracy: 77.8%\n",
      "Epoch: 170, loss: 462.93, accuracy: 77.7%\n",
      "Epoch: 180, loss: 466.55, accuracy: 77.3%\n",
      "Epoch: 190, loss: 470.66, accuracy: 77.4%\n",
      "Epoch: 200, loss: 475.14, accuracy: 77.1%\n",
      "Epoch: 210, loss: 479.75, accuracy: 77.0%\n",
      "Epoch: 220, loss: 484.37, accuracy: 77.0%\n",
      "Epoch: 230, loss: 488.96, accuracy: 77.0%\n",
      "Epoch: 240, loss: 493.46, accuracy: 77.0%\n",
      "Epoch: 250, loss: 497.83, accuracy: 77.1%\n",
      "Epoch: 260, loss: 502.08, accuracy: 77.5%\n",
      "Epoch: 270, loss: 506.17, accuracy: 77.4%\n",
      "Epoch: 280, loss: 510.11, accuracy: 77.3%\n",
      "Epoch: 290, loss: 513.91, accuracy: 77.1%\n",
      "Epoch: 300, loss: 517.55, accuracy: 77.1%\n",
      "Mean accuracy on T3: 73.13\n",
      "Mean losses on T3: 534.95\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 702.95, accuracy: 55.4%\n",
      "Epoch: 20, loss: 702.18, accuracy: 55.4%\n",
      "Epoch: 30, loss: 697.52, accuracy: 55.4%\n",
      "Epoch: 40, loss: 669.67, accuracy: 55.4%\n",
      "Epoch: 50, loss: 592.74, accuracy: 71.2%\n",
      "Epoch: 60, loss: 536.24, accuracy: 74.9%\n",
      "Epoch: 70, loss: 516.06, accuracy: 75.1%\n",
      "Epoch: 80, loss: 510.54, accuracy: 74.5%\n",
      "Epoch: 90, loss: 511.42, accuracy: 73.8%\n",
      "Epoch: 100, loss: 499.80, accuracy: 75.4%\n",
      "Epoch: 110, loss: 494.15, accuracy: 75.4%\n",
      "Epoch: 120, loss: 493.41, accuracy: 75.7%\n",
      "Epoch: 130, loss: 489.11, accuracy: 76.6%\n",
      "Epoch: 140, loss: 511.73, accuracy: 77.0%\n",
      "Epoch: 150, loss: 494.85, accuracy: 76.4%\n",
      "Epoch: 160, loss: 505.46, accuracy: 76.8%\n",
      "Epoch: 170, loss: 511.99, accuracy: 77.1%\n",
      "Epoch: 180, loss: 519.91, accuracy: 77.1%\n",
      "Epoch: 190, loss: 528.15, accuracy: 76.7%\n",
      "Epoch: 200, loss: 536.29, accuracy: 76.4%\n",
      "Epoch: 210, loss: 544.25, accuracy: 76.7%\n",
      "Epoch: 220, loss: 551.97, accuracy: 76.8%\n",
      "Epoch: 230, loss: 559.41, accuracy: 77.1%\n",
      "Epoch: 240, loss: 566.52, accuracy: 77.1%\n",
      "Epoch: 250, loss: 573.29, accuracy: 77.2%\n",
      "Epoch: 260, loss: 579.73, accuracy: 77.4%\n",
      "Epoch: 270, loss: 585.85, accuracy: 77.4%\n",
      "Epoch: 280, loss: 591.66, accuracy: 77.5%\n",
      "Epoch: 290, loss: 597.18, accuracy: 77.5%\n",
      "Epoch: 300, loss: 602.43, accuracy: 77.6%\n",
      "Mean accuracy on T4: 72.56\n",
      "Mean losses on T4: 571.29\n",
      "\n",
      "Accuracy list on Ti sets: [72.53285151116951, 72.06314790480361, 68.72481384143671, 73.12527376259307, 72.55950292397661]\n",
      "Losses list on Ti sets: [571.1209069584077, 573.7672916783079, 627.7189116535844, 534.9467331418724, 571.2931494106936]\n",
      "Mean accuracy on all T sets: 71.80\n",
      "Mean losses on all T sets: 575.77\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ szÃ³sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 100, activation: sigmoid\n",
      "2. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 773.11, accuracy: 32.3%\n",
      "Epoch: 20, loss: 758.21, accuracy: 32.3%\n",
      "Epoch: 30, loss: 739.35, accuracy: 32.3%\n",
      "Epoch: 40, loss: 720.14, accuracy: 57.6%\n",
      "Epoch: 50, loss: 788.66, accuracy: 57.6%\n",
      "Epoch: 60, loss: 762.70, accuracy: 57.6%\n",
      "Epoch: 70, loss: 745.16, accuracy: 57.6%\n",
      "Epoch: 80, loss: 733.03, accuracy: 57.6%\n",
      "Epoch: 90, loss: 724.49, accuracy: 57.6%\n",
      "Epoch: 100, loss: 718.29, accuracy: 57.6%\n",
      "Epoch: 110, loss: 713.48, accuracy: 57.6%\n",
      "Epoch: 120, loss: 709.32, accuracy: 57.6%\n",
      "Epoch: 130, loss: 705.17, accuracy: 57.6%\n",
      "Epoch: 140, loss: 700.33, accuracy: 57.6%\n",
      "Epoch: 150, loss: 694.02, accuracy: 57.6%\n",
      "Epoch: 160, loss: 685.31, accuracy: 57.6%\n",
      "Epoch: 170, loss: 673.32, accuracy: 57.8%\n",
      "Epoch: 180, loss: 657.72, accuracy: 58.9%\n",
      "Epoch: 190, loss: 639.37, accuracy: 60.4%\n",
      "Epoch: 200, loss: 620.70, accuracy: 63.9%\n",
      "Epoch: 210, loss: 604.55, accuracy: 66.8%\n",
      "Epoch: 220, loss: 592.24, accuracy: 69.5%\n",
      "Epoch: 230, loss: 583.35, accuracy: 70.8%\n",
      "Epoch: 240, loss: 576.93, accuracy: 71.0%\n",
      "Epoch: 250, loss: 572.16, accuracy: 71.0%\n",
      "Epoch: 260, loss: 568.50, accuracy: 71.2%\n",
      "Epoch: 270, loss: 565.60, accuracy: 71.9%\n",
      "Epoch: 280, loss: 563.27, accuracy: 71.6%\n",
      "Epoch: 290, loss: 561.40, accuracy: 71.6%\n",
      "Epoch: 300, loss: 559.92, accuracy: 71.6%\n",
      "Mean accuracy on T0: 59.02\n",
      "Mean losses on T0: 685.03\n",
      "\n",
      "Current fold: T1,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 789.89, accuracy: 32.3%\n",
      "Epoch: 20, loss: 778.46, accuracy: 32.3%\n",
      "Epoch: 30, loss: 762.77, accuracy: 32.3%\n",
      "Epoch: 40, loss: 747.87, accuracy: 55.2%\n",
      "Epoch: 50, loss: 830.94, accuracy: 55.2%\n",
      "Epoch: 60, loss: 803.62, accuracy: 55.2%\n",
      "Epoch: 70, loss: 784.52, accuracy: 55.2%\n",
      "Epoch: 80, loss: 770.93, accuracy: 55.2%\n",
      "Epoch: 90, loss: 761.29, accuracy: 55.2%\n",
      "Epoch: 100, loss: 754.29, accuracy: 55.2%\n",
      "Epoch: 110, loss: 748.93, accuracy: 55.2%\n",
      "Epoch: 120, loss: 744.39, accuracy: 55.2%\n",
      "Epoch: 130, loss: 740.00, accuracy: 55.2%\n",
      "Epoch: 140, loss: 735.07, accuracy: 55.2%\n",
      "Epoch: 150, loss: 728.79, accuracy: 55.2%\n",
      "Epoch: 160, loss: 720.20, accuracy: 55.2%\n",
      "Epoch: 170, loss: 708.22, accuracy: 55.2%\n",
      "Epoch: 180, loss: 691.98, accuracy: 55.3%\n",
      "Epoch: 190, loss: 671.38, accuracy: 56.6%\n",
      "Epoch: 200, loss: 647.84, accuracy: 60.3%\n",
      "Epoch: 210, loss: 624.15, accuracy: 64.8%\n",
      "Epoch: 220, loss: 602.96, accuracy: 67.8%\n",
      "Epoch: 230, loss: 585.38, accuracy: 71.1%\n",
      "Epoch: 240, loss: 571.25, accuracy: 71.9%\n",
      "Epoch: 250, loss: 559.93, accuracy: 73.1%\n",
      "Epoch: 260, loss: 550.79, accuracy: 74.0%\n",
      "Epoch: 270, loss: 543.30, accuracy: 74.4%\n",
      "Epoch: 280, loss: 537.11, accuracy: 74.1%\n",
      "Epoch: 290, loss: 531.93, accuracy: 74.2%\n",
      "Epoch: 300, loss: 527.58, accuracy: 74.2%\n",
      "Mean accuracy on T1: 58.41\n",
      "Mean losses on T1: 700.94\n",
      "\n",
      "Current fold: T2,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 786.80, accuracy: 31.3%\n",
      "Epoch: 20, loss: 773.66, accuracy: 31.3%\n",
      "Epoch: 30, loss: 756.17, accuracy: 31.3%\n",
      "Epoch: 40, loss: 739.57, accuracy: 57.0%\n",
      "Epoch: 50, loss: 802.17, accuracy: 57.0%\n",
      "Epoch: 60, loss: 776.45, accuracy: 57.0%\n",
      "Epoch: 70, loss: 758.87, accuracy: 57.0%\n",
      "Epoch: 80, loss: 746.63, accuracy: 57.0%\n",
      "Epoch: 90, loss: 738.11, accuracy: 57.0%\n",
      "Epoch: 100, loss: 732.00, accuracy: 57.0%\n",
      "Epoch: 110, loss: 727.29, accuracy: 57.0%\n",
      "Epoch: 120, loss: 723.19, accuracy: 57.0%\n",
      "Epoch: 130, loss: 719.01, accuracy: 57.0%\n",
      "Epoch: 140, loss: 714.05, accuracy: 57.0%\n",
      "Epoch: 150, loss: 707.47, accuracy: 57.0%\n",
      "Epoch: 160, loss: 698.28, accuracy: 57.0%\n",
      "Epoch: 170, loss: 685.48, accuracy: 57.2%\n",
      "Epoch: 180, loss: 668.46, accuracy: 57.7%\n",
      "Epoch: 190, loss: 647.68, accuracy: 59.8%\n",
      "Epoch: 200, loss: 625.23, accuracy: 62.8%\n",
      "Epoch: 210, loss: 604.00, accuracy: 66.9%\n",
      "Epoch: 220, loss: 585.95, accuracy: 68.7%\n",
      "Epoch: 230, loss: 571.43, accuracy: 70.8%\n",
      "Epoch: 240, loss: 560.00, accuracy: 71.0%\n",
      "Epoch: 250, loss: 550.97, accuracy: 71.9%\n",
      "Epoch: 260, loss: 543.77, accuracy: 72.5%\n",
      "Epoch: 270, loss: 537.93, accuracy: 73.2%\n",
      "Epoch: 280, loss: 533.13, accuracy: 73.2%\n",
      "Epoch: 290, loss: 529.12, accuracy: 73.7%\n",
      "Epoch: 300, loss: 525.75, accuracy: 74.0%\n",
      "Mean accuracy on T2: 58.98\n",
      "Mean losses on T2: 686.77\n",
      "\n",
      "Current fold: T3,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 775.97, accuracy: 32.3%\n",
      "Epoch: 20, loss: 754.27, accuracy: 32.3%\n",
      "Epoch: 30, loss: 736.24, accuracy: 32.3%\n",
      "Epoch: 40, loss: 724.05, accuracy: 32.3%\n",
      "Epoch: 50, loss: 714.47, accuracy: 57.7%\n",
      "Epoch: 60, loss: 769.82, accuracy: 57.7%\n",
      "Epoch: 70, loss: 744.89, accuracy: 57.7%\n",
      "Epoch: 80, loss: 728.28, accuracy: 57.7%\n",
      "Epoch: 90, loss: 717.69, accuracy: 57.7%\n",
      "Epoch: 100, loss: 710.81, accuracy: 57.7%\n",
      "Epoch: 110, loss: 705.97, accuracy: 57.7%\n",
      "Epoch: 120, loss: 702.08, accuracy: 57.7%\n",
      "Epoch: 130, loss: 698.28, accuracy: 57.7%\n",
      "Epoch: 140, loss: 693.81, accuracy: 57.7%\n",
      "Epoch: 150, loss: 687.80, accuracy: 57.7%\n",
      "Epoch: 160, loss: 679.21, accuracy: 57.7%\n",
      "Epoch: 170, loss: 666.92, accuracy: 57.7%\n",
      "Epoch: 180, loss: 650.10, accuracy: 58.2%\n",
      "Epoch: 190, loss: 629.09, accuracy: 60.3%\n",
      "Epoch: 200, loss: 606.33, accuracy: 65.3%\n",
      "Epoch: 210, loss: 585.18, accuracy: 69.8%\n",
      "Epoch: 220, loss: 567.42, accuracy: 72.3%\n",
      "Epoch: 230, loss: 553.37, accuracy: 73.2%\n",
      "Epoch: 240, loss: 542.63, accuracy: 73.2%\n",
      "Epoch: 250, loss: 534.55, accuracy: 73.3%\n",
      "Epoch: 260, loss: 528.48, accuracy: 73.7%\n",
      "Epoch: 270, loss: 523.89, accuracy: 74.2%\n",
      "Epoch: 280, loss: 520.41, accuracy: 74.2%\n",
      "Epoch: 290, loss: 517.75, accuracy: 74.0%\n",
      "Epoch: 300, loss: 515.72, accuracy: 74.0%\n",
      "Mean accuracy on T3: 60.30\n",
      "Mean losses on T3: 668.69\n",
      "\n",
      "Current fold: T4,\n",
      "Len of x_train: 3044,\n",
      "Len of x_test: 760.\n",
      "Epoch: 10, loss: 778.54, accuracy: 32.0%\n",
      "Epoch: 20, loss: 780.65, accuracy: 32.0%\n",
      "Epoch: 30, loss: 776.85, accuracy: 32.0%\n",
      "Epoch: 40, loss: 770.79, accuracy: 32.0%\n",
      "Epoch: 50, loss: 764.56, accuracy: 32.0%\n",
      "Epoch: 60, loss: 758.58, accuracy: 32.0%\n",
      "Epoch: 70, loss: 753.14, accuracy: 32.0%\n",
      "Epoch: 80, loss: 748.44, accuracy: 32.0%\n",
      "Epoch: 90, loss: 744.45, accuracy: 32.0%\n",
      "Epoch: 100, loss: 740.98, accuracy: 32.0%\n",
      "Epoch: 110, loss: 737.76, accuracy: 32.0%\n",
      "Epoch: 120, loss: 734.45, accuracy: 42.8%\n",
      "Epoch: 130, loss: 730.64, accuracy: 68.0%\n",
      "Epoch: 140, loss: 725.76, accuracy: 71.1%\n",
      "Epoch: 150, loss: 719.01, accuracy: 71.1%\n",
      "Epoch: 160, loss: 709.44, accuracy: 71.4%\n",
      "Epoch: 170, loss: 696.00, accuracy: 71.6%\n",
      "Epoch: 180, loss: 678.00, accuracy: 71.6%\n",
      "Epoch: 190, loss: 655.84, accuracy: 71.6%\n",
      "Epoch: 200, loss: 631.93, accuracy: 71.2%\n",
      "Epoch: 210, loss: 609.99, accuracy: 70.7%\n",
      "Epoch: 220, loss: 592.21, accuracy: 70.8%\n",
      "Epoch: 230, loss: 578.55, accuracy: 70.9%\n",
      "Epoch: 240, loss: 568.15, accuracy: 70.9%\n",
      "Epoch: 250, loss: 560.12, accuracy: 71.2%\n",
      "Epoch: 260, loss: 553.78, accuracy: 71.1%\n",
      "Epoch: 270, loss: 548.66, accuracy: 71.4%\n",
      "Epoch: 280, loss: 544.46, accuracy: 71.6%\n",
      "Epoch: 290, loss: 540.96, accuracy: 71.6%\n",
      "Epoch: 300, loss: 538.04, accuracy: 71.6%\n",
      "Mean accuracy on T4: 59.32\n",
      "Mean losses on T4: 683.48\n",
      "\n",
      "Accuracy list on Ti sets: [59.019710906701704, 58.4138925390568, 58.97667542706966, 60.29664914586072, 59.31655701754386]\n",
      "Losses list on Ti sets: [685.0283254389401, 700.9399888461104, 686.768397609402, 668.6927319966649, 683.4766299604087]\n",
      "Mean accuracy on all T sets: 59.20\n",
      "Mean losses on all T sets: 684.98\n"
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SieÄ‡ siÃ³dma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 1000, activation: sigmoid\n",
      "2. Layer - input_dim: 1000, output_dim: 100, activation: sigmoid\n",
      "3. Layer - input_dim: 100, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp = net.MLP()\n",
    "\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=1000, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=1000, output_dim=100, activation=\"sigmoid\")\n",
    "\n",
    "mlp.add_layer(input_dim=100, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fold: T0,\n",
      "Len of x_train: 3043,\n",
      "Len of x_test: 761.\n",
      "Epoch: 10, loss: 728.86, accuracy: 56.9%\n",
      "Epoch: 20, loss: 728.06, accuracy: 56.9%\n",
      "Epoch: 30, loss: 727.71, accuracy: 56.9%\n",
      "Epoch: 40, loss: 727.48, accuracy: 56.9%\n",
      "Epoch: 50, loss: 727.30, accuracy: 56.9%\n",
      "Epoch: 60, loss: 727.14, accuracy: 56.9%\n",
      "Epoch: 70, loss: 727.00, accuracy: 56.9%\n",
      "Epoch: 80, loss: 726.87, accuracy: 56.9%\n",
      "Epoch: 90, loss: 726.75, accuracy: 56.9%\n",
      "Epoch: 100, loss: 726.65, accuracy: 56.9%\n",
      "Epoch: 110, loss: 726.56, accuracy: 56.9%\n",
      "Epoch: 120, loss: 726.49, accuracy: 56.9%\n",
      "Epoch: 130, loss: 726.42, accuracy: 56.9%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-07dcfcc32050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_fold_validation_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Dropbox\\version-control\\pszt-projekt\\pszt\\net.py\u001b[0m in \u001b[0;36mk_fold_validation_minibatch\u001b[1;34m(self, x, y_true, k, epochs, batch_size)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m                     \u001b[1;31m# Calculate accuracy of the trained network on test set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m                     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m                     \u001b[0maccuracies_curr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Dropbox\\version-control\\pszt-projekt\\pszt\\net.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, x, y_true, forward)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;31m# Perform forward step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;31m# Get y_pred values from memory.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Dropbox\\version-control\\pszt-projekt\\pszt\\net.py\u001b[0m in \u001b[0;36m_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;31m# Calculate input with weights and perform activation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp.k_fold_validation_minibatch(x_train, y_train, k=5, epochs=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
