{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import MLP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (1000, 8257), y_train.shape: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "print(f'x_train.shape: {x_train.shape}, y_train.shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: relu\n"
     ]
    }
   ],
   "source": [
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=50, activation=\"relu\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Layer - input_dim: 8257, output_dim: 50, activation: relu\n",
      "1. Layer - input_dim: 50, output_dim: 3, activation: softmax\n"
     ]
    }
   ],
   "source": [
    "mlp.add_layer(input_dim=50, output_dim=3, activation=\"softmax\")\n",
    "mlp.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.init_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1290.59, accuracy: 34.2%\n",
      "loss: 26323.13, accuracy: 55.5%\n",
      "loss: 17031.91, accuracy: 12.2%\n",
      "loss: 1936.81, accuracy: 34.0%\n",
      "loss: 1147.97, accuracy: 34.0%\n",
      "loss: 1008.34, accuracy: 60.8%\n",
      "loss: 963.46, accuracy: 59.9%\n",
      "loss: 944.65, accuracy: 59.8%\n",
      "loss: 933.35, accuracy: 59.5%\n",
      "loss: 924.59, accuracy: 59.4%\n",
      "loss: 916.73, accuracy: 59.8%\n",
      "loss: 909.22, accuracy: 60.3%\n",
      "loss: 901.73, accuracy: 60.4%\n",
      "loss: 894.04, accuracy: 60.7%\n",
      "loss: 885.64, accuracy: 60.8%\n",
      "loss: 875.49, accuracy: 61.6%\n",
      "loss: 863.47, accuracy: 62.1%\n",
      "loss: 847.48, accuracy: 62.5%\n",
      "loss: 828.72, accuracy: 63.0%\n",
      "loss: 809.82, accuracy: 63.9%\n",
      "loss: 792.77, accuracy: 65.4%\n",
      "loss: 777.43, accuracy: 67.4%\n",
      "loss: 763.96, accuracy: 68.2%\n",
      "loss: 751.77, accuracy: 70.0%\n",
      "loss: 740.71, accuracy: 71.3%\n",
      "loss: 730.32, accuracy: 71.8%\n",
      "loss: 720.40, accuracy: 72.3%\n",
      "loss: 711.20, accuracy: 73.6%\n",
      "loss: 702.21, accuracy: 73.6%\n",
      "loss: 693.53, accuracy: 73.8%\n",
      "loss: 685.23, accuracy: 74.2%\n",
      "loss: 677.09, accuracy: 74.8%\n",
      "loss: 668.94, accuracy: 75.2%\n",
      "loss: 660.94, accuracy: 75.7%\n",
      "loss: 652.85, accuracy: 76.1%\n",
      "loss: 644.70, accuracy: 76.4%\n",
      "loss: 636.64, accuracy: 76.8%\n",
      "loss: 628.59, accuracy: 77.4%\n",
      "loss: 620.29, accuracy: 77.8%\n",
      "loss: 612.01, accuracy: 77.9%\n",
      "loss: 603.46, accuracy: 78.3%\n",
      "loss: 595.42, accuracy: 78.9%\n",
      "loss: 588.17, accuracy: 79.3%\n",
      "loss: 583.21, accuracy: 78.9%\n",
      "loss: 586.85, accuracy: 79.0%\n",
      "loss: 631.10, accuracy: 75.9%\n",
      "loss: 790.21, accuracy: 63.8%\n",
      "loss: 1153.10, accuracy: 61.3%\n",
      "loss: 888.73, accuracy: 59.1%\n",
      "loss: 611.16, accuracy: 77.1%\n",
      "loss: 594.91, accuracy: 78.0%\n",
      "loss: 580.61, accuracy: 78.8%\n",
      "loss: 568.22, accuracy: 79.2%\n",
      "loss: 557.28, accuracy: 79.8%\n",
      "loss: 547.42, accuracy: 80.5%\n",
      "loss: 538.34, accuracy: 80.6%\n",
      "loss: 529.83, accuracy: 80.8%\n",
      "loss: 521.49, accuracy: 81.0%\n",
      "loss: 513.38, accuracy: 81.3%\n",
      "loss: 506.11, accuracy: 81.9%\n",
      "loss: 499.07, accuracy: 82.2%\n",
      "loss: 492.41, accuracy: 82.4%\n",
      "loss: 486.10, accuracy: 82.6%\n",
      "loss: 480.94, accuracy: 82.9%\n",
      "loss: 481.85, accuracy: 83.4%\n",
      "loss: 514.27, accuracy: 80.4%\n",
      "loss: 660.00, accuracy: 72.3%\n",
      "loss: 1188.13, accuracy: 63.0%\n",
      "loss: 1021.10, accuracy: 50.2%\n",
      "loss: 555.75, accuracy: 77.9%\n",
      "loss: 530.16, accuracy: 79.7%\n",
      "loss: 510.53, accuracy: 80.6%\n",
      "loss: 493.73, accuracy: 81.0%\n",
      "loss: 480.92, accuracy: 82.3%\n",
      "loss: 470.33, accuracy: 82.6%\n",
      "loss: 460.32, accuracy: 83.2%\n",
      "loss: 451.85, accuracy: 83.5%\n",
      "loss: 444.39, accuracy: 83.7%\n",
      "loss: 437.96, accuracy: 83.8%\n",
      "loss: 432.81, accuracy: 83.3%\n",
      "loss: 430.18, accuracy: 84.6%\n",
      "loss: 435.72, accuracy: 83.3%\n",
      "loss: 464.61, accuracy: 83.4%\n",
      "loss: 568.37, accuracy: 78.2%\n",
      "loss: 750.63, accuracy: 65.8%\n",
      "loss: 967.40, accuracy: 67.6%\n",
      "loss: 790.90, accuracy: 58.7%\n",
      "loss: 470.20, accuracy: 81.5%\n",
      "loss: 447.90, accuracy: 82.0%\n",
      "loss: 432.07, accuracy: 82.3%\n",
      "loss: 419.44, accuracy: 83.0%\n",
      "loss: 409.76, accuracy: 84.6%\n",
      "loss: 401.34, accuracy: 84.7%\n",
      "loss: 394.75, accuracy: 85.3%\n",
      "loss: 388.90, accuracy: 85.4%\n",
      "loss: 383.50, accuracy: 85.9%\n",
      "loss: 378.33, accuracy: 85.9%\n",
      "loss: 374.73, accuracy: 86.5%\n",
      "loss: 373.71, accuracy: 85.0%\n",
      "loss: 383.92, accuracy: 87.1%\n",
      "loss: 420.05, accuracy: 83.2%\n",
      "loss: 505.07, accuracy: 80.2%\n",
      "loss: 743.68, accuracy: 73.2%\n",
      "loss: 875.85, accuracy: 58.8%\n",
      "loss: 615.21, accuracy: 74.2%\n",
      "loss: 563.78, accuracy: 75.6%\n",
      "loss: 404.82, accuracy: 83.7%\n",
      "loss: 383.43, accuracy: 85.0%\n",
      "loss: 369.21, accuracy: 86.0%\n",
      "loss: 358.04, accuracy: 87.7%\n",
      "loss: 350.48, accuracy: 86.9%\n",
      "loss: 344.89, accuracy: 88.7%\n",
      "loss: 341.72, accuracy: 87.2%\n",
      "loss: 343.30, accuracy: 88.8%\n",
      "loss: 349.95, accuracy: 86.4%\n",
      "loss: 370.89, accuracy: 87.6%\n",
      "loss: 406.00, accuracy: 84.5%\n",
      "loss: 464.24, accuracy: 81.4%\n",
      "loss: 565.44, accuracy: 78.2%\n",
      "loss: 630.19, accuracy: 70.8%\n",
      "loss: 568.43, accuracy: 76.9%\n",
      "loss: 537.73, accuracy: 75.6%\n",
      "loss: 370.88, accuracy: 85.4%\n",
      "loss: 344.48, accuracy: 88.6%\n",
      "loss: 324.67, accuracy: 88.5%\n",
      "loss: 314.93, accuracy: 90.0%\n",
      "loss: 307.92, accuracy: 89.3%\n",
      "loss: 302.06, accuracy: 90.6%\n",
      "loss: 297.79, accuracy: 89.5%\n",
      "loss: 294.66, accuracy: 91.2%\n",
      "loss: 292.64, accuracy: 89.4%\n",
      "loss: 291.65, accuracy: 91.4%\n",
      "loss: 293.37, accuracy: 89.1%\n",
      "loss: 296.11, accuracy: 91.2%\n",
      "loss: 305.29, accuracy: 88.6%\n",
      "loss: 322.62, accuracy: 90.1%\n",
      "loss: 359.29, accuracy: 86.0%\n",
      "loss: 391.46, accuracy: 85.4%\n",
      "loss: 474.04, accuracy: 82.4%\n",
      "loss: 515.82, accuracy: 76.3%\n",
      "loss: 532.05, accuracy: 78.5%\n",
      "loss: 503.86, accuracy: 76.8%\n",
      "loss: 340.48, accuracy: 87.2%\n",
      "loss: 303.75, accuracy: 90.6%\n",
      "loss: 272.97, accuracy: 90.5%\n",
      "loss: 262.13, accuracy: 92.3%\n",
      "loss: 256.05, accuracy: 91.5%\n",
      "loss: 251.12, accuracy: 92.3%\n",
      "loss: 246.83, accuracy: 92.0%\n",
      "loss: 242.60, accuracy: 92.2%\n",
      "loss: 239.78, accuracy: 92.1%\n",
      "loss: 237.97, accuracy: 92.4%\n",
      "loss: 235.43, accuracy: 91.7%\n",
      "loss: 234.76, accuracy: 92.7%\n",
      "loss: 236.05, accuracy: 91.5%\n",
      "loss: 237.68, accuracy: 92.6%\n",
      "loss: 245.67, accuracy: 90.9%\n",
      "loss: 251.90, accuracy: 92.3%\n",
      "loss: 280.57, accuracy: 88.9%\n",
      "loss: 294.81, accuracy: 90.5%\n",
      "loss: 359.20, accuracy: 86.2%\n",
      "loss: 361.33, accuracy: 86.7%\n",
      "loss: 360.76, accuracy: 86.0%\n",
      "loss: 334.20, accuracy: 88.6%\n",
      "loss: 287.46, accuracy: 88.9%\n",
      "loss: 257.25, accuracy: 91.9%\n",
      "loss: 222.85, accuracy: 92.1%\n",
      "loss: 213.01, accuracy: 93.6%\n",
      "loss: 206.49, accuracy: 93.3%\n",
      "loss: 203.81, accuracy: 93.7%\n",
      "loss: 203.64, accuracy: 93.2%\n",
      "loss: 206.11, accuracy: 93.8%\n",
      "loss: 221.04, accuracy: 91.6%\n",
      "loss: 220.49, accuracy: 93.4%\n",
      "loss: 245.80, accuracy: 90.4%\n",
      "loss: 240.61, accuracy: 92.1%\n",
      "loss: 268.22, accuracy: 89.6%\n",
      "loss: 260.99, accuracy: 92.0%\n",
      "loss: 284.58, accuracy: 89.0%\n",
      "loss: 257.22, accuracy: 92.0%\n",
      "loss: 237.98, accuracy: 91.5%\n",
      "loss: 215.12, accuracy: 93.3%\n",
      "loss: 201.54, accuracy: 92.5%\n",
      "loss: 185.74, accuracy: 94.2%\n",
      "loss: 178.27, accuracy: 94.2%\n",
      "loss: 172.95, accuracy: 94.9%\n",
      "loss: 178.01, accuracy: 93.9%\n",
      "loss: 180.47, accuracy: 94.8%\n",
      "loss: 206.16, accuracy: 91.9%\n",
      "loss: 201.68, accuracy: 94.1%\n",
      "loss: 248.18, accuracy: 90.4%\n",
      "loss: 241.14, accuracy: 92.7%\n",
      "loss: 299.17, accuracy: 88.6%\n",
      "loss: 284.84, accuracy: 90.3%\n",
      "loss: 310.58, accuracy: 88.0%\n",
      "loss: 268.96, accuracy: 90.6%\n",
      "loss: 244.42, accuracy: 90.9%\n",
      "loss: 194.35, accuracy: 93.5%\n",
      "loss: 160.43, accuracy: 94.8%\n",
      "loss: 149.72, accuracy: 95.7%\n",
      "loss: 145.23, accuracy: 95.5%\n",
      "loss: 142.51, accuracy: 95.9%\n",
      "loss: 140.69, accuracy: 95.5%\n",
      "loss: 139.15, accuracy: 96.4%\n",
      "loss: 141.28, accuracy: 95.4%\n",
      "loss: 144.42, accuracy: 96.5%\n",
      "loss: 161.20, accuracy: 93.7%\n",
      "loss: 165.17, accuracy: 95.2%\n",
      "loss: 211.80, accuracy: 91.7%\n",
      "loss: 189.74, accuracy: 94.7%\n",
      "loss: 232.47, accuracy: 91.0%\n",
      "loss: 192.96, accuracy: 94.3%\n",
      "loss: 178.27, accuracy: 92.8%\n",
      "loss: 144.59, accuracy: 96.0%\n",
      "loss: 132.93, accuracy: 95.8%\n",
      "loss: 123.86, accuracy: 96.8%\n",
      "loss: 121.66, accuracy: 96.8%\n",
      "loss: 118.38, accuracy: 96.9%\n",
      "loss: 117.98, accuracy: 96.6%\n",
      "loss: 117.17, accuracy: 96.8%\n",
      "loss: 119.70, accuracy: 96.3%\n",
      "loss: 121.15, accuracy: 96.9%\n",
      "loss: 134.06, accuracy: 95.2%\n",
      "loss: 137.39, accuracy: 96.6%\n",
      "loss: 168.02, accuracy: 93.0%\n",
      "loss: 153.23, accuracy: 95.8%\n",
      "loss: 180.97, accuracy: 92.6%\n",
      "loss: 162.08, accuracy: 95.3%\n",
      "loss: 171.91, accuracy: 93.1%\n",
      "loss: 132.28, accuracy: 96.5%\n",
      "loss: 114.20, accuracy: 96.6%\n",
      "loss: 103.26, accuracy: 97.2%\n",
      "loss: 99.23, accuracy: 97.4%\n",
      "loss: 97.14, accuracy: 97.6%\n",
      "loss: 96.04, accuracy: 97.4%\n",
      "loss: 95.02, accuracy: 97.5%\n",
      "loss: 95.18, accuracy: 97.2%\n",
      "loss: 95.09, accuracy: 97.4%\n",
      "loss: 97.23, accuracy: 97.0%\n",
      "loss: 98.77, accuracy: 97.3%\n",
      "loss: 103.64, accuracy: 96.5%\n",
      "loss: 105.97, accuracy: 97.1%\n",
      "loss: 118.87, accuracy: 95.5%\n",
      "loss: 117.06, accuracy: 96.6%\n",
      "loss: 134.35, accuracy: 95.1%\n",
      "loss: 127.62, accuracy: 96.4%\n",
      "loss: 141.94, accuracy: 94.8%\n",
      "loss: 117.63, accuracy: 96.7%\n",
      "loss: 106.77, accuracy: 96.6%\n",
      "loss: 89.56, accuracy: 97.5%\n",
      "loss: 84.72, accuracy: 97.5%\n",
      "loss: 81.76, accuracy: 98.0%\n",
      "loss: 79.94, accuracy: 97.7%\n",
      "loss: 78.52, accuracy: 97.9%\n",
      "loss: 77.46, accuracy: 97.7%\n",
      "loss: 76.81, accuracy: 98.0%\n",
      "loss: 76.30, accuracy: 97.8%\n",
      "loss: 76.04, accuracy: 98.0%\n",
      "loss: 76.46, accuracy: 97.7%\n",
      "loss: 76.21, accuracy: 98.0%\n",
      "loss: 77.02, accuracy: 97.5%\n",
      "loss: 76.32, accuracy: 98.1%\n",
      "loss: 78.02, accuracy: 97.7%\n",
      "loss: 77.38, accuracy: 98.3%\n",
      "loss: 78.84, accuracy: 97.6%\n",
      "loss: 78.07, accuracy: 98.3%\n",
      "loss: 79.34, accuracy: 97.6%\n",
      "loss: 76.23, accuracy: 98.3%\n",
      "loss: 74.96, accuracy: 97.7%\n",
      "loss: 72.30, accuracy: 98.4%\n",
      "loss: 71.34, accuracy: 97.8%\n",
      "loss: 68.31, accuracy: 98.6%\n",
      "loss: 66.87, accuracy: 98.2%\n",
      "loss: 65.45, accuracy: 98.7%\n",
      "loss: 65.04, accuracy: 98.3%\n",
      "loss: 64.48, accuracy: 98.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 64.41, accuracy: 98.2%\n",
      "loss: 64.30, accuracy: 98.6%\n",
      "loss: 65.33, accuracy: 98.2%\n",
      "loss: 66.69, accuracy: 98.6%\n",
      "loss: 68.23, accuracy: 97.7%\n",
      "loss: 69.09, accuracy: 98.6%\n",
      "loss: 69.10, accuracy: 97.8%\n",
      "loss: 66.22, accuracy: 98.6%\n",
      "loss: 63.77, accuracy: 98.3%\n",
      "loss: 60.62, accuracy: 98.7%\n",
      "loss: 59.34, accuracy: 98.6%\n",
      "loss: 58.27, accuracy: 98.9%\n",
      "loss: 57.65, accuracy: 98.7%\n",
      "loss: 56.83, accuracy: 98.9%\n",
      "loss: 57.09, accuracy: 98.6%\n",
      "loss: 56.59, accuracy: 98.9%\n",
      "loss: 57.23, accuracy: 98.5%\n",
      "loss: 58.55, accuracy: 98.8%\n",
      "loss: 58.29, accuracy: 98.4%\n",
      "loss: 56.81, accuracy: 98.8%\n",
      "loss: 57.09, accuracy: 98.5%\n",
      "loss: 56.05, accuracy: 98.9%\n",
      "loss: 55.33, accuracy: 98.6%\n",
      "loss: 54.09, accuracy: 98.9%\n",
      "loss: 53.59, accuracy: 98.7%\n",
      "loss: 52.57, accuracy: 98.9%\n",
      "loss: 52.02, accuracy: 98.8%\n",
      "loss: 51.30, accuracy: 98.9%\n",
      "loss: 51.10, accuracy: 98.8%\n",
      "loss: 50.63, accuracy: 98.9%\n",
      "loss: 50.42, accuracy: 98.8%\n",
      "loss: 50.17, accuracy: 99.0%\n",
      "loss: 50.07, accuracy: 98.8%\n",
      "loss: 49.74, accuracy: 99.0%\n",
      "loss: 49.50, accuracy: 98.8%\n",
      "loss: 49.27, accuracy: 99.0%\n",
      "loss: 48.90, accuracy: 98.8%\n",
      "loss: 48.55, accuracy: 99.0%\n",
      "loss: 48.17, accuracy: 98.8%\n",
      "loss: 47.85, accuracy: 99.0%\n",
      "loss: 47.57, accuracy: 98.9%\n",
      "loss: 46.96, accuracy: 99.0%\n",
      "loss: 46.42, accuracy: 99.0%\n",
      "loss: 46.20, accuracy: 99.0%\n",
      "loss: 45.76, accuracy: 99.0%\n",
      "loss: 45.31, accuracy: 99.0%\n",
      "loss: 45.13, accuracy: 99.0%\n",
      "loss: 44.96, accuracy: 99.0%\n",
      "loss: 44.77, accuracy: 99.0%\n",
      "loss: 44.72, accuracy: 99.0%\n",
      "loss: 44.53, accuracy: 99.0%\n",
      "loss: 44.60, accuracy: 99.0%\n",
      "loss: 44.22, accuracy: 99.0%\n",
      "loss: 44.41, accuracy: 99.0%\n",
      "loss: 44.21, accuracy: 99.0%\n",
      "loss: 44.08, accuracy: 99.0%\n",
      "loss: 43.37, accuracy: 99.0%\n",
      "loss: 43.42, accuracy: 99.0%\n",
      "loss: 42.78, accuracy: 99.0%\n",
      "loss: 42.41, accuracy: 99.0%\n",
      "loss: 42.27, accuracy: 99.0%\n",
      "loss: 42.73, accuracy: 99.0%\n",
      "loss: 42.72, accuracy: 99.0%\n",
      "loss: 43.31, accuracy: 99.0%\n",
      "loss: 43.05, accuracy: 99.0%\n",
      "loss: 42.87, accuracy: 99.0%\n",
      "loss: 41.66, accuracy: 99.0%\n",
      "loss: 41.08, accuracy: 99.0%\n",
      "loss: 40.61, accuracy: 99.0%\n",
      "loss: 40.52, accuracy: 99.0%\n",
      "loss: 40.20, accuracy: 99.0%\n",
      "loss: 40.46, accuracy: 99.0%\n",
      "loss: 40.28, accuracy: 99.0%\n",
      "loss: 40.50, accuracy: 99.0%\n",
      "loss: 40.25, accuracy: 99.0%\n",
      "loss: 40.55, accuracy: 99.0%\n",
      "loss: 40.29, accuracy: 99.0%\n",
      "loss: 40.21, accuracy: 99.0%\n",
      "loss: 39.62, accuracy: 99.0%\n",
      "loss: 39.10, accuracy: 99.0%\n",
      "loss: 38.62, accuracy: 99.0%\n",
      "loss: 38.36, accuracy: 99.0%\n",
      "loss: 38.07, accuracy: 99.0%\n",
      "loss: 38.06, accuracy: 99.0%\n",
      "loss: 37.88, accuracy: 99.0%\n",
      "loss: 38.03, accuracy: 99.0%\n",
      "loss: 37.75, accuracy: 99.0%\n",
      "loss: 38.08, accuracy: 99.0%\n",
      "loss: 37.80, accuracy: 99.0%\n",
      "loss: 37.56, accuracy: 99.0%\n",
      "loss: 37.14, accuracy: 99.0%\n",
      "loss: 37.29, accuracy: 99.0%\n",
      "loss: 37.05, accuracy: 99.0%\n",
      "loss: 36.79, accuracy: 99.0%\n",
      "loss: 36.50, accuracy: 99.0%\n",
      "loss: 36.69, accuracy: 99.0%\n",
      "loss: 36.31, accuracy: 99.0%\n",
      "loss: 36.34, accuracy: 99.0%\n",
      "loss: 35.97, accuracy: 99.0%\n",
      "loss: 36.01, accuracy: 99.0%\n",
      "loss: 35.75, accuracy: 99.0%\n",
      "loss: 35.96, accuracy: 99.0%\n",
      "loss: 35.51, accuracy: 99.0%\n",
      "loss: 35.33, accuracy: 99.0%\n",
      "loss: 35.03, accuracy: 99.0%\n",
      "loss: 34.80, accuracy: 99.0%\n",
      "loss: 34.57, accuracy: 99.0%\n",
      "loss: 34.47, accuracy: 99.1%\n",
      "loss: 34.32, accuracy: 99.0%\n",
      "loss: 34.38, accuracy: 99.1%\n",
      "loss: 34.21, accuracy: 99.1%\n",
      "loss: 34.45, accuracy: 99.1%\n",
      "loss: 34.34, accuracy: 99.1%\n",
      "loss: 34.49, accuracy: 99.1%\n",
      "loss: 33.99, accuracy: 99.1%\n",
      "loss: 33.72, accuracy: 99.1%\n",
      "loss: 33.40, accuracy: 99.1%\n",
      "loss: 33.21, accuracy: 99.1%\n",
      "loss: 33.03, accuracy: 99.1%\n",
      "loss: 32.99, accuracy: 99.1%\n",
      "loss: 32.86, accuracy: 99.1%\n",
      "loss: 33.07, accuracy: 99.1%\n",
      "loss: 32.83, accuracy: 99.1%\n",
      "loss: 32.89, accuracy: 99.1%\n",
      "loss: 32.55, accuracy: 99.1%\n",
      "loss: 32.54, accuracy: 99.1%\n",
      "loss: 32.24, accuracy: 99.1%\n",
      "loss: 32.02, accuracy: 99.1%\n",
      "loss: 31.83, accuracy: 99.1%\n",
      "loss: 31.72, accuracy: 99.1%\n",
      "loss: 31.63, accuracy: 99.1%\n",
      "loss: 31.67, accuracy: 99.1%\n",
      "loss: 31.79, accuracy: 99.1%\n",
      "loss: 32.09, accuracy: 99.1%\n",
      "loss: 31.68, accuracy: 99.1%\n",
      "loss: 31.59, accuracy: 99.1%\n",
      "loss: 31.26, accuracy: 99.1%\n",
      "loss: 31.05, accuracy: 99.1%\n",
      "loss: 30.86, accuracy: 99.1%\n",
      "loss: 30.78, accuracy: 99.1%\n",
      "loss: 30.62, accuracy: 99.1%\n",
      "loss: 30.59, accuracy: 99.1%\n",
      "loss: 30.49, accuracy: 99.1%\n",
      "loss: 30.80, accuracy: 99.1%\n",
      "loss: 30.55, accuracy: 99.1%\n",
      "loss: 30.40, accuracy: 99.1%\n",
      "loss: 30.15, accuracy: 99.1%\n",
      "loss: 29.99, accuracy: 99.1%\n",
      "loss: 29.84, accuracy: 99.1%\n",
      "loss: 29.73, accuracy: 99.1%\n",
      "loss: 29.60, accuracy: 99.1%\n",
      "loss: 29.73, accuracy: 99.1%\n",
      "loss: 29.59, accuracy: 99.1%\n",
      "loss: 29.48, accuracy: 99.1%\n",
      "loss: 29.37, accuracy: 99.1%\n",
      "loss: 29.51, accuracy: 99.1%\n",
      "loss: 29.36, accuracy: 99.1%\n",
      "loss: 29.37, accuracy: 99.1%\n",
      "loss: 29.16, accuracy: 99.1%\n",
      "loss: 29.25, accuracy: 99.1%\n",
      "loss: 28.98, accuracy: 99.1%\n",
      "loss: 28.71, accuracy: 99.1%\n",
      "loss: 28.59, accuracy: 99.1%\n",
      "loss: 28.56, accuracy: 99.1%\n",
      "loss: 28.43, accuracy: 99.1%\n",
      "loss: 28.39, accuracy: 99.1%\n",
      "loss: 28.30, accuracy: 99.1%\n",
      "loss: 28.28, accuracy: 99.1%\n",
      "loss: 28.19, accuracy: 99.1%\n",
      "loss: 28.26, accuracy: 99.1%\n",
      "loss: 28.17, accuracy: 99.1%\n",
      "loss: 28.42, accuracy: 99.1%\n",
      "loss: 28.09, accuracy: 99.1%\n",
      "loss: 27.87, accuracy: 99.1%\n",
      "loss: 27.71, accuracy: 99.1%\n",
      "loss: 27.59, accuracy: 99.1%\n",
      "loss: 27.47, accuracy: 99.1%\n",
      "loss: 27.45, accuracy: 99.2%\n",
      "loss: 27.36, accuracy: 99.2%\n",
      "loss: 27.33, accuracy: 99.2%\n",
      "loss: 27.24, accuracy: 99.2%\n",
      "loss: 27.31, accuracy: 99.2%\n",
      "loss: 27.30, accuracy: 99.2%\n",
      "loss: 27.39, accuracy: 99.2%\n",
      "loss: 27.18, accuracy: 99.2%\n",
      "loss: 26.92, accuracy: 99.2%\n",
      "loss: 26.81, accuracy: 99.2%\n",
      "loss: 26.83, accuracy: 99.2%\n",
      "loss: 26.71, accuracy: 99.2%\n",
      "loss: 26.71, accuracy: 99.2%\n",
      "loss: 26.63, accuracy: 99.2%\n",
      "loss: 26.66, accuracy: 99.2%\n",
      "loss: 26.61, accuracy: 99.2%\n",
      "loss: 26.62, accuracy: 99.2%\n",
      "loss: 26.41, accuracy: 99.2%\n",
      "loss: 26.32, accuracy: 99.2%\n",
      "loss: 26.18, accuracy: 99.2%\n",
      "loss: 26.09, accuracy: 99.2%\n",
      "loss: 25.99, accuracy: 99.2%\n",
      "loss: 25.92, accuracy: 99.2%\n",
      "loss: 25.84, accuracy: 99.2%\n",
      "loss: 25.89, accuracy: 99.2%\n",
      "loss: 25.84, accuracy: 99.2%\n",
      "loss: 25.81, accuracy: 99.2%\n",
      "loss: 25.70, accuracy: 99.2%\n",
      "loss: 25.73, accuracy: 99.2%\n",
      "loss: 25.61, accuracy: 99.3%\n",
      "loss: 25.53, accuracy: 99.3%\n",
      "loss: 25.43, accuracy: 99.3%\n",
      "loss: 25.53, accuracy: 99.3%\n",
      "loss: 25.37, accuracy: 99.3%\n",
      "loss: 25.36, accuracy: 99.3%\n",
      "loss: 25.25, accuracy: 99.3%\n",
      "loss: 25.16, accuracy: 99.3%\n",
      "loss: 25.08, accuracy: 99.3%\n",
      "loss: 25.12, accuracy: 99.3%\n",
      "loss: 25.03, accuracy: 99.3%\n",
      "loss: 25.00, accuracy: 99.3%\n",
      "loss: 24.90, accuracy: 99.3%\n",
      "loss: 24.88, accuracy: 99.3%\n",
      "loss: 24.77, accuracy: 99.3%\n",
      "loss: 24.70, accuracy: 99.3%\n",
      "loss: 24.64, accuracy: 99.3%\n",
      "loss: 24.81, accuracy: 99.3%\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    mlp.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.74459323e-05, 9.99336295e-01, 6.36258903e-04],\n",
       "       [2.47580990e-06, 9.99916927e-01, 8.05969859e-05],\n",
       "       [7.54377005e-04, 9.95981840e-01, 3.26378272e-03],\n",
       "       ...,\n",
       "       [1.41313455e-06, 9.99821758e-01, 1.76828462e-04],\n",
       "       [9.99834007e-01, 1.31701984e-04, 3.42908926e-05],\n",
       "       [1.26103568e-07, 9.99204022e-01, 7.95851725e-04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.memory[\"a2\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
