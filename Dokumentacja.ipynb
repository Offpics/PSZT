{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dokumentacja projektu Sentiment Classification\n",
    "#### Aleksander Ogonowski\n",
    "   \n",
    "#### Paweł Rybak\n",
    "\n",
    "#### Kornel Szymczyk\n",
    "\n",
    "##### 10 stycznia 2019 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instrukcja użytkownika\n",
    "\n",
    "Pakiet pszt zawiera dwa moduły:\n",
    "- refactor_csv.py zawierający funkcje pomocnicze do refaktoryzacji zbioru danych Tweetów dotyczących produktów Apple,\n",
    "- net.py zawierający klasę MLP, która jest definicją modelu sieci neuronowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Wymagania uruchomienia\n",
    "- Python 3.6+\n",
    "- Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Przekształcenie zbioru danych wykorzystując moduł refactor_csv.py\n",
    "\n",
    "Przed przystąpieniem do trenowania sieci neuronowej należy przekrzstałcić odpowiednio zbiór danych i zapisać go w nowym pliku csv. W tym celu korzystamy z funkcji *refactor(csv_path, new_csv_path)*. Pierwszy argument funkcji to ścieżka do pliku ze zbiorem danych w formacie *.csv*, drugi argument to ścieżka pliku, w którym zostanie zapisany przekształcony zbiór. \n",
    "\n",
    "Przykład kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pszt import refactor_csv\n",
    "\n",
    "# File path to the csv file.\n",
    "csv_path = 'Apple-Twitter-Sentiment-DFE.csv'\n",
    "\n",
    "# File path to the new csv file.\n",
    "new_csv_path = 'apple-twitter_example.csv'\n",
    "    \n",
    "refactor_csv.refactor(csv_path, new_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posiadając przekształcony zbiór plik csv należy przystąpić do wektoryzacji zbioru, w tym celu należy skorzystać z funkcji *vectorize_dataset(csv_path, x_train_path, y_train_path, ignore_words=None)*. Jej celem jest wykonanie przekształcenia zdań do modelu bag-of-words oraz zamiana sentymentu zdań (1, 3, 5) do postaci one-hot. Przekształcone zdania zapisywane są w tablicy numpy *x_train*, etykiety w tablicy numpy *y_train* oraz zapisywane są na dysku w formacie *.npy*. Pierwszy argument funkcji to ścieżka do pliku csv utworzonego przy pomocy funkcji *refactor()*, drugi argument to ścieżka pliku do zapisania tablicy *x_train*, trzeci argument to ścieżka pliku do zapisania tablicy *y_train* oraz ostatni argument przyjmuje opcjonalną tablicę słów, które mają być pomijane w zdaniach.\n",
    "\n",
    "Przykład kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_csv.vectorize_dataset(new_csv_path, 'x_train_no_ignore_no_norm', 'y_train_no_ignore_no_norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Trenowanie sieci neuronowej wykorzystując moduł net.py\n",
    "\n",
    "W celu wytrenowania sieci tworzymy obiekt klasy MLP. Następnie dodajemy warstwy sieci wykorzystując metodę *add_layer(input_dim, output_dim, activation)*. Pierwszy argument określa liczbę wejść do warstwy, drugi argument określa liczbę wyjść z warstwy, a ostatni określa rodzaj funkcji aktywacji ('relu', 'sigmoid', 'softmax'). Ze względu na to, że etykiety zbioru reprezentowane są w postaci one-hot, ostatnia warstwa powinna zawsze posiadać output_dim=3 oraz activation='softmax'. Model sieci został napisany w taki sposób, że należy pominąć dodawanie warstwy wejściowej sieci neurnowej, a input_dim pierwszej warstwy powinien odpowiadać długości wektora danych wejściowych. Po dodaniu warstw należy je zainicjalizować funkcją *init_layers()*.\n",
    "\n",
    "Proces trenowania wywołuje się funkcją *train(x, y_true, epochs, silent=False)*. Pierwszy argument to tablica numpy ze zdaniami w postaci bag-of-words, drugi argument to tablicy etykiet zdań w postaci one-hot, trzeci argument określa liczbę kroków trenowania, czwarty agrument opcjonalny określający czy wypisywać na ekran aktualny status trenowania sieci.\n",
    "\n",
    "Przykład kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pszt import net\n",
    "\n",
    "# Create a neural net.\n",
    "mlp = net.MLP();\n",
    "\n",
    "# Load dataset.\n",
    "x_train = np.load('x_train_3k.npy')\n",
    "y_train = np.load('y_train_3k.npy')\n",
    "\n",
    "# Add layers.\n",
    "mlp.add_layer(input_dim=x_train.shape[1], output_dim=20, activation=\"sigmoid\")\n",
    "mlp.add_layer(input_dim=20, output_dim=3, activation=\"softmax\")\n",
    "\n",
    "# Initialize weights in layers.\n",
    "mlp.init_layers()\n",
    "\n",
    "# Train the neural net.\n",
    "mlp.train(x=x_train, y_true=y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa MLP zawiera również metodę *k_fold_validation(x, y_true, k, epochs)*, która poddaje sieć neuronową walidacji k-fold.\n",
    "\n",
    "Przykład kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.k_fold_validation(x=x_train, y_true=y_train, k=5, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Struktura programu\n",
    "\n",
    "Najważniejszym elementem programu jest klasa MLP reprezentująca sieć neuronową. Ogólna idea schematu klasy MLP i przechowywania obliczeń sieci została oparta na podstawie artykułu [\"Let’s code a Neural Network in plain NumPy\"](https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795). Klasa ta tworzy pustą sieć. Należy do niej dodać warstwy przy pomocy metody *add_layer*. Dodaje ona jedynie informacje o nowej warstwie do metadanych sieci. Aby sieć faktycznie wykorzystała dodaną warstwę należy wywołać metodę *init_layers*. Dla każdej warstwy opisanej w strukturze sieci utworzy ona odpowiadającą macierz wag i przesunięć. Wartości dla wag warstw ukrytych są losowane z rozkładu jednostajnego. Przedstawia to poniższy fragment kodu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.param_values['w' + str(i)] = np.random.uniform(\n",
    "    low=(-1/np.sqrt(input_size)),\n",
    "    high=(1/np.sqrt(input_size)),\n",
    "    size=(input_size, output_size)\n",
    ")\n",
    "self.param_values['b' + str(i)] = np.random.uniform(\n",
    "    low=(-1/np.sqrt(input_size)),\n",
    "    high=(1/np.sqrt(input_size)),\n",
    "    size=output_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wagi warstwy wyjściowej inicjowane są wartością 0.\n",
    "\n",
    "Klasa ta posiada również funkcję *_forward()*, która wylicza wektor wyjściowy funkcji, w zależności od podanego wektora wejściowego. Dla każdej warstwy sieci wylicza wyjście i funkcję aktywacji, która następnie jest podawane jako wejście do kolejnej warstwy. Każda z pośrednich wartości takiego przejścia jest zapisywana w pamięci sieci. Jest to potrzebne do późniejszej wstecznej propagacji w procesie uczenia sieci. Wspomniana wsteczna propagacja została zaimplementowana w funkcji *_backward()*. Funkcja ta wylicza gradient funkcji kosztu względem wag sieci. Wyliczone gradienty są zapisywane wewnątrz sieci, aby potem mogły być użyte wewnątrz funkcji *_update_weights()*. Modyfikuje ona wagi sieci o wartości proporcjonalne do gradientu. Obliczenia wykonywane przez funkcję *_forward()*, *_backward()* i *_update_weights()* zostały napisane na podstawie artykułu [\"Creating a Neural Network from Scratch in Python: Multi-class Classification\"](https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/).\n",
    "\n",
    "Uczenie sieci można zainicjować przy pomocy metody *train*, której najważniejszy fragment został przytoczony poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, x, y_true, epochs, silent=False):\n",
    "    for i in range(1, epochs+1):\n",
    "        # Perform forward propagation over neural network.\n",
    "        self._forward(x)\n",
    "\n",
    "        # Perform backward propagation over neural network.\n",
    "        self._backward(x, y_true)\n",
    "\n",
    "        # Update weights of neural network.\n",
    "        self._update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametrami tej funkcji jest wektor wartości wejściowych, oraz odpowiadającym im wartościom wyjściowym. Wymagane jest również podanie liczby epok, definiującej ile iteracji ma wykonać proces uczenia. Jest to jedyny zaimplementowany warunek stopu.\n",
    "Uczenie odbywa się na całym zbiorze trenującym podanym do funkcji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kluczowe decyzje projektowe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zbiorze danych sentyment zdania został przedstawiony w formie trzech etykiet: \"1\" - negatywny, \"3\" - neutralny, \"5\" - pozytywny. Etykiety zbioru danych zostały zamienione do postaci \"1 z n\" (ang. One-hot Encoding), czyli w postaci wektora o rozmiarze liczby etykiet w zbiorze, gdzie wartość 1 w wierszu oznacza przynależność do danej klasy. Stosując taką reprezentację, warstwa wyjściowa sieci neuronowej musi mieć funkcję aktywacji Softmax. Funkcja Softmax umożliwia nam prezentację obliczeń sieci w postaci prawdopodobieństwa wystąpienia danej etykiety. Funkcja Softmax:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ S(z) = \\frac{e^{z_i}}{\\sum_{j}e^{z_j}}  \\\\\\\n",
    "\\end{equation*}\n",
    "gdzie:  **S(z)** - wektor wartości funkcji Softmax,\n",
    "\n",
    "**z** - wektor pierwotnych wyników klasyfikacji.\n",
    "\n",
    "Posiadając etykiety w postaci \"1 z n\" sieć oblicza funkcję błędu *Cross-Entropy* według wzoru:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\ Q_i(S(z), y_t) = - \\sum_{i} y_{t_i} log(S(z)_i)\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "gdzie: Qi - funkcja błędu na zdaniu, \n",
    "\n",
    "**S(z)** - wektor wartości funkcji Softmax, \n",
    "\n",
    "S(z)i - i-ta wartość wektora S(z), \n",
    "\n",
    "**z** - wektor klasyfikacji sieci, \n",
    "\n",
    "**yt** - wektor prawdziwych etykiet obrazu w postaci „1 z n”,\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W modelu bag-of-words zdania są reprezentowane w postaci multizbioru słów. Multizbiorem nazywamy zbiór, w którym jeden element może występować wiele razy. W praktyce model ten odnosi się do wektora, w którym dany indeks odpowiada jednemu słowu w zbiorze danych, a wartość wektora i-tego indeksu odpowiada ilości wystąpień słowa w zdaniu.\n",
    "\n",
    "W takim modelu na uczenie sieci neuronowej najbardziej wpływają słowa, które pojawiają się najczęściej w danym zdaniu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model TFIDF pozwala nam na utworzenie wektora, w którym najczęściej występujące słowa w całym zbiorze danych mają najmniejszą wartość, zazwyczaj zerową.\n",
    "\n",
    "W takim modelu na uczenie sieci neuronowej najbardziej wpływają słowa, które pojawiają się najrzadziej w danym zbiorze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wnioski dotyczące osiągniętych rezultatów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trenowanie sieci na początku odbywało się przy pomocy całego zbioru danych o rozmiarze (3804, 8257). Podczas trenowania często występował problem, w którym błąd na zbiorze nie zmieniał się z kolejnymi iteracjami. Zarejestrowana taka sytuacja jest np. w zeszycie \"Testowanie.ipynb\" (sieć piąta). Po dokładniejszej analizie okazało się, że wartości liczone przez warstwy były tak duże, że wynikiem funkcji aktywacji (sigmoid, softmax) były wartości nieskończone.\n",
    "\n",
    "Żeby zlikwidować ten problem postanowiliśmy podzielić zbiór na mniejsze części podczas trenowania implementując funkcję *train_batch*. W ten sposób sieć neuronowa będzie wykonywała obliczenia na mniejszych wartościach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
